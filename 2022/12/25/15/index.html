<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="待莲花开尽后，便是清欢。小轲博客欢迎您的到来。站长QQ：2501521908">
    <meta name="description" content="小轲博客，是小轲的个人博客！更新个人教程，网站程序等。当然会更新小轲自己的生活等">
    <meta name="author" content="小轲">
    
    <title>
        
            Kafka学习笔记 |
        
        小轲博客
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="shortcut icon" href="https://www.keshaowl.cn/favicon.ico">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/fontawesome/css/fontawesome.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/fontawesome/css/regular.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/fontawesome/css/solid.min.css">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/fontawesome/css/brands.min.css">
    <script id="hexo-configurations">
    let KEEP = window.KEEP || {};
    KEEP.hexo_config = {"hostname":"52xk.cc","root":"/","language":"zh-CN","path":"search.xml"};
    KEEP.theme_config = {"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true},"style":{"primary_color":"#0066CC","avatar":"/images/head-img.jpg","favicon":"https://www.keshaowl.cn/favicon.ico","article_img_align":"center","left_side_width":"260px","content_max_width":"920px","hover":{"shadow":false,"scale":false},"first_screen":{"enable":true,"background_img":"/images/bg.svg","description":"所谓自由，不是随心所欲，而是自我主宰。"},"scroll":{"progress_bar":{"enable":true},"percent":{"enable":true}}},"local_search":{"enable":true,"trigger":"auto","unescape":false,"preload":true},"code_copy":{"enable":true,"style":"default"},"code_block":{},"side_tools":{},"pjax":{"enable":true},"lazyload":{"enable":true},"comment":{"enable":true,"use":"gitalk","valine":{"appid":"OUpmlYUxojJKPUexl0jbqBen-gzGzoHsz","appkey":"bFVLKcqpcD4wxmOMvXpnATHG","placeholder":"😜 尽情吐槽吧~"},"gitalk":{"github_id":"a2501521908","repository":"blog-comment","client_id":"98dcf7e70682aab74e7d","client_secret":"f20e879a10afe8fd7f0f47d82780eb1d17d0804f"},"twikoo":{"env_id":null,"region":null}},"post":{"word_count":{"enable":true,"wordcount":true,"min2read":true},"author_label":{"enable":true,"auto":true,"custom_label_list":["Trainee","Engineer","Architect","CTO","BOSS"]}},"version":"3.5.2"};
    KEEP.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
    KEEP.language_code_block = {"copy":"复制代码","copied":"已复制","fold":"折叠代码块","folded":"已折叠"};
  </script>
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <i class="pjax-progress-icon fas fa-circle-notch fa-spin"></i>
    
</div>


<main class="page-container">

    

    <div class="page-main-content">

        <div class="page-main-content-top">
            
<header class="header-wrapper">

    <div class="header-content">
        <div class="left">
            
            <a class="logo-title" href="/">
               小轲博客
            </a>
        </div>

        <div class="right">
            <div class="pc">
                <ul class="menu-list">
                    
                        <li class="menu-item">
                            <a class=""
                               href="/"
                            >
                                首页
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/archives"
                            >
                                归档
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/tags"
                            >
                                标签
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/links"
                            >
                                友链
                            </a>
                        </li>
                    
                        <li class="menu-item">
                            <a class=""
                               href="/about"
                            >
                                关于
                            </a>
                        </li>
                    
                    
                        <li class="menu-item search search-popup-trigger">
                            <i class="fas fa-search"></i>
                        </li>
                    
                </ul>
            </div>
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div>
                
                <div class="icon-item menu-bar">
                    <div class="menu-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <div class="header-drawer">
        <ul class="drawer-menu-list">
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/">首页</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/archives">归档</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/tags">标签</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/links">友链</a>
                </li>
            
                <li class="drawer-menu-item flex-center">
                    <a class=""
                       href="/about">关于</a>
                </li>
            
        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="page-main-content-middle">

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            <div class="article-title">
                <span class="title-hover-animation">Kafka学习笔记</span>
            </div>

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/head-img.jpg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">小轲</span>
                            
                                <span class="author-label">Lv3</span>
                            
                        </div>
                        <div class="meta-info">
                            
<div class="article-meta-info">
    <span class="article-date article-meta-item">
        
            <i class="fa-regular fa-calendar-plus"></i>&nbsp;
        
        <span class="pc">2022-12-25 13:31:08</span>
        <span class="mobile">2022-12-25 13:31</span>
    </span>
    
        <span class="article-update-date article-meta-item">
        <i class="fas fa-file-pen"></i>&nbsp;
        <span class="pc">2024-02-06 10:40:48</span>
    </span>
    
    
    
        <span class="article-tags article-meta-item">
            <i class="fas fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/springboot/">springboot</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/kafka/">kafka</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/mq/">mq</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/Docker/">Docker</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content keep-markdown-body">
                <h1 id="Kafka基本的概念"><a href="#Kafka基本的概念" class="headerlink" title="Kafka基本的概念"></a>Kafka基本的概念</h1><p>Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，**基于zookeeper协调的分布式日志系统(也可以当做MQ系统)**，常见可以用于web&#x2F;nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。<br><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/uYclj9Fd5HU7v8N.png"
                      alt="image.png"
                ></p>
<h2 id="Kafka部分名词解释如下"><a href="#Kafka部分名词解释如下" class="headerlink" title="Kafka部分名词解释如下"></a>Kafka部分名词解释如下</h2><ul>
<li>Broker：消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。</li>
<li>Topic：一类消息，例如page view日志、click、短信、日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。</li>
<li>Partition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。相当于分区</li>
<li>Segment：partition物理上由多个segment组成</li>
<li>offset：每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息.</li>
<li>Producer：消息的生产者，负责向Broker中投递消息</li>
<li>Consumer：消息的消费者，负责从Broker中拉取消息</li>
<li>Consumer Group：消费者组，在同一个消费者组中是不能够消费同一个分区中的消息的。在多个不同的消费组中，多个不同的消费者可以消费同一条消息</li>
</ul>
<p><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/amST2Jd53PFiXYx.png"
                      alt="image.png"
                ></p>
<h2 id="Kafka消息队列模型"><a href="#Kafka消息队列模型" class="headerlink" title="Kafka消息队列模型"></a>Kafka消息队列模型</h2><p>kafka的消息队列一般分为两种模式：点对点和订阅模式</p>
<h3 id="点对点模式"><a href="#点对点模式" class="headerlink" title="点对点模式"></a>点对点模式</h3><p>Kafka 是支持消费者群组的，也就是说 Kafka 中会有一个或者多个消费者，如果一个生产者生产的消息由一个消费者进行消费的话，那么这种模式就是点对点模式<br><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/tIs7KyxDedwA1VM.png"
                      alt="image.png"
                ></p>
<h3 id="发布订阅模式"><a href="#发布订阅模式" class="headerlink" title="发布订阅模式"></a>发布订阅模式</h3><p>如果一个生产者或者多个生产者产生的消息能够被多个消费者同时消费的情况，这样的消息队列成为发布订阅模式的消息队列<br><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/TqlW5aN6siXt7nz.png"
                      alt="image.png"
                ></p>
<h2 id="Kafka有哪些特性致使它性能这么高？"><a href="#Kafka有哪些特性致使它性能这么高？" class="headerlink" title="Kafka有哪些特性致使它性能这么高？"></a>Kafka有哪些特性致使它性能这么高？</h2><ul>
<li>顺序读写</li>
<li>零拷贝</li>
<li>消息压缩</li>
<li>分批发送</li>
</ul>
<h2 id="Kafka的设计亮点"><a href="#Kafka的设计亮点" class="headerlink" title="Kafka的设计亮点"></a>Kafka的设计亮点</h2><ol>
<li>高吞吐、低延迟：kakfa 最大的特点就是收发消息非常快，kafka 每秒可以处理几十万条消息，它的最低延迟只有几毫秒；</li>
<li>高伸缩性：每个主题(topic) 包含多个分区(partition)，主题中的分区可以分布在不同的主机(broker)中；</li>
<li>持久性、可靠性：Kafka 能够允许数据的持久化存储，消息被持久化到磁盘，并支持数据备份防止数据丢失，Kafka 底层的数据存储是基于 Zookeeper 存储的，Zookeeper 我们知道它的数据能够持久存储；</li>
<li>容错性：允许集群中的节点失败，某个节点宕机，Kafka 集群能够正常工作；</li>
<li>高并发：支持数千个客户端同时读写。</li>
</ol>
<h1 id="kafka的设计原理"><a href="#kafka的设计原理" class="headerlink" title="kafka的设计原理"></a>kafka的设计原理</h1><h2 id="kafka文件存储原理"><a href="#kafka文件存储原理" class="headerlink" title="kafka文件存储原理"></a>kafka文件存储原理</h2><ol>
<li>Kafka实际上就是日志消息存储系统， 根据offset获取对应的消息，消费者获取到消息之后</li>
</ol>
<p>该消息不会立即从mq中移除。</p>
<ol start="2">
<li>将topic分成多个不同的分区、每个分区中拆分成多个不同的segment文件存储日志。</li>
<li>每个segment文件会有<ul>
<li>.index 消息偏移量索引文件</li>
<li>.log文件 消息物理存放的位置</li>
</ul>
</li>
</ol>
<p>在默认的情况下，每个segment文件容量最大是为500mb，如果超过500mb的情况下依次内推，产生一个新的segment文件</p>
<h3 id="topic中partition存储分布"><a href="#topic中partition存储分布" class="headerlink" title="topic中partition存储分布"></a>topic中partition存储分布</h3><p>假设一个kafka集群中只有一个broker，<code>opt/keshao/message-data</code>为数据文件存储目录，在Kafka broker中的server.properties文件配置（参数：<code>log.dirs</code>），例如创建2个topic名称分别为report_pushlaunch_info,partitions数量都为<code>partitions=4 </code></p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">|--report_push<span class="number">-0</span></span><br><span class="line">|--report_push<span class="number">-1</span></span><br><span class="line">|--report_push<span class="number">-2</span></span><br><span class="line">|--report_push<span class="number">-3</span></span><br><span class="line">|--launch_info<span class="number">-0</span></span><br><span class="line">|--launch_info<span class="number">-1</span></span><br><span class="line">|--launch_info<span class="number">-2</span></span><br><span class="line">|--launch_info<span class="number">-3</span></span><br></pre></td></tr></table></figure>
<p>在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 如果是多broker分布情况，请参考<a class="link"   target="_blank" rel="noopener" href="http://blog.csdn.net/lizhitao/article/details/41778193" >kafka集群partition分布原理分析<i class="fas fa-external-link-alt"></i></a></p>
<h3 id="partition中文件存储方式"><a href="#partition中文件存储方式" class="headerlink" title="partition中文件存储方式"></a>partition中文件存储方式</h3><ul>
<li>每个partiton中相当于一个巨型文件被平均分配到多个大小相等segment（段）数据中。但每个段segment file消息数量不一定相等，这种特性方便old segment被快速检索删除</li>
<li>每个partiton只需要顺序读写就可以了，segment文件生命周期由服务端配置参数决定。</li>
<li>这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。</li>
</ul>
<p><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/wPCEouTMVZAlvLa.png"
                      alt="image.png"
                ></p>
<h3 id="partiton中segment文件存储结构"><a href="#partiton中segment文件存储结构" class="headerlink" title="partiton中segment文件存储结构"></a>partiton中segment文件存储结构</h3><ul>
<li>segment file由2大部分组成，分别index file和data file，两个文件一一对应，成对出现。后缀<code>.index</code>和<code>.log</code>分别表示为segment索引文件、数据文件；</li>
<li>segment文件命名规则：partiton全局的第一个segment0开始，后续每个setment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充；</li>
</ul>
<p>文件的是Kafka broker中做的一个实验，创建一个topicXXX包含1 partiton，设置每个segment大小为500MB，并启动producer向Kafka broker写入大量数据<br><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/wRC7YxMThmutsVF.png"
                      alt="image.png"
                ><br>segment file中，index与file的物理关系如下：<br><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/vqM6nWLVGPlaSkF.png"
                      alt="image.png"
                ><br>查找顺序：<br>查找offset&#x3D;6的消息的流程如下</p>
<ol>
<li>二分查找算法：查找到该分区中所有的Segment文件 list排序  每个Segment文件都是有一个命名规范，offset&#x3D;7在我们的Segment文件中，此处定位到index文件</li>
<li>先访问该index文件，根据offset值查询到物理存放位置，Offset&#x3D;7&gt;6&lt;9 所以定位到offset&#x3D;6 获取到物理存放位置1407</li>
<li>根据该物理存放位置9807 去对应的log文件查找消息，依次向下查找+1次 获取到offset&#x3D;7的消息。</li>
</ol>
<p>为什么kafka中的 索引文件没有对每个消息建立索引呢？</p>
<ol>
<li>目的是为了节约我们空间的资源</li>
<li>稀疏索引算法+二分查找算法，定位到位置，在根据顺序遍历查找。<br>如果该offset消息 没有对应的索引的情况下，时间复杂度是为多少：（ON）<br>如果该offset消息 有对应的索引的情况下，时间复杂度是为多少：（O1）</li>
</ol>
<h3 id="message的存储方式"><a href="#message的存储方式" class="headerlink" title="message的存储方式"></a>message的存储方式</h3><p>表格中列出了message的物理结构：</p>
<table>
<thead>
<tr>
<th>关键字</th>
<th>解释说明</th>
</tr>
</thead>
<tbody><tr>
<td>8 byte offset</td>
<td>在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message</td>
</tr>
<tr>
<td>4 byte message size</td>
<td>message大小</td>
</tr>
<tr>
<td>4 byte CRC32</td>
<td>用crc32校验message</td>
</tr>
<tr>
<td>1 byte “magic”</td>
<td>表示本次发布Kafka服务程序协议版本号</td>
</tr>
<tr>
<td>1 byte “attributes”</td>
<td>表示为独立版本、或标识压缩类型、或编码类型。</td>
</tr>
<tr>
<td>4 byte key length</td>
<td>表示key的长度,当key为-1时，K byte key字段不填</td>
</tr>
<tr>
<td>K byte key</td>
<td>可选</td>
</tr>
<tr>
<td>value bytes payload</td>
<td>表示实际消息数据。</td>
</tr>
</tbody></table>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>Kafka运行时很少有大量读磁盘的操作，主要是定期批量写磁盘操作，因此操作磁盘很高效。这跟Kafka文件存储中读写message的设计是息息相关的。Kafka中读写message有如下特点:<br>写message</p>
<ul>
<li>消息从java堆转入page cache(即物理内存)。</li>
<li>由异步线程刷盘,消息从page cache刷入磁盘。</li>
</ul>
<p>读message</p>
<ul>
<li>消息直接从page cache转入socket发送出去。</li>
<li>当从page cache没有找到相应数据时，此时会产生磁盘IO,从磁 盘Load消息到page cache,然后直接从socket发出去</li>
</ul>
<p>Kafka高效文件存储设计特点</p>
<ol>
<li>Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。</li>
<li>通过索引信息可以快速定位message和确定response的最大大小。</li>
<li>通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。</li>
<li>通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。</li>
</ol>
<p>无论消息是否被消费，kafka都会保存所有的消息，旧消息删除策略<br>1、 基于时间，默认配置是168小时（7天）。<br>2、 基于大小，默认配置是1073741824。</p>
<h2 id="Kafka如何保证生产消息可靠性"><a href="#Kafka如何保证生产消息可靠性" class="headerlink" title="Kafka如何保证生产消息可靠性"></a>Kafka如何保证生产消息可靠性</h2><h3 id="副本机制"><a href="#副本机制" class="headerlink" title="副本机制"></a>副本机制</h3><p>副本机制（Replication）：也可以称之为备份机制，通常是指分布式系统在多台互联网的机器上保存相同的数据拷贝，副本机制有什么好处？</p>
<ul>
<li>提供数据冗余：即使系统部分组件失效，系统依然能够继续运转，因而增加了整体可用性以及数据持久性</li>
<li>提供高伸缩性：支持横向扩展，能够通过添加机器的方式来提升读的性能，进而提高读操作吞吐量</li>
<li>改善数据局部性：允许将数据放入与用户地理位置相近的地方，从而降低系统延时</li>
</ul>
<p>kafka是有主题概念的，而每一个主题又进一步划分成若干个分区。副本的概念实际上是在分区层级下定义的，每个分区配置有多若干个副本。<br>所谓的副本，本质上就是一个只能追加写消息的提交日志，根据kafka副本机制的定义，同一个分区下的所有副本保存有相同的消息序列，这些副本分散的保存在不同的Broker上，从而能够对抗部分Broker宕机带来的数据不可用。<br><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/TF1geoplAOyHvVj.png"
                      alt="image.png"
                ></p>
<h3 id="kafka有了副本机制是否会发生数据丢失？"><a href="#kafka有了副本机制是否会发生数据丢失？" class="headerlink" title="kafka有了副本机制是否会发生数据丢失？"></a>kafka有了副本机制是否会发生数据丢失？</h3><p>会。写入数据都是往某个Partition的Leader写入的，然后那个Partition的Follower会从Leader同步数据，但是这个同步过程是异步的。也就是说如果此时1条数据刚写入Leader Partition1，还没来得及同步给Follower，Leader Partiton1所在机器突然就宕机了的话，此时就会选举Partition1的Follower作为新的Leader对外提供服务，然后用户就读不到刚才写入的那条数据了。因为Partition0的Follower上是没有同步到最新的一条数据的，这个时候就会造成数据丢失的问题。</p>
<h3 id="Kafka的ISR机制"><a href="#Kafka的ISR机制" class="headerlink" title="Kafka的ISR机制"></a>Kafka的ISR机制</h3><blockquote>
<p><strong>此处的Leader是Partition的Leader，而不是Broker的Leader</strong></p>
</blockquote>
<p>这个机制简单来说，就是会自动给每个Partition维护一个ISR列表，这个列表里一定会有Leader，然后还会包含跟Leader保持同步的Follower。也就是说，只要Leader的某个Follower一直跟他保持数据同步，那么就会存在于ISR列表里。<br>但是如果Follower因为自身发生一些问题，导致不能及时的从Leader同步数据过去，那么这个Follower就会被认为是“out-of-sync”，从ISR列表里移除。</p>
<h4 id="怎么保证Kafka写入的数据不丢失？"><a href="#怎么保证Kafka写入的数据不丢失？" class="headerlink" title="怎么保证Kafka写入的数据不丢失？"></a>怎么保证Kafka写入的数据不丢失？</h4><ol>
<li>每个Partition都至少得有1个Follower在ISR列表里，跟上了Leader的数据同步</li>
<li>每次写入数据的时候，都要求至少写入Partition Leader成功，同时还有至少一个ISR里的Follower也写入成功，才算这个写入是成功了</li>
<li>如果不满足上述两个条件，那就一直写入失败，让生产系统不停的尝试重试，直到满足上述两个条件，然后才能认为写入成功</li>
<li>这个时候万一leader宕机，就可以切换到那个follower上去，那么Follower上是有刚写入的数据的，此时数据就不会丢失了。</li>
</ol>
<p>关于第二点就需要去配置相应ack参数，才能保证写入Kafka的数据不会丢失。</p>
<h3 id="Kafka的ack消息模式"><a href="#Kafka的ack消息模式" class="headerlink" title="Kafka的ack消息模式"></a>Kafka的ack消息模式</h3><p>acks参数，是在Kafka Producer，也就是生产者里设置的。<br>这个参数实际上有三种常见的值可以设置，分别是：0、1 和 all。</p>
<ul>
<li>0： Producer 不等待 Broker 的 ACK，这提供了最低延迟，Broker 一收到数据还没有写入磁盘就已经返回，当 Broker 故障时有可能丢失数据。</li>
<li>1:   Producer 等待 Broker 的 ACK，Partition 的 Leader 落盘成功后返回 ACK，如果在 Follower 同步成功之前 Leader 故障，那么将会丢失数据。</li>
<li>-1 （all）： Producer 等待 Broker 的 ACK，Partition 的 Leader 和 Follower 全部落盘成功后才返回 ACK。但是在 Broker 发送 ACK 时，Leader 发生故障，则会造成数据重复。</li>
</ul>
<h3 id="副本选举实现原理"><a href="#副本选举实现原理" class="headerlink" title="副本选举实现原理"></a>副本选举实现原理</h3><p>当Leader副本宕机之后，会从ISR同步副本列表中剔除，然后取出剩下的ISR列表中第一个为Lader副本，显然还有可能数据没有及时同步完成，当选择为Leader副本之后，数据还会可能存在丢失的情况。<br><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/TjdnofsSGYVCuHF.png"
                      alt="image.png"
                ></p>
<h3 id="副本故障处理机制"><a href="#副本故障处理机制" class="headerlink" title="副本故障处理机制"></a>副本故障处理机制</h3><ul>
<li>LEO:每个副本数据最后一个的offset或者最大的offset值。</li>
<li>HW 消费者能够看见到的最大offset值。</li>
</ul>
<p><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/s9dfBZwzlkT25yK.png"
                      alt="image.png"
                ></p>
<h4 id="Follower节点发生故障原理"><a href="#Follower节点发生故障原理" class="headerlink" title="Follower节点发生故障原理"></a>Follower节点发生故障原理</h4><p>当我们follower2节点如果宕机之后，就会从ISR列表中剔除，有突然恢复了，则开始同步Leader 节点的数据。<br>如何同步：如果follower 的leo不等于Leader 节点的leo，则开始截取高于当前hw位置的log，从该hw位置开始同步Leader 节点数据，如果该follower的leo大于该分区的hw，则从新加入isr列表中。<br><strong>Kafka实现集群，保证每个副本的数据一致性问题，但是不能保证消息不会丢失</strong></p>
<blockquote>
<p>发生该问题如何解决？<br>生产者投递消息采用日志形式记录下来，如果消费者消费成功之后，在可以将该消息给删除。</p>
</blockquote>
<h4 id="Leader节点发生故障原理"><a href="#Leader节点发生故障原理" class="headerlink" title="Leader节点发生故障原理"></a>Leader节点发生故障原理</h4><p>如果Leader节点宕机之后，会从新在剩余的isr列表中，选举一个新的Leader节点。为了保证每个节点中副本一致性的问题，会将高与hw位置的log给截取掉。<br>所以我们kafka为了严格意义上，保证每个节点副本数据一致性问题，但是不能保证数据不丢失。<br>概率：—-非常低。<br>解决办法：<br>生产者投递消息的时候采用日志记录的方式，如果发生Leader变为follower 部分的消息被丢失的情况下，我们可以使用生产投递日志实现补偿。</p>
<h2 id="Kafka选举原理控制器原理"><a href="#Kafka选举原理控制器原理" class="headerlink" title="Kafka选举原理控制器原理"></a>Kafka选举原理控制器原理</h2><p><strong>控制器组件（Controller），是 Apache Kafka 的核心组件。它的主要作用是在 Apache ZooKeeper 的帮助下管理和协调整个 Kafka 集群</strong>。在分布式系统中，通常需要有一个协调者，该协调者会在分布式系统发生异常时发挥特殊的作用。在Kafka中该协调者称之为控制器(Controller),其实该控制器并没有什么特殊之处，它本身也是一个普通的Broker，只不过需要负责一些额外的工作(追踪集群中的其他Broker，并在合适的时候处理新加入的和失败的Broker节点、Rebalance分区、分配新的leader分区等)。值得注意的是：<strong>Kafka集群中始终只有一个Controller Broker。</strong></p>
<h3 id="Controller-Broker是如何被选出来的"><a href="#Controller-Broker是如何被选出来的" class="headerlink" title="Controller Broker是如何被选出来的"></a>Controller Broker是如何被选出来的</h3><p>Broker 在启动时，会尝试去 ZooKeeper 中创建 &#x2F;controller 节点。Kafka 当前选举控制器的规则是：<strong>第一个成功创建 &#x2F;controller 节点的 Broker 会被指定为控制器</strong>。<br><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/5DPETtRd9GxZUfM.png"
                      alt="image.png"
                ><br>Kafka的Broker Controller通过注册一个controller临时节点节点进行竞选，如果未set成功，则watch该节点，等待成为controller</p>
<h3 id="Controller-Broker的具体作用是什么"><a href="#Controller-Broker的具体作用是什么" class="headerlink" title="Controller Broker的具体作用是什么"></a>Controller Broker的具体作用是什么</h3><p>Controller Broker的主要职责有很多，主要是一些管理行为，主要包括以下几个方面：</p>
<ul>
<li>创建、删除主题，增加分区并分配leader分区</li>
<li>集群Broker管理（新增 Broker、Broker 主动关闭、Broker 故障)</li>
<li><strong>preferred leader</strong>选举</li>
<li>分区重分配</li>
</ul>
<h4 id="主题管理"><a href="#主题管理" class="headerlink" title="主题管理"></a>主题管理</h4><p>这里的主题管理，就是指控制器帮助我们完成对 Kafka 主题的创建、删除以及分区增加的操作。换句话说，当我们执行kafka-topics 脚本时，大部分的后台工作都是控制器来完成的。</p>
<h4 id="分区重分配"><a href="#分区重分配" class="headerlink" title="分区重分配"></a>分区重分配</h4><p>分区重分配主要是指，kafka-reassign-partitions 脚本提供的对已有主题分区进行细粒度的分配功能。这部分功能也是控制器实现的。</p>
<h4 id="集群成员管理"><a href="#集群成员管理" class="headerlink" title="集群成员管理"></a>集群成员管理</h4><p>自动检测新增 Broker、Broker 主动关闭及被动宕机。这种自动检测是依赖于前面提到的 Watch 功能和 ZooKeeper 临时节点组合实现的。比如，控制器组件会利用Watch 机制检查 ZooKeeper 的 <code>/brokers/ids</code> 节点下的子节点数量变更。目前，当有新 Broker 启动后，它会在 <code>/brokers</code> 下创建专属的 znode 节点。一旦创建完毕，ZooKeeper 会通过 Watch 机制将消息通知推送给控制器，这样，控制器就能自动地感知到这个变化，进而开启后续的新增 Broker 作业。<br>侦测 Broker 存活性则是依赖于刚刚提到的另一个机制：临时节点。每个 Broker 启动后，会在 <code>/brokers/ids </code>下创建一个临时 znode。当 Broker 宕机或主动关闭后，该 Broker 与 ZooKeeper 的会话结束，这个 znode 会被自动删除。同理，ZooKeeper 的 Watch 机制将这一变更推送给控制器，这样控制器就能知道有 Broker 关闭或宕机了，从而进行“善后”。</p>
<h4 id="数据服务"><a href="#数据服务" class="headerlink" title="数据服务"></a>数据服务</h4><p>控制器的最后一大类工作，就是向其他 Broker 提供数据服务，控制器上保存了最全的集群元数据信息，其他所有 Broker 会定期接收控制器发来的元数据更新请求，从而更新其内存中的缓存数据。<br>当控制器发现一个 broker 离开集群（通过观察相关 ZooKeeper 路径），控制器会收到消息：这个 broker 所管理的那些分区需要一个新的 Leader。控制器会依次遍历每个分区，确定谁能够作为新的 Leader，然后向所有包含新 Leader 或现有 Follower 的分区发送消息，该请求消息包含谁是新的 Leader 以及谁是 Follower 的信息。随后，新的 Leader 开始处理来自生产者和消费者的请求，Follower 用于从新的 Leader 那里进行复制。</p>
<h4 id="消费者Rebalance机制（再平衡）"><a href="#消费者Rebalance机制（再平衡）" class="headerlink" title="消费者Rebalance机制（再平衡）"></a><strong>消费者Rebalance机制（再平衡）</strong></h4><p>rebalance就是说如果消费组里的消费者数量有变化或消费的分区数有变化，kafka会重新分配消费者消费分区的关系。比如consumer group中某个消费者挂了，此时会自动把分配给他的分区交给其他的消费者，如果他又重启了，那么又会把一些分区重新交还给他。<br><strong>注意：rebalance只针对subscribe这种不指定分区消费的情况，如果通过assign这种消费方式指定了分区，kafka不会进行rebanlance。</strong><br>如下情况可能会触发消费者rebalance:</p>
<ul>
<li>消费组里的consumer增加或减少了</li>
<li>动态给topic增加了分区</li>
<li>消费组订阅了更多的topic</li>
</ul>
<p>主要有三种rebalance的策略：range()、round-robin(轮询)、sticky(粘性)。<br>Kafka 提供了消费者客户端参数partition.assignment.strategy 来设置消费者与订阅主题之间的分区分配策略。默认情况为range分配策略。</p>
<ol>
<li>range策略就是按照分区序号排序(范围分配)，假设 n＝分区数／消费者数量 &#x3D; 3， m＝分区数%消费者数量 &#x3D; 1，那么前 m 个消费者每个分配 n+1 个分区，后面的（消费者数量－m ）个消费者每个分配 n 个分区。比如分区0<del>3给一个consumer，分区4</del>6给一个consumer，分区7~9给一个consumer。</li>
<li>round-robin策略就是轮询分配，比如分区0、3、6、9给一个consumer，分区1、4、7给一个consumer，分区2、5、8给一个consumer</li>
<li>sticky策略初始时分配策略与round-robin类似，但是在rebalance的时候，需要保证如下两个原则。<ol>
<li>分区的分配要尽可能均匀</li>
<li>分区的分配尽可能与上次分配的保持相同。</li>
<li>当两者发生冲突时，第一个目标优先于第二个目标 。这样可以最大程度维持原来的分区分配的策略。比如对于第一种range情况的分配，如果第三个consumer挂了，那么重新用sticky策略分配的结果如下：<ol>
<li>consumer1除了原有的0~3，会再分配一个7</li>
<li>consumer2除了原有的4~6，会再分配8和9</li>
</ol>
</li>
</ol>
</li>
</ol>
<h4 id="同步副本-in-sync-replica-ISR-列表"><a href="#同步副本-in-sync-replica-ISR-列表" class="headerlink" title="同步副本(in-sync replica ,ISR)列表"></a>同步副本(in-sync replica ,ISR)列表</h4><p>ISR中的副本都是与Leader进行同步的副本，所以不在该列表的follower会被认为与Leader是不同步的. 那么，ISR中存在是什么副本呢？首先可以明确的是：Leader副本总是存在于ISR中。 而follower副本是否在ISR中，取决于该follower副本是否与Leader副本保持了“同步”。<br>始终保证拥有足够数量的同步副本是非常重要的。要将follower提升为Leader，它必须存在于<strong>同步副本列表中</strong>。每个分区都有一个同步副本列表，该列表由Leader分区和Controller进行更新。<br>选择一个同步副本列表中的分区作为leader 分区的过程称为<strong>clean leader election</strong>。注意，这里要与在非同步副本中选一个分区作为leader分区的过程区分开，在非同步副本中选一个分区作为leader的过程称之为<strong>unclean leader election</strong>。由于ISR是动态调整的，所以会存在ISR列表为空的情况，通常来说，非同步副本落后 Leader 太多，因此，如果选择这些副本作为新 Leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老 Leader 中的消息。在 Kafka 中，选举这种副本的过程可以通过Broker 端参数 *_<em>*unclean.leader.election.enable <em><strong>_控制是否允许 Unclean 领导者选举。开启 Unclean 领导者选举可能会造成数据丢失，但好处是，它使得分区 Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止 Unclean Leader 选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。分布式系统的CAP理论说的就是这种情况。<br>不幸的是，</strong>unclean leader election</em>*的选举过程仍可能会造成数据的不一致，因为同步副本并不是</em>*完全<strong>同步的。由于复制是</strong>异步**完成的，因此无法保证follower可以获取最新消息。比如Leader分区的最后一条消息的offset是100，此时副本的offset可能不是100，这受到两个参数的影响：</p>
<ul>
<li><strong>replica.lag.time.max.ms</strong>：同步副本滞后与leader副本的时间</li>
<li><strong>zookeeper.session.timeout.ms</strong>：与zookeeper会话超时时间</li>
</ul>
<h3 id="脑裂现象"><a href="#脑裂现象" class="headerlink" title="脑裂现象"></a>脑裂现象</h3><p>如果controller Broker 挂掉了，Kafka集群必须找到可以替代的controller，集群将不能正常运转。这里面存在一个问题，很难确定Broker是挂掉了，还是仅仅只是短暂性的故障。但是，集群为了正常运转，必须选出新的controller。如果之前被取代的controller又正常了，他并不知道自己已经被取代了，那么此时集群中会出现两台controller。<br>其实这种情况是很容易发生。比如，某个controller由于GC而被认为已经挂掉，并选择了一个新的controller。在GC的情况下，在最初的controller眼中，并没有改变任何东西，该Broker甚至不知道它已经暂停了。因此，它将继续充当当前controller，这是分布式系统中的常见情况，称为脑裂。<br>假如，处于活跃状态的controller进入了长时间的GC暂停。它的ZooKeeper会话过期了，之前注册的&#x2F;controller节点被删除。集群中其他Broker会收到zookeeper的这一通知。<br><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/SPvYCEBxK7ONWRh.png"
                      alt="image.png"
                ><br>由于集群中必须存在一个controller Broker，所以现在每个Broker都试图尝试成为新的controller。假设Broker 2速度比较快，成为了最新的controller Broker。此时，每个Broker会收到Broker2成为新的controller的通知，由于Broker3正在进行”stop the world”的GC，可能不会收到Broker2成为最新的controller的通知。<br><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/q8lmFXLucptJDy5.png"
                      alt="image.png"
                ><br>等到Broker3的GC完成之后，仍会认为自己是集群的controller，在Broker3的眼中好像什么都没有发生一样。<br><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/tCsSx1UbXGeqpjQ.png"
                      alt="image.png"
                ><br>现在，集群中出现了两个controller，它们可能一起发出具有冲突的命令，就会出现脑裂的现象。如果对这种情况不加以处理，可能会导致严重的不一致。所以需要一种方法来区分谁是集群当前最新的Controller。<br>Kafka是通过使用<strong>epoch number</strong>（纪元编号，也称为隔离令牌）来完成的。epoch number只是单调递增的数字，第一次选出Controller时，epoch number值为1，如果再次选出新的Controller，则epoch number将为2，依次单调递增。<br>每个新选出的controller通过Zookeeper 的条件递增操作获得一个全新的、数值更大的epoch number 。其他Broker 在知道当前epoch number 后，如果收到由controller发出的包含较旧(较小)epoch number的消息，就会忽略它们，即Broker根据最大的epoch number来区分当前最新的controller。</p>
<p><img  
                     lazyload
                     alt="image"
                     data-src="https://s2.loli.net/2022/12/25/yIKh7fMUDknPdCc.png"
                      alt="image.png"
                ><br>上图，Broker3向Broker1发出命令:让Broker1上的某个分区副本成为leader，该消息的epoch number值为1。于此同时，Broker2也向Broker1发送了相同的命令，不同的是，该消息的epoch number值为2，此时Broker1只听从Broker2的命令(由于其epoch number较大)，会忽略Broker3的命令，从而避免脑裂的发生。</p>
<h1 id="Kafka优化策略"><a href="#Kafka优化策略" class="headerlink" title="Kafka优化策略"></a>Kafka优化策略</h1><h2 id="常见核心配置"><a href="#常见核心配置" class="headerlink" title="常见核心配置"></a>常见核心配置</h2><h3 id="内存缓冲的大小：buffer-memory"><a href="#内存缓冲的大小：buffer-memory" class="headerlink" title="内存缓冲的大小：buffer.memory"></a>内存缓冲的大小：buffer.memory</h3><ol>
<li>生产者投递消息先存放在本地缓冲区中，将消息组装成n多个不同的Batch，在通过send线程将缓冲区的数据批量的形式发送给kafka服务器端存放。</li>
<li>生产者本地内存缓冲区如果设置太小了，在高并发情况下有可能会发生内存溢出，导致生产者无法继续写入消息到缓冲区卡死。</li>
<li>实际生产环境中，根据压力测试情况下，合理设置内存缓冲区大小。</li>
</ol>
<p>参数：<code>buffer.memory</code></p>
<h3 id="最大请求大小：“max-request-size”"><a href="#最大请求大小：“max-request-size”" class="headerlink" title="最大请求大小：“max.request.size”"></a>最大请求大小：“max.request.size”</h3><p>该参数kafkamq服务器端限制接受的消息</p>
<h3 id="重试策略“retries”和“retries-backoff-ms”"><a href="#重试策略“retries”和“retries-backoff-ms”" class="headerlink" title="重试策略“retries”和“retries.backoff.ms”"></a>重试策略“retries”和“retries.backoff.ms”</h3><p>该参数设置定重试的次数、间隔时间</p>
<h3 id="确认机制：acks"><a href="#确认机制：acks" class="headerlink" title="确认机制：acks"></a>确认机制：acks</h3><p>建议设置为1<br>ACK 参数配置：</p>
<ol>
<li>0：Producer 不等待 Broker 的 ACK，这提供了最低延迟，Broker 一收到数据还没有写入磁盘就已经返回，当 Broker 故障时有可能丢失数据。</li>
<li>1：Producer 等待 Broker 的 ACK，Partition 的 Leader 落盘成功后返回 ACK，如果在 Follower 同步成功之前 Leader 故障，那么将会丢失数据。</li>
<li>-1（all）：Producer 等待 Broker 的 ACK，Partition 的 Leader 和 Follower 全部落盘成功后才返回 ACK。但是在 Broker 发送 ACK 时，Leader 发生故障，则会造成数据重复。</li>
</ol>
<p><strong>具体看业务要求：-1 all  延迟概率一定很低 0 延迟概率为适中1</strong></p>
<h3 id="消费者分区的个数"><a href="#消费者分区的个数" class="headerlink" title="消费者分区的个数"></a>消费者分区的个数</h3><p>消费者怎么知道我应该从哪个位置开始消费呢？应该有一个记录分组对应消费分区offset位置。</p>
<ul>
<li>在老的版本kafka中是记录在zk上，记录在zk上频繁读写操作，性能不是很好。</li>
<li>在新的版本kafka中，消费者消费分区中的消息是记录在topic主题日志文件中，默认的情况下分成50个文件记录。</li>
</ul>
<p>为什么消费者需要使用50个文件记录消费者消费记录呢？</p>
<ul>
<li>如果消费者（分组）比较多的，都记录在同一个日志文件中，读写操作就非常麻烦。</li>
</ul>
<p>消费者怎么知道我应该读取那个日志文件 知道从那个offset开始消费呢？</p>
<ol>
<li>消费者消费消息的时候：key&#x3D;group-id.topic.partition</li>
<li>group-id.topic.partition&#x3D;mayikt.mttopic.0</li>
<li>(key&#x3D;group-id.topic.partition)%consumer_offsets.size(50)&#x3D;12</li>
<li>Offset消费记录 记录在consumer_offsets-12文件夹</li>
<li>记录Offset消费记录 的是consumer分组对应消费记录 不是记录单个消费者消费记录。</li>
</ol>
<p>–记录当前分组消费的记录—<br>offsets.topic.replication.factor 参数的约束，默认值为3（注意：该参数的使用限制在0.11.0.0版本发生变化），分区数可以通过 offsets.topic.num.partitions 参数设置，默认值为50。</p>
<h2 id="优化策略"><a href="#优化策略" class="headerlink" title="优化策略"></a>优化策略</h2><h3 id="Broker优化"><a href="#Broker优化" class="headerlink" title="Broker优化"></a>Broker优化</h3><ol>
<li>replica复制配置</li>
</ol>
<p>follow从leader拉取消息进行同步数据</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">num.replica.fetchers  拉取线程数 配置多可以提高follower的I/O并发度，单位时间内leader持有更多请求，相应负载会增大，需要根据机器硬件资源做权衡</span><br><span class="line">replica.fetch.min.bytes=<span class="number">1</span>  拉取最小字节数 默认配置为<span class="number">1</span>字节，否则读取消息不及时</span><br><span class="line">replica.fetch.max.bytes= <span class="number">5</span> * <span class="number">1024</span> * <span class="number">1024</span> 拉取最大字节数  默认为1MB，这个值太小，5MB为宜，根据业务情况调整</span><br><span class="line">replica.fetch.wait.max.ms follow 最大等待时间 </span><br></pre></td></tr></table></figure>

<ol start="2">
<li>压缩速度 compression.type：压缩的速度上lz4&#x3D;snappy&lt;gzip。</li>
</ol>
<h3 id="Produer优化"><a href="#Produer优化" class="headerlink" title="Produer优化"></a>Produer优化</h3><p>幂等性：enable.idempotence<br>是否使用幂等性。如果设置为true，表示producer将确保每一条消息只会存放一份；如果设置为false，则表示producer因发送数据到broker失败重试使，可能往数据流中写入多分重试的消息。<br>注意：如果使用idempotence，即enable.idempotence为true，那么要求配置项max.in.flight.requests.per.connection的值必须小于或等于5；配置项retries的值必须大于0；acks配置项必须设置为all。如果这些值没有被用户明确地设置，那么系统将自动选择合适的值。如果设置　　的值不合适，那么会抛出ConfigException异常。</p>
<h3 id="网络和IO线程配置优化"><a href="#网络和IO线程配置优化" class="headerlink" title="网络和IO线程配置优化"></a>网络和IO线程配置优化</h3><p>1.num.network.threads：Broker处理消息的最大线程数<br>2.num.io.threads：Broker处理磁盘IO的线程数<br>一般num.network.threads主要处理网络io，读写缓冲区数据，配置线程数量为cpu核数加1<br>num.io.threads主要进行磁盘io操作，高峰期可能会发生IO等待，因此配置需要大些，配置线程数量为cpu核数2倍，最大不超过3倍.</p>
<h3 id="日志保留策略配置"><a href="#日志保留策略配置" class="headerlink" title="日志保留策略配置"></a>日志保留策略配置</h3><p>生产者投递消息到kafka的mq中，消费者获取到消息之后不会立即被删除，会有一个日志保留策略。</p>
<ol>
<li>减少日志保留时间，建议三天或则更多时间。log.retention.hours&#x3D;72</li>
<li>分段文件配置1GB， 默认是500mb 有利于快速回收磁盘空间，重启kafka加载也会加快(如果文件过小，则文件数量比较多，kafka启动时是单线程扫描目录(log.dir)下所有数据文件)，文件较多时性能会稍微降低。log.segment.bytes&#x3D;1073741824<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">##日志滚动的周期时间，到达指定周期时间时，强制生成一个新的segment</span><br><span class="line">log.roll.hours=<span class="number">72</span></span><br><span class="line">##segment的索引文件最大尺寸限制，即时log.segment.bytes没达到，也会生成一个新的segment</span><br><span class="line">log.index.size.max.bytes=<span class="number">10</span>*<span class="number">1024</span>*<span class="number">1024</span></span><br><span class="line">##控制日志segment文件的大小，超出该大小则追加到一个新的日志segment文件中（-<span class="number">1</span>表示没有限制）</span><br><span class="line">log.segment.bytes=<span class="number">1014</span>*<span class="number">1024</span>*<span class="number">1024</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="log数据文件刷盘策略"><a href="#log数据文件刷盘策略" class="headerlink" title="log数据文件刷盘策略"></a>log数据文件刷盘策略</h3><p>当我们把数据写入到文件系统之后，数据其实在操作系统的page cache里面，并没有刷到磁盘上去。如果此时操作系统挂了，其实数据就丢了。</p>
<ol>
<li>每当producer写入10000条消息时，刷数据到磁盘 配置为：log.flush.interval.messages&#x3D;10000</li>
<li>每间隔1秒钟时间，刷数据到磁盘。log.flush.interval.ms&#x3D;1000</li>
</ol>
<h3 id="配置优化案例（重要）"><a href="#配置优化案例（重要）" class="headerlink" title="配置优化案例（重要）"></a>配置优化案例（重要）</h3><p>Broker<br>num.replica.fetchers： 适量提高同步leader副本线程<br>Producer<br>inger.ms 0 或者1 定时将批量消息发送到Broker中<br>Consumer<br>auto.commit.enable —配置为手动提交offset</p>
<h1 id="Docker环境下的Kafka环境搭建（单机）"><a href="#Docker环境下的Kafka环境搭建（单机）" class="headerlink" title="Docker环境下的Kafka环境搭建（单机）"></a>Docker环境下的Kafka环境搭建（单机）</h1><p><a class="link"   target="_blank" rel="noopener" href="https://www.lixueduan.com/posts/kafka/01-install/" >https://www.lixueduan.com/posts/kafka/01-install/<i class="fas fa-external-link-alt"></i></a></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">zookeeper:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">&#x27;bitnami/zookeeper:latest&#x27;</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;2181:2181&#x27;</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="comment"># 匿名登录--必须开启</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ALLOW_ANONYMOUS_LOGIN=yes</span></span><br><span class="line">    <span class="comment">#volumes:</span></span><br><span class="line">      <span class="comment">#- ./zookeeper:/bitnami/zookeeper</span></span><br><span class="line">  <span class="comment"># 该镜像具体配置参考 https://github.com/bitnami/bitnami-docker-kafka/blob/master/README.md</span></span><br><span class="line">  <span class="attr">kafka:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">&#x27;bitnami/kafka:2.8.0&#x27;</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;9092:9092&#x27;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&#x27;9999:9999&#x27;</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">KAFKA_BROKER_ID=1</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">KAFKA_CFG_LISTENERS=PLAINTEXT://:9092</span></span><br><span class="line">      <span class="comment"># 客户端访问地址，更换成自己的</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181</span></span><br><span class="line">      <span class="comment"># 允许使用PLAINTEXT协议(镜像中默认为关闭,需要手动开启)</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ALLOW_PLAINTEXT_LISTENER=yes</span></span><br><span class="line">      <span class="comment"># 关闭自动创建 topic 功能</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=false</span></span><br><span class="line">      <span class="comment"># 全局消息过期时间 6 小时(测试时可以设置短一点)</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">KAFKA_CFG_LOG_RETENTION_HOURS=6</span></span><br><span class="line">      <span class="comment"># 开启JMX监控</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">JMX_PORT=9999</span></span><br><span class="line">    <span class="comment">#volumes:</span></span><br><span class="line">      <span class="comment">#- ./kafka:/bitnami/kafka</span></span><br><span class="line">    <span class="attr">depends_on:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">zookeeper</span></span><br><span class="line">  <span class="comment"># Web 管理界面 另外也可以用exporter+prometheus+grafana的方式来监控 https://github.com/danielqsj/kafka_exporter</span></span><br><span class="line">  <span class="attr">kafka_manager:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">&#x27;hlebalbau/kafka-manager:latest&#x27;</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;9000:9000&quot;</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">ZK_HOSTS:</span> <span class="string">&quot;zookeeper:2181&quot;</span></span><br><span class="line">      <span class="attr">APPLICATION_SECRET:</span> <span class="string">letmein</span></span><br><span class="line">    <span class="attr">depends_on:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">zookeeper</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">kafka</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="SpringBoot启动和配置kafka"><a href="#SpringBoot启动和配置kafka" class="headerlink" title="SpringBoot启动和配置kafka"></a>SpringBoot启动和配置kafka</h1><p>SpringBoot中很多kafka的使用技巧，简单记录和探索</p>
<h2 id="1-引入依赖"><a href="#1-引入依赖" class="headerlink" title="1. 引入依赖"></a>1. 引入依赖</h2><p>引入基本的maven的pom文件，包含kafka的Server</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-kafka-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.projectlombok<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lombok<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.8.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoding</span>&gt;</span>UTF-8<span class="tag">&lt;/<span class="name">encoding</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="2-使用Spring-Test启动一个kafka"><a href="#2-使用Spring-Test启动一个kafka" class="headerlink" title="2. 使用Spring Test启动一个kafka"></a>2. 使用Spring Test启动一个kafka</h2><p>只要maven项目中引入<code>spring-kafka-test</code>就可以直接使用springboot启动一个kafka的Server，非常方便在开发阶段</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 启动kafka dev环境的实例</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SpringBootTest(classes = ApplicationTests.class)</span></span><br><span class="line"><span class="meta">@EmbeddedKafka(count = 4, ports = &#123;9092, 9093, 9094, 9095&#125;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ApplicationTests</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 启动kafka</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * dev test模式</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">startServer</span><span class="params">()</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        System.in.read();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-初步测试"><a href="#3-初步测试" class="headerlink" title="3. 初步测试"></a>3. 初步测试</h2><h3 id="3-1-编写生产者和消费者的demo代码"><a href="#3-1-编写生产者和消费者的demo代码" class="headerlink" title="3.1 编写生产者和消费者的demo代码"></a>3.1 编写生产者和消费者的demo代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kafka的demo</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> zhangshuaike</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaDemoApplication</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        SpringApplication.run(KafkaDemoApplication.class, args);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;Object, Object&gt; template;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping(&quot;/send/&#123;input&#125;&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendFoo</span><span class="params">(<span class="meta">@PathVariable</span> String input)</span> &#123;</span><br><span class="line">        <span class="built_in">this</span>.template.send(<span class="string">&quot;topic_input&quot;</span>, input);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener(id = &quot;webGroup&quot;, topics = &quot;topic_input&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">listen</span><span class="params">(String input)</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;input value:&quot;</span> + input);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-2-发送测试"><a href="#3-2-发送测试" class="headerlink" title="3.2 发送测试"></a>3.2 发送测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http:<span class="comment">//127.0.0.1:8080/send/test</span></span><br></pre></td></tr></table></figure>
<h2 id="4-带回调函数的生产者"><a href="#4-带回调函数的生产者" class="headerlink" title="4. 带回调函数的生产者"></a>4. 带回调函数的生产者</h2><p>kafkaTemplate提供了一个回调方法addCallback，我们可以在回调方法中监控消息是否发送成功 或 失败时做补偿处理，有两种写法。</p>
<h3 id="4-1-第一种"><a href="#4-1-第一种" class="headerlink" title="4.1 第一种"></a>4.1 第一种</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@GetMapping(&quot;/one/&#123;message&#125;&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage2</span><span class="params">(<span class="meta">@PathVariable(&quot;message&quot;)</span> String callbackMessage)</span> &#123;</span><br><span class="line">    kafkaTemplate.send(<span class="string">&quot;topic1&quot;</span>, callbackMessage).addCallback(success -&gt; &#123;</span><br><span class="line">        <span class="comment">// 消息发送到的topic</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> success.getRecordMetadata().topic();</span><br><span class="line">        <span class="comment">// 消息发送到的分区</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">partition</span> <span class="operator">=</span> success.getRecordMetadata().partition();</span><br><span class="line">        <span class="comment">// 消息在分区内的offset</span></span><br><span class="line">        <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> success.getRecordMetadata().offset();</span><br><span class="line">        System.out.println(<span class="string">&quot;发送消息成功:&quot;</span> + topic + <span class="string">&quot;-&quot;</span> + partition + <span class="string">&quot;-&quot;</span> + offset);</span><br><span class="line">    &#125;, failure -&gt; &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;发送消息失败:&quot;</span> + failure.getMessage());</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-2-第二种"><a href="#4-2-第二种" class="headerlink" title="4.2 第二种"></a>4.2 第二种</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@GetMapping(&quot;/two/&#123;message&#125;&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage3</span><span class="params">(<span class="meta">@PathVariable(&quot;message&quot;)</span> String callbackMessage)</span> &#123;</span><br><span class="line">    kafkaTemplate.send(<span class="string">&quot;topic1&quot;</span>, callbackMessage).addCallback(<span class="keyword">new</span> <span class="title class_">ListenableFutureCallback</span>&lt;SendResult&lt;String, Object&gt;&gt;() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onFailure</span><span class="params">(Throwable ex)</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;发送消息失败：&quot;</span>+ex.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onSuccess</span><span class="params">(SendResult&lt;String, Object&gt; result)</span> &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;发送消息成功：&quot;</span> + result.getRecordMetadata().topic() + <span class="string">&quot;-&quot;</span></span><br><span class="line">                    + result.getRecordMetadata().partition() + <span class="string">&quot;-&quot;</span> + result.getRecordMetadata().offset());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="5-自定义分区器"><a href="#5-自定义分区器" class="headerlink" title="5. 自定义分区器"></a>5. 自定义分区器</h2><p>kafka中每个topic被划分为多个分区，那么生产者将消息发送到topic时，具体要追加到哪个分区？这就是分区策略，Kafka 为我们提供了默认的分区策略，同时它也支持自定义分区策略。其路由机制为：</p>
<ol>
<li>若发送消息时指定了分区（即自定义分区策略），则直接将消息append到指定分区；</li>
<li>若发送消息时未指定 patition，但指定了 key（kafka允许为每条消息设置一个key），则对key值进行hash计算，根据计算结果路由到指定分区，这种情况下可以保证同一个 Key 的所有消息都进入到相同的分区；</li>
<li>patition 和 key 都未指定，则使用kafka默认的分区策略，轮询选出一个 patition；</li>
</ol>
<p>我们自定义一个分区策略，将消息发送到我们指定的partition，首先新建一个分区器类实现Partitioner接口，重写方法，<strong>其中partition方法的返回值就表示将消息发送到几号分区。</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomizePartitioner</span> <span class="keyword">implements</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(String s, Object o, <span class="type">byte</span>[] bytes, Object o1, <span class="type">byte</span>[] bytes1, Cluster cluster)</span> &#123;</span><br><span class="line">        <span class="comment">//自定义分区规则（这里假设全部发到0号分区）</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Map&lt;String, ?&gt; map)</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在application.propertise中配置自定义分区器，配置的值就是分区器类的全路径名</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 自定义分区器</span><br><span class="line">spring.kafka.producer.properties.partitioner.class=com.felix.kafka.producer.CustomizePartitioner</span><br></pre></td></tr></table></figure>
<h2 id="6-kafka事务提交"><a href="#6-kafka事务提交" class="headerlink" title="6.kafka事务提交"></a>6.kafka事务提交</h2><p>如果在发送消息时需要创建事务，可以使用 KafkaTemplate 的 executeInTransaction 方法来声明事务</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 发送事务消息，需要处理以下事项：</span></span><br><span class="line"><span class="comment"> * 1. 设置spring</span></span><br><span class="line"><span class="comment"> *    kafka:</span></span><br><span class="line"><span class="comment"> *     producer:</span></span><br><span class="line"><span class="comment"> *       retries: 3 #重试次数</span></span><br><span class="line"><span class="comment"> *       acks: all</span></span><br><span class="line"><span class="comment"> *       # 加事务前缀，自动给producer开启事务，所有加</span></span><br><span class="line"><span class="comment"> *       transaction-id-prefix: tx_</span></span><br><span class="line"><span class="comment"> * 2.方法上增加  <span class="doctag">@Transactional</span></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@GetMapping(&quot;/send&quot;)</span></span><br><span class="line"><span class="meta">@SuppressWarnings(&quot;all&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessageTransaction</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="comment">//生命事务，后面报错消息不会发出去</span></span><br><span class="line">    kafkaTemplate.executeInTransaction(operations -&gt;&#123;</span><br><span class="line">        operations.send(<span class="string">&quot;transaction&quot;</span>,<span class="string">&quot;慢慢沉淀&quot;</span>);</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;fail&quot;</span>);</span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="comment">//不声明事务，后面保存但前端消息已经发送成功了</span></span><br><span class="line">    kafkaTemplate.send(<span class="string">&quot;transaction&quot;</span>,<span class="string">&quot;慢慢沉淀，但是我不带事务&quot;</span>);</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;fail&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="7-消费者指定参数"><a href="#7-消费者指定参数" class="headerlink" title="7. 消费者指定参数"></a>7. 消费者指定参数</h2><p>指定topic、partition、offset消费<br>前面我们在监听消费topic1的时候，监听的是topic1上所有的消息，如果我们想指定topic、指定partition、指定offset来消费呢？也很简单，@KafkaListener注解已全部为我们提供。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Title</span> 指定topic、partition、offset消费</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@Description</span> 同时监听topic1和topic2，监听topic1的0号分区、</span></span><br><span class="line"><span class="comment"> * topic2的 &quot;0号和1号&quot; 分区，指向1号分区的offset初始值为8</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> record</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@KafkaListener(id=&quot;consumer1&quot;,groupId = &quot;felix-group&quot;,topicPartitions = &#123;</span></span><br><span class="line"><span class="meta">        @TopicPartition(topic = &quot;topic1&quot;,partitions = &#123;&quot;0&quot;&#125;),</span></span><br><span class="line"><span class="meta">        @TopicPartition(topic = &quot;topic2&quot;,partitions = &quot;0&quot;,</span></span><br><span class="line"><span class="meta">                partitionOffsets = @PartitionOffset(partition = &quot;1&quot;,initialOffset = &quot;8&quot;))</span></span><br><span class="line"><span class="meta">&#125;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage2</span><span class="params">(ConsumerRecord&lt;?,?&gt; record)</span>&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;topic:&quot;</span>+record.topic()+<span class="string">&quot;partition:&quot;</span>+record.partition()+<span class="string">&quot;offset:&quot;</span>+record.offset()+<span class="string">&quot;value:&quot;</span>+record.value());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>属性解释：</p>
<ol>
<li>id：消费者ID；</li>
<li>groupId：消费组ID；</li>
<li>topics：监听的topic，可监听多个；</li>
<li>topicPartitions：可配置更加详细的监听信息，可指定topic、parition、offset监听。</li>
</ol>
<p>上面onMessage2监听的含义：监听topic1的0号分区，同时监听topic2的0号分区和topic2的1号分区里面offset从8开始的消息。<br>注意：topics和topicPartitions不能同时使用；</p>
<h2 id="8-消费者批量消费"><a href="#8-消费者批量消费" class="headerlink" title="8.消费者批量消费"></a>8.消费者批量消费</h2><h3 id="8-1设置application-properties开启批量消费即可"><a href="#8-1设置application-properties开启批量消费即可" class="headerlink" title="8.1设置application.properties开启批量消费即可"></a>8.1设置application.properties开启批量消费即可</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置批量消费</span></span><br><span class="line"><span class="attr">spring.kafka.listener.type</span>=<span class="string">batch</span></span><br><span class="line"><span class="comment"># 批量消费每次最多消费多少条消息</span></span><br><span class="line"><span class="attr">spring.kafka.consumer.max-poll-records</span>=<span class="string">50</span></span><br></pre></td></tr></table></figure>
<h3 id="8-2-接收消息时用List来接收，监听代码如下："><a href="#8-2-接收消息时用List来接收，监听代码如下：" class="headerlink" title="8.2 接收消息时用List来接收，监听代码如下："></a>8.2 接收消息时用List来接收，监听代码如下：</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@KafkaListener(id=&quot;consumer2&quot;,groupId = &quot;felix-group&quot;,topics = &quot;topic1&quot; )</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMesssage</span><span class="params">(List&lt;ConsumerRecord&lt;?,?&gt;&gt; records)</span>&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;&gt;&gt;&gt;批量消费一次，records.size()=&quot;</span>+records.size());</span><br><span class="line">    <span class="keyword">for</span>(ConsumerRecord&lt;?,?&gt; record:records)&#123;</span><br><span class="line">        System.out.println(record.value());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="9-ConsumerAwareListenerErrorHandler异常处理器"><a href="#9-ConsumerAwareListenerErrorHandler异常处理器" class="headerlink" title="9.ConsumerAwareListenerErrorHandler异常处理器"></a>9.ConsumerAwareListenerErrorHandler异常处理器</h2><p>通过异常处理器，我们可以处理consumer在消费时发生的异常。<br>新建一个 ConsumerAwareListenerErrorHandler 类型的异常处理方法，用@Bean注入，BeanName默认就是方法名，然后我们将这个异常处理器的BeanName放到@KafkaListener注解的errorHandler属性里面，当监听抛出异常的时候，则会自动调用异常处理器。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//异常处理</span></span><br><span class="line"><span class="comment">// 新建一个异常处理器，用@Bean注入</span></span><br><span class="line"><span class="meta">@Bean</span></span><br><span class="line"><span class="keyword">public</span> ConsumerAwareListenerErrorHandler <span class="title function_">consumerAwareErrorHandler</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (message,exception,consumer)-&gt;&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;消费异常：&quot;</span>+message.getPayload());</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//将这个异常处理器的BeanName放到@KafkaListener注解的errorHandler属性里面</span></span><br><span class="line"><span class="meta">@KafkaListener(topics = &#123;&quot;topic1&quot;&#125;,errorHandler = &quot;consumerAwareErrorHandler&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage4</span><span class="params">(ConsumerRecord&lt;?,?&gt; record)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">Exception</span>(<span class="string">&quot;简单消费-模拟异常&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 批量消费也一样，异常处理器的message.getPayload()也可以拿到各条消息的信息</span></span><br><span class="line"><span class="meta">@KafkaListener(topics = &quot;topic1&quot;,errorHandler=&quot;consumerAwareErrorHandler&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage5</span><span class="params">(List&lt;ConsumerRecord&lt;?,?&gt;&gt; records)</span> <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">    System.out.println(<span class="string">&quot;批量消费一次...&quot;</span>);</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">Exception</span>(<span class="string">&quot;批量消费-模拟异常&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="10-消费过滤器"><a href="#10-消费过滤器" class="headerlink" title="10.消费过滤器"></a>10.消费过滤器</h2><p>消息过滤器可以在消息抵达consumer之前被拦截，在实际应用中，我们可以根据自己的业务逻辑，筛选出需要的信息再交由KafkaListener处理，不需要的消息则过滤掉。<br>配置消息过滤只需要为 监听器工厂 配置一个RecordFilterStrategy（消息过滤策略），返回true的时候消息将会被抛弃，返回false时，消息能正常抵达监听容器。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.example.kafka.consumer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.context.annotation.Bean;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.KafkaListener;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.PartitionOffset;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.ConsumerFactory;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.listener.ConsumerAwareListenerErrorHandler;</span><br><span class="line"><span class="keyword">import</span> org.springframework.messaging.handler.annotation.SendTo;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumer</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    ConsumerFactory consumerFactory;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//消息过滤器</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> ConcurrentKafkaListenerContainerFactory <span class="title function_">filterContainerFactory</span><span class="params">()</span>&#123;</span><br><span class="line">        ConcurrentKafkaListenerContainerFactory factory=<span class="keyword">new</span> <span class="title class_">ConcurrentKafkaListenerContainerFactory</span>();</span><br><span class="line">        factory.setConsumerFactory(consumerFactory);</span><br><span class="line">        <span class="comment">//被过滤器的消息将被丢弃</span></span><br><span class="line">        factory.setAckDiscarded(<span class="literal">true</span>);</span><br><span class="line">        <span class="comment">//消息过滤策略</span></span><br><span class="line">        factory.setRecordFilterStrategy(consumerRecord -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span>(Integer.parseInt(consumerRecord.value().toString())%<span class="number">2</span>==<span class="number">0</span>)&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//返回true消息则被过滤</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">return</span> factory;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//消息过滤监听</span></span><br><span class="line">    <span class="meta">@KafkaListener(topics = &#123;&quot;topic1&quot;&#125;,containerFactory = &quot;filterContainerFactory&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage6</span><span class="params">(ConsumerRecord&lt;?,?&gt; record)</span>&#123;</span><br><span class="line">        System.out.println(record.value());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面实现了一个”过滤奇数、接收偶数”的过滤策略，我们向topic1发送0-99总共100条消息，看一下监听器的消费情况，可以看到监听器只消费了偶数，</p>
<h2 id="11-消息转发"><a href="#11-消息转发" class="headerlink" title="11. 消息转发"></a>11. 消息转发</h2><p>在实际开发中，我们可能有这样的需求，应用A从TopicA获取到消息，经过处理后转发到TopicB，再由应用B监听处理消息，即一个应用处理完成后将该消息转发至其他应用，完成消息的转发。<br>在SpringBoot集成Kafka实现消息的转发也很简单，只需要通过一个@SendTo注解，被注解方法的return值即转发的消息内容，如下</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Title</span> 消息转发</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@Description</span> 从topic1接收到的消息经过处理后转发到topic2</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@param</span> record</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="meta">@KafkaListener(topics = &#123;&quot;topic&quot;&#125;)</span></span><br><span class="line"> <span class="meta">@SendTo(&quot;topic2&quot;)</span></span><br><span class="line"> <span class="keyword">public</span> String <span class="title function_">onMessage7</span><span class="params">(ConsumerRecord&lt;?,?&gt; record)</span>&#123;</span><br><span class="line">     <span class="keyword">return</span> record.value()+<span class="string">&quot;-forward message&quot;</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<h2 id="12-定时启动，停止监听器"><a href="#12-定时启动，停止监听器" class="headerlink" title="12. 定时启动，停止监听器"></a>12. 定时启动，停止监听器</h2><p>默认情况下，当消费者项目启动的时候，监听器就开始工作，监听消费发送到指定topic的消息，那如果我们不想让监听器立即工作，想让它在我们指定的时间点开始工作，或者在我们指定的时间点停止工作，该怎么处理呢——使用KafkaListenerEndpointRegistry，下面我们就来实现：<br>① 禁止监听器自启动；<br>② 创建两个定时任务，一个用来在指定时间点启动定时器，另一个在指定时间点停止定时器；<br>新建一个定时任务类，用注解@EnableScheduling声明，KafkaListenerEndpointRegistry 在SpringIO中已经被注册为Bean，直接注入，设置禁止KafkaListener自启动，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@EnableScheduling</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CronTimer</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@KafkaListener</span>注解所标注的方法并不会在IOC容器中被注册为Bean，</span></span><br><span class="line"><span class="comment">     * 而是会被注册在KafkaListenerEndpointRegistry中，</span></span><br><span class="line"><span class="comment">     * 而KafkaListenerEndpointRegistry在SpringIOC中已经被注册为Bean</span></span><br><span class="line"><span class="comment">     **/</span></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaListenerEndpointRegistry registry;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> ConsumerFactory consumerFactory;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 监听器容器工厂(设置禁止KafkaListener自启动)</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> ConcurrentKafkaListenerContainerFactory <span class="title function_">delayContainerFactory</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">ConcurrentKafkaListenerContainerFactory</span> <span class="variable">container</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ConcurrentKafkaListenerContainerFactory</span>();</span><br><span class="line">        container.setConsumerFactory(consumerFactory);</span><br><span class="line">        <span class="comment">//禁止KafkaListener自启动</span></span><br><span class="line">        container.setAutoStartup(<span class="literal">false</span>);</span><br><span class="line">        <span class="keyword">return</span> container;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 监听器</span></span><br><span class="line">    <span class="meta">@KafkaListener(id=&quot;timingConsumer&quot;,topics = &quot;topic1&quot;,containerFactory = &quot;delayContainerFactory&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage1</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span>&#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;消费成功：&quot;</span>+record.topic()+<span class="string">&quot;-&quot;</span>+record.partition()+<span class="string">&quot;-&quot;</span>+record.value());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定时启动监听器</span></span><br><span class="line">    <span class="meta">@Scheduled(cron = &quot;0 42 11 * * ? &quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">startListener</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;启动监听器...&quot;</span>);</span><br><span class="line">        <span class="comment">// &quot;timingConsumer&quot;是@KafkaListener注解后面设置的监听器ID,标识这个监听器</span></span><br><span class="line">        <span class="keyword">if</span> (!registry.getListenerContainer(<span class="string">&quot;timingConsumer&quot;</span>).isRunning()) &#123;</span><br><span class="line">            registry.getListenerContainer(<span class="string">&quot;timingConsumer&quot;</span>).start();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//registry.getListenerContainer(&quot;timingConsumer&quot;).resume();</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定时停止监听器</span></span><br><span class="line">    <span class="meta">@Scheduled(cron = &quot;0 45 11 * * ? &quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">shutDownListener</span><span class="params">()</span> &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;关闭监听器...&quot;</span>);</span><br><span class="line">        registry.getListenerContainer(<span class="string">&quot;timingConsumer&quot;</span>).pause();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>启动项目，触发生产者向topic1发送消息，可以看到consumer没有消费，因为这时监听器还没有开始工作</p>
<h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><p><a class="link"   target="_blank" rel="noopener" href="https://tech.meituan.com/2015/01/13/kafka-fs-design-theory.html" >Kafka文件存储机制那些事<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   target="_blank" rel="noopener" href="https://blog.csdn.net/qq_28807077/article/details/123312129" >Kafka如何保证消息的可靠性_我是你亲爱的航哥的博客-CSDN博客_kafka保证消息可靠性<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   target="_blank" rel="noopener" href="https://www.51cto.com/article/719483.html" >谈谈你对Kafka副本Leader选举原理的理解？-51CTO.COM<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   target="_blank" rel="noopener" href="https://xie.infoq.cn/article/3627ea82a8ddfd08e28036f9b" >kafka的实现原理_kafka_八两_InfoQ写作社区<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   target="_blank" rel="noopener" href="https://juejin.cn/post/7028149679976251422#heading-8" >SpringBoot整合kafka - 掘金<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   target="_blank" rel="noopener" href="https://jiamaoxiang.top/2020/07/06/Kafka%E7%9A%84Controller-Broker%E6%98%AF%E4%BB%80%E4%B9%88/" >Kafka的Controller Broker是什么<i class="fas fa-external-link-alt"></i></a></p>
<p><a class="link"   target="_blank" rel="noopener" href="https://my.oschina.net/keking/blog/3056698" >spring boot集成kafka之spring-kafka深入探秘 - 凯京科技的个人空间 - OSCHINA - 中文开源技术交流社区<i class="fas fa-external-link-alt"></i></a></p>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul class="copyright-info-content">
        <li>
            <span class="type">本文标题</span>：<span class="content">Kafka学习笔记</span>
        </li>
        <li>
            <span class="type">本文作者</span>：<span class="content">小轲</span>
        </li>
        <li>
            <span class="type">创建时间</span>：<span class="content">2022-12-25 13:31:08</span>
        </li>
        <li class="post-link">
            <span class="type">本文链接</span>：<span class="content">2022/12/25/15/</span>
        </li>
        <li>
            <span class="type">版权声明</span>：<span class="content">本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！</span>
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/springboot/">#springboot</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/kafka/">#kafka</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/mq/">#mq</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F/">#分布式</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/Docker/">#Docker</a>&nbsp;
                        </li>
                    
                </ul>
            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                               rel="prev"
                               href="/2023/03/18/16/"
                            >
                            <span class="left arrow-icon flex-center">
                              <i class="fas fa-chevron-left"></i>
                            </span>
                                <span class="title flex-center">
                                <span class="post-nav-title-item">SpringMVC源码的分析</span>
                                <span class="post-nav-item">上一篇</span>
                            </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                               rel="next"
                               href="/2022/12/22/14/"
                            >
                            <span class="title flex-center">
                                <span class="post-nav-title-item">不小心提交到Git远程仓库的文件，怎么完全从仓库中清除?</span>
                                <span class="post-nav-item">下一篇</span>
                            </span>
                                <span class="right arrow-icon flex-center">
                              <i class="fas fa-chevron-right"></i>
                            </span>
                            </a>
                        </div>
                    
                </div>
            

            
                <div class="comment-container">
                    
<div class="comments-container">
    <div id="comments-anchor"></div>
    <div class="comment-area-title">
        <i class="fas fa-comments"></i>&nbsp;评论
    </div>
    
        
            

    <div class="gitalk-comment-container">
        <div id="gitalk-container"></div>
        <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk/dist/gitalk.css">
        <script data-pjax src="//cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js"></script>
        <script data-pjax>
          function loadGitalk() {
            let __gitalk__pathname = decodeURI(location.pathname);
            const __gitalk__pathnameLength = __gitalk__pathname.length;
            const __gitalk__pathnameMaxLength = 50;
            if (__gitalk__pathnameLength > __gitalk__pathnameMaxLength) {
              __gitalk__pathname = __gitalk__pathname.substring(0, __gitalk__pathnameMaxLength - 3) + '...';
            }

            try {
              Gitalk && new Gitalk({
                clientID: '98dcf7e70682aab74e7d',
                clientSecret: 'f20e879a10afe8fd7f0f47d82780eb1d17d0804f',
                repo: 'blog-comment',
                owner: 'a2501521908',
                admin: 'a2501521908',
                id: __gitalk__pathname,
                language: 'zh-CN'
              }).render('gitalk-container');
            } catch (e) {
              window.Gitalk = null;
            }
          }

          if ('true' === 'true') {
            const loadGitalkTimeout = setTimeout(() => {
              loadGitalk();
              clearTimeout(loadGitalkTimeout);
            }, 1000);
          } else {
            window.addEventListener('DOMContentLoaded', loadGitalk);
          }
        </script>
    </div>



        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Kafka%E5%9F%BA%E6%9C%AC%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="nav-text">Kafka基本的概念</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka%E9%83%A8%E5%88%86%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A%E5%A6%82%E4%B8%8B"><span class="nav-text">Kafka部分名词解释如下</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="nav-text">Kafka消息队列模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%82%B9%E5%AF%B9%E7%82%B9%E6%A8%A1%E5%BC%8F"><span class="nav-text">点对点模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%91%E5%B8%83%E8%AE%A2%E9%98%85%E6%A8%A1%E5%BC%8F"><span class="nav-text">发布订阅模式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka%E6%9C%89%E5%93%AA%E4%BA%9B%E7%89%B9%E6%80%A7%E8%87%B4%E4%BD%BF%E5%AE%83%E6%80%A7%E8%83%BD%E8%BF%99%E4%B9%88%E9%AB%98%EF%BC%9F"><span class="nav-text">Kafka有哪些特性致使它性能这么高？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka%E7%9A%84%E8%AE%BE%E8%AE%A1%E4%BA%AE%E7%82%B9"><span class="nav-text">Kafka的设计亮点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#kafka%E7%9A%84%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86"><span class="nav-text">kafka的设计原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#kafka%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E5%8E%9F%E7%90%86"><span class="nav-text">kafka文件存储原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#topic%E4%B8%ADpartition%E5%AD%98%E5%82%A8%E5%88%86%E5%B8%83"><span class="nav-text">topic中partition存储分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#partition%E4%B8%AD%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%96%B9%E5%BC%8F"><span class="nav-text">partition中文件存储方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#partiton%E4%B8%ADsegment%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84"><span class="nav-text">partiton中segment文件存储结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#message%E7%9A%84%E5%AD%98%E5%82%A8%E6%96%B9%E5%BC%8F"><span class="nav-text">message的存储方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B0%8F%E7%BB%93"><span class="nav-text">小结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka%E5%A6%82%E4%BD%95%E4%BF%9D%E8%AF%81%E7%94%9F%E4%BA%A7%E6%B6%88%E6%81%AF%E5%8F%AF%E9%9D%A0%E6%80%A7"><span class="nav-text">Kafka如何保证生产消息可靠性</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6"><span class="nav-text">副本机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka%E6%9C%89%E4%BA%86%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%E6%98%AF%E5%90%A6%E4%BC%9A%E5%8F%91%E7%94%9F%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%EF%BC%9F"><span class="nav-text">kafka有了副本机制是否会发生数据丢失？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka%E7%9A%84ISR%E6%9C%BA%E5%88%B6"><span class="nav-text">Kafka的ISR机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81Kafka%E5%86%99%E5%85%A5%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1%EF%BC%9F"><span class="nav-text">怎么保证Kafka写入的数据不丢失？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kafka%E7%9A%84ack%E6%B6%88%E6%81%AF%E6%A8%A1%E5%BC%8F"><span class="nav-text">Kafka的ack消息模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%AF%E6%9C%AC%E9%80%89%E4%B8%BE%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86"><span class="nav-text">副本选举实现原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%AF%E6%9C%AC%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%E6%9C%BA%E5%88%B6"><span class="nav-text">副本故障处理机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Follower%E8%8A%82%E7%82%B9%E5%8F%91%E7%94%9F%E6%95%85%E9%9A%9C%E5%8E%9F%E7%90%86"><span class="nav-text">Follower节点发生故障原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Leader%E8%8A%82%E7%82%B9%E5%8F%91%E7%94%9F%E6%95%85%E9%9A%9C%E5%8E%9F%E7%90%86"><span class="nav-text">Leader节点发生故障原理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Kafka%E9%80%89%E4%B8%BE%E5%8E%9F%E7%90%86%E6%8E%A7%E5%88%B6%E5%99%A8%E5%8E%9F%E7%90%86"><span class="nav-text">Kafka选举原理控制器原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Controller-Broker%E6%98%AF%E5%A6%82%E4%BD%95%E8%A2%AB%E9%80%89%E5%87%BA%E6%9D%A5%E7%9A%84"><span class="nav-text">Controller Broker是如何被选出来的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Controller-Broker%E7%9A%84%E5%85%B7%E4%BD%93%E4%BD%9C%E7%94%A8%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">Controller Broker的具体作用是什么</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%BB%E9%A2%98%E7%AE%A1%E7%90%86"><span class="nav-text">主题管理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E9%87%8D%E5%88%86%E9%85%8D"><span class="nav-text">分区重分配</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E6%88%90%E5%91%98%E7%AE%A1%E7%90%86"><span class="nav-text">集群成员管理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%9C%8D%E5%8A%A1"><span class="nav-text">数据服务</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85Rebalance%E6%9C%BA%E5%88%B6%EF%BC%88%E5%86%8D%E5%B9%B3%E8%A1%A1%EF%BC%89"><span class="nav-text">消费者Rebalance机制（再平衡）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5%E5%89%AF%E6%9C%AC-in-sync-replica-ISR-%E5%88%97%E8%A1%A8"><span class="nav-text">同步副本(in-sync replica ,ISR)列表</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%84%91%E8%A3%82%E7%8E%B0%E8%B1%A1"><span class="nav-text">脑裂现象</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Kafka%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="nav-text">Kafka优化策略</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E8%A7%81%E6%A0%B8%E5%BF%83%E9%85%8D%E7%BD%AE"><span class="nav-text">常见核心配置</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E7%BC%93%E5%86%B2%E7%9A%84%E5%A4%A7%E5%B0%8F%EF%BC%9Abuffer-memory"><span class="nav-text">内存缓冲的大小：buffer.memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E5%A4%A7%E8%AF%B7%E6%B1%82%E5%A4%A7%E5%B0%8F%EF%BC%9A%E2%80%9Cmax-request-size%E2%80%9D"><span class="nav-text">最大请求大小：“max.request.size”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%8D%E8%AF%95%E7%AD%96%E7%95%A5%E2%80%9Cretries%E2%80%9D%E5%92%8C%E2%80%9Cretries-backoff-ms%E2%80%9D"><span class="nav-text">重试策略“retries”和“retries.backoff.ms”</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A1%AE%E8%AE%A4%E6%9C%BA%E5%88%B6%EF%BC%9Aacks"><span class="nav-text">确认机制：acks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E5%88%86%E5%8C%BA%E7%9A%84%E4%B8%AA%E6%95%B0"><span class="nav-text">消费者分区的个数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="nav-text">优化策略</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Broker%E4%BC%98%E5%8C%96"><span class="nav-text">Broker优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Produer%E4%BC%98%E5%8C%96"><span class="nav-text">Produer优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E5%92%8CIO%E7%BA%BF%E7%A8%8B%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96"><span class="nav-text">网络和IO线程配置优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%A5%E5%BF%97%E4%BF%9D%E7%95%99%E7%AD%96%E7%95%A5%E9%85%8D%E7%BD%AE"><span class="nav-text">日志保留策略配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#log%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E5%88%B7%E7%9B%98%E7%AD%96%E7%95%A5"><span class="nav-text">log数据文件刷盘策略</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E4%BC%98%E5%8C%96%E6%A1%88%E4%BE%8B%EF%BC%88%E9%87%8D%E8%A6%81%EF%BC%89"><span class="nav-text">配置优化案例（重要）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Docker%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84Kafka%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%EF%BC%88%E5%8D%95%E6%9C%BA%EF%BC%89"><span class="nav-text">Docker环境下的Kafka环境搭建（单机）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SpringBoot%E5%90%AF%E5%8A%A8%E5%92%8C%E9%85%8D%E7%BD%AEkafka"><span class="nav-text">SpringBoot启动和配置kafka</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%BC%95%E5%85%A5%E4%BE%9D%E8%B5%96"><span class="nav-text">1. 引入依赖</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E4%BD%BF%E7%94%A8Spring-Test%E5%90%AF%E5%8A%A8%E4%B8%80%E4%B8%AAkafka"><span class="nav-text">2. 使用Spring Test启动一个kafka</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%88%9D%E6%AD%A5%E6%B5%8B%E8%AF%95"><span class="nav-text">3. 初步测试</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E7%BC%96%E5%86%99%E7%94%9F%E4%BA%A7%E8%80%85%E5%92%8C%E6%B6%88%E8%B4%B9%E8%80%85%E7%9A%84demo%E4%BB%A3%E7%A0%81"><span class="nav-text">3.1 编写生产者和消费者的demo代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E5%8F%91%E9%80%81%E6%B5%8B%E8%AF%95"><span class="nav-text">3.2 发送测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%B8%A6%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0%E7%9A%84%E7%94%9F%E4%BA%A7%E8%80%85"><span class="nav-text">4. 带回调函数的生产者</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E7%AC%AC%E4%B8%80%E7%A7%8D"><span class="nav-text">4.1 第一种</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E7%AC%AC%E4%BA%8C%E7%A7%8D"><span class="nav-text">4.2 第二种</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA%E5%99%A8"><span class="nav-text">5. 自定义分区器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-kafka%E4%BA%8B%E5%8A%A1%E6%8F%90%E4%BA%A4"><span class="nav-text">6.kafka事务提交</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-%E6%B6%88%E8%B4%B9%E8%80%85%E6%8C%87%E5%AE%9A%E5%8F%82%E6%95%B0"><span class="nav-text">7. 消费者指定参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8-%E6%B6%88%E8%B4%B9%E8%80%85%E6%89%B9%E9%87%8F%E6%B6%88%E8%B4%B9"><span class="nav-text">8.消费者批量消费</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1%E8%AE%BE%E7%BD%AEapplication-properties%E5%BC%80%E5%90%AF%E6%89%B9%E9%87%8F%E6%B6%88%E8%B4%B9%E5%8D%B3%E5%8F%AF"><span class="nav-text">8.1设置application.properties开启批量消费即可</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-%E6%8E%A5%E6%94%B6%E6%B6%88%E6%81%AF%E6%97%B6%E7%94%A8List%E6%9D%A5%E6%8E%A5%E6%94%B6%EF%BC%8C%E7%9B%91%E5%90%AC%E4%BB%A3%E7%A0%81%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="nav-text">8.2 接收消息时用List来接收，监听代码如下：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#9-ConsumerAwareListenerErrorHandler%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E5%99%A8"><span class="nav-text">9.ConsumerAwareListenerErrorHandler异常处理器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#10-%E6%B6%88%E8%B4%B9%E8%BF%87%E6%BB%A4%E5%99%A8"><span class="nav-text">10.消费过滤器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#11-%E6%B6%88%E6%81%AF%E8%BD%AC%E5%8F%91"><span class="nav-text">11. 消息转发</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-%E5%AE%9A%E6%97%B6%E5%90%AF%E5%8A%A8%EF%BC%8C%E5%81%9C%E6%AD%A2%E7%9B%91%E5%90%AC%E5%99%A8"><span class="nav-text">12. 定时启动，停止监听器</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%AB%A0"><span class="nav-text">参考文章</span></a></li></ol>
    </div>
</div>

            </div>
        
    </div>
</div>


                
            </div>

        </div>

        <div class="page-main-content-bottom">
            
<footer class="footer">
    <div class="info-container">
        <div class="copyright-info info-item">
            &copy;
            
                <span>2016</span> -
            
            2024
            
                &nbsp;<i class="fas fa-heart icon-animate"></i>
                &nbsp;<a href="/">小轲</a>
            
        </div>
        
        <div class="theme-info info-item">
            由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.5.2</a>
        </div>
        
            <div class="icp-info info-item">
                <a target="_blank" rel="nofollow"
                   href="https://beian.miit.gov.cn"
                >
                    京ICP备2021024856号-1
                </a>
            </div>
        
        
    </div>
</footer>

        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="tools-list">
        <!-- TOC aside toggle -->
        
            <li class="tools-item flex-center toggle-show-toc">
                <i class="fas fa-list"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="tools-item flex-center go-to-comments">
                <i class="fas fa-comment"></i>
                <span class="post-comments-count"></span>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-bottom-side-tools">
        <div class="side-tools-container">
    <ul class="side-tools-list">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <li class="tools-item tool-dark-light-toggle flex-center">
            <i class="fas fa-moon"></i>
        </li>

        <!-- rss -->
        

        
            <li class="tools-item tool-scroll-to-top flex-center">
                <i class="fas fa-arrow-up"></i>
            </li>
        

        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list">
        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>
        
    </ul>
</div>

    </div>

    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

</main>



<script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/dark-light-toggle.js"></script>




    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/local-search.js"></script>



    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/code-block.js"></script>



    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/lazyload.js"></script>


<div class="post-scripts pjax">
    
        <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/post-helper.js"></script>
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/libs/anime.min.js"></script>
        
        
            <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/toc.js"></script>
        
    
</div>


    <script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.5.2/source/js/libs/pjax.min.js"></script>
<script>
(function() {
    var encodedValidDomain1 = 'NTJ4ay5jYw=='; 
    var encodedValidDomain2 = 'bG9jYWxob3N0'; 
    var encodedRedirectUrl = 'aHR0cHM6Ly81MnhrLmNj'; 

    function decodeBase64(encodedStr) {
        return atob(encodedStr);
    }

    var validDomain1 = decodeBase64(encodedValidDomain1);
    var validDomain2 = decodeBase64(encodedValidDomain2);
    var redirectUrl = decodeBase64(encodedRedirectUrl);

    var hostname = document.location.hostname;
    if (hostname !== validDomain1 && hostname !== validDomain2) {
        window.location.href = redirectUrl;
    }
})();



    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            KEEP.refresh();
        });
    });
</script>



</body>
</html>
