[{"title":"2019给自己的年终总结","url":"/2019/12/31/10/","content":"开头:新的一年新的气象，我看各个博主都已经开始给自己立了一个新年的总结，下午前脚把代码Push上，我就过来给自己一个总结，毕竟觉得自己是挺重要的一年！\n\n因为自己是做技术的，先总结一下自己的技术:\n技术:今年是个提升自己的一年，为什么这么说呢？ 自己的编码水平得到了突飞猛进的进展(目前自认为)。因为之前自己的水平回忆起来真的是不堪回首。\n新的一年，掌握了新的微服务框架技术、Docker容器化技术、以及Java相关的常用化组件等等…..当然2020年我还会继续向这种高用户量的需求解决框架延申，为自己扩展提升更好的技能。\n旅游:今年的旅游收获了很多的好心情，并且发现去一个很美的地方可以让人快速的修复自己疲惫的心灵，也跟自己的好哥们第一次去人生中的海边，旅游没得说，之后有机会我就一定会选择去旅游，也还有自己在北京觉得很好看的地方。\n日出\n\n海边:\n海洋馆:\n冰面:\n在北京的记忆:\n\n\n工作:今年年底入职北京的一家公司担任  Java开发工程师  一职 ，目前的项目为: “金库管理平台”，当然，我只是一个这个面向B端产品的一个小蚂蚁，新的一年我也会更加努力的去做好我的本职工作，努力提升自己。好好生活，对自己要好一点！家人都不在身边，只能自己照顾好自己，嘻嘻~\n博客:在2019年，自己没有抽出来时间管理自己的发霉的小窝。但是我一直有关注。我、以及我的朋友们的博客似乎都缺少了时间来管理自己的博客，因为回想起来从17年到现在了，大家有的站点还在坚持更新记录。有的已经遗憾的Get不到了，我也好几次没有打理出来自己的博客，不过我还是坚持过来了！\n不管怎么样希望看见这篇总结的朋友 新的一年前程似锦。\n往前的目标我就更清楚了。新的一年，我要抽出来更多的时间来管理自己的博客，记录自己的生活。\n总结:扯了这么多，把自己的2019年的flag拿出来，对比一下。\n1.在北京实习找到一份稳定的饭碗。  √\n2.去一次旅游(见一个人) 不知道能不能   ×   我想之后可能也不会了。\n3.希望在年底学会开车   嗯，这个已经在练习考科一做题了，年假回家学习开车。\n新的一年对自己说的话:\n2020,张帅轲 希望你好好努力，要对自己的技术要求更严格，对自己的提升永远不要放下。不辜负自己、朋友、家人才是最重要的，加油。\n","tags":["总结"]},{"title":"29.22分钟学会正则表达式","url":"/2018/11/22/7/","content":"写在最前面看到标题你可能会疑惑为什么不是30分钟？因为我这个文章图文并茂，非常恐怖，兄弟，其实你不用30分钟就可以看懂。你可能会以为我在吹牛B，但是当你看完的时候，一掐表，你会发现我真的是在吹牛B那又为什么是.22呢？作为一个理科生，保留两位小数是不变的信仰。而在下，仅仅是喜欢2这个数字，如是而已\n正则表达式正则表达式，又称规则表达式。（英语：Regular Expression，在代码中常简写为regex、regexp或RE），计算机科学的一个概念。正则表达式通常被用来检索、替换、校验那些符合某个模式(规则)的文本。\nRegExp对象在爪洼死苦瑞per特中，RegExp 对象表示正则表达式，它是对字符串执行模式匹配的强大工具。那么要如何使用呢？两种方式：字面量，构造函数\nvar reg = /\\bhello\\b/g  //字面量 // \\b代表单词边界（WordBoundary） 也就是说这个正则匹配的是 hello world这种hello 而不是helloworld//因为helloworld连起来了，没有单词边界\n\nvar reg = new RegExp(&#x27;\\\\bhello\\\\b&#x27;,&#x27;g&#x27;)//注意两者的区别//后面这种方法需要转义反斜杠（javascript的原因）,//而且这个g(修饰符,全局匹配)是单独提取出来的//而且正则两边没有/包围的，上面第一种是这样的=&gt; /正则表达式/\n\n正则可视化工具Regulex可视化图形，对理解正则有非常大的帮助二话不说先进来这个网站，这个文章将使用这个网站来验证写的例子。\n元字符正则表达式由两种基本字符类组成\n\n原义字符\n元字符\n\n原义字符，就是表示原本意思的字符，像上面正则中的hello，就代表匹配hello这个字符串元字符呢，就是表示不是原本意思的字符，这样想就简单多了吧。像上面这个\\b\n\n\n既然元字符表示的不是本身的字符，那我如果就要匹配它原本的字符呢？比如说我就要匹配+号，*号，那么请使用 \\ 来转义字符\n下面这些元字符先随便过一遍先，不用背熟也可往下看~\n\n$ 匹配输入字符串的结尾位置。如果设置了 RegExp 对象的 Multiline 属性，则 $ 也匹配 ‘n’ 或 ‘r’。要匹配 $ 字符本身，请使用 $。\n() 标记一个子表达式的开始和结束位置。子表达式可以获取供以后使用。要匹配这些字符，请使用 ( 和 )。\n* 匹配前面的子表达式零次或多次。要匹配 * 字符，请使用 *。\n+ 匹配前面的子表达式一次或多次。要匹配 + 字符，请使用 +。\n. 匹配除换行符 n 之外的任何单字符。要匹配 . ，请使用 . 。\n[] 标记一个中括号表达式的开始。要匹配 [，请使用 [。\n{} 标记限定符表达式的开始。要匹配 {，请使用 {。\n| 指明两项之间的一个选择。要匹配 |，请使用 |。\n? 匹配前面的子表达式零次或一次，或指明一个非贪婪限定符。要匹配 ? 字符，请使用 ?。\n\\ 将下一个字符标记为或特殊字符、或原义字符、或向后引用、或八进制转义符。例如， ‘n’ 匹配字符 ‘n’。’n’ 匹配换行符。序列 ‘&#39; 匹配 “”，而 ‘(‘ 则匹配 “(“。\n^ 匹配输入字符串的开始位置，除非在方括号表达式中使用，此时它表示不接受该字符集合。要匹配 ^ 字符本身，请使用 ^。\n\\cX 匹配由x指明的控制字符。例如， cM 匹配一个 Control-M 或回车符。x 的值必须为 A-Z 或 a-z 之一。否则，将 c 视为一个原义的 ‘c’ 字符。\n\\f 匹配一个换页符。等价于 x0c 和 cL。\n\\n 匹配一个换行符。等价于 x0a 和 cJ。\n\\r 匹配一个回车符。等价于 x0d 和 cM。\n\\s 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 [ fnrtv]。注意 Unicode 正则表达式会匹配全角空格符。\n\\S 匹配任何非空白字符。等价于 1。\n\\t 匹配一个制表符。等价于 x09 和 cI。\n\\v 匹配一个垂直制表符。等价于 x0b 和 cK\n\n边界\n从一开始的例子我们就知道了这个b,不对，是这个\\b他表示的就是单词边界的意思.我们知道，fck这个是有很多用法的，可以单独用，也可以加个ing多种词性使用。然后我们只想找到单独的fck,看代码\n//作为光荣的社会主义接班人怎么可能用f*ck做例子呢?var reg = /\\bis\\b/g;var str = &quot;this is me&quot;;str.replace(reg,&#x27;X&#x27;)//&quot;this X me&quot;\n\nvar reg = /is/g;var str = &quot;this is me&quot;;str.replace(reg,&#x27;X&#x27;)//&quot;thX X me&quot;\n\n两者区别清晰可见，不容我多说了吧，各位客官。\n再来看看一个问题，如果我只要开头部分的A字符而文本中间的A字符却不要，又该如何？只需如此，便可对敌\nvar reg = /^A/g;var str = &quot;ABA&quot;;str.replace(reg,&#x27;X&#x27;);//&quot;XBA&quot;\n\n\n需要以A为结尾的正则，则是如下\nvar reg = /A$/g;var str = &quot;ABA&quot;;str.replace(reg,&#x27;X&#x27;);//&quot;ABX&quot;\n\n\n注意，正如开头结尾的位置一样，^和$的位置也是如此，^放在正则表达式前面，$放在表达式后面\n字符类一般情况下，正则表达式一个字符对应字符串的一个字符比如表达式 \\bhello 就表示 匹配 字符\\b h e l l o，\n如果我们想要匹配一类字符的时候?比如我要匹配a或者b或者c，我们就可以使用元字符 []来构建一个简单的类[a,b,c]就把a，b，c归为一类，表示可以匹配a或者b或者c。如果你会一丢丢英文的话，你应该就可以看懂下面的图，one of a，b，c，也就是匹配abc中任意一个~\n\n范围类当我们学习了上面的内容以后，如果我们要写匹配0到9的数字，就应该是这样写\n\n但是如果我要匹配更多呢？那不是键盘都要敲烂了？这正则也太不智能了吧？？？显然，你能想到的，创造正则的人也想到了我们可以这样子\n\n好了，方便了一些，然后你可能又会吃惊，那么我的短横线-呢？我如果要匹配0-9以及短横线呢？莫慌，只要在后面补回去即可这个图可以清楚看到有两条分支，也就是说我可以走0-9这条路也可以走短横线这条路\n\n\n预定义类学习了上面以后，我们就可以书写匹配数字的正则了，[0-9]\n那么有没有更简便更短的方法呢？\n巧了，正则就是辣么强大\n在上面的元字符部分内容中，你可能已经窥得其中精妙了\n上表格，不是，上图（这个segmentfault哪里插入表格啊？？）\n\n\n\n\n\n我们可以根据英文单词的意思，来记住这些预定义类的用法。我们发现，大写字母和小写字母的区别就是取反!，如d和D同时我们从表格中的等价类可以发现如果我们要一个类的取反，那么就在类中加一个 ^none of abc\n\n量词如果要你写一个匹配10个数字的正则？你会怎么写诶~你可能已经胸有成竹的写下了\n\\d\\d\\d\\d\\d\\d\\d\\d\\d\\d\n\n吃惊，你会发现，尽管是你单身二十余年的右手，依然感到了一丝乏力！疲惫，有时是在过度劳累之后为了挽救一些人的右臂，正则有了量词实现上面的需求我们只要 \\d{10}\nDigit 10times为了方便一些英语不好的人，比如我，我甚至使用了鲜为人知的百度翻译（广告费私我）\n\n但是，如果我不知道要匹配具体多少个数字呢？反正就是匹配100个到1000个之间的数字当当当当~\n\n让我们看看可视化工具的结果，方便理解\n注意，这个{n,m}是包括n次和m次的哦，是闭区间哦\n\n贪婪模式与非贪婪模式从上面一则我们知道，如果我们要匹配100到1000个数字的话，是这样写\\d{100,1000}如果我给的字符串里有1000个数字，但是我只想匹配前面100个呢？\n如果按照上面这样写，则如下\nvar reg = /\\d&#123;3,6&#125;/;var str = &quot;123456789&quot;;str.replace(reg,&#x27;替换成这个&#x27;);//&quot;替换成这个789&quot;\n\n我们可以看到，上面这个例子是匹配了6个数字，将6个数字替换了，尽管他的正则匹配的是3到6个数字。\n没错，它是贪婪的！它会尽可能地匹配更多！这就是正则的 贪婪匹配，这是默认的，如果我们不想要那么贪婪，如何变得容易满足一点？只需要在量词后面加上 ? 即可\nvar reg = /\\d&#123;3,6&#125;?/;var str = &quot;123456789&quot;;str.replace(reg,&#x27;替换成这个&#x27;);//&quot;替换成这个456789&quot;\n\n可以清楚看到正则只匹配了前面3个数字~这就是正则的非贪婪模式\n分支条件如果我只需要匹配100个或者1000个数字呢？就只有100和1000两种可能，而不是100到1000任意一个数字，又该如何对敌？这就要设计到正则的分支条件了\n\\d&#123;100&#125;|\\d&#123;1000&#125;\n\n\n需要注意的是这个 | 分割的是左右两边所有部分，而不是仅仅连着这个符号的左右两部分，看下图\n\n有时候我们只需要一部分是分支，后面走的是同一条主干，只需要把分支用()包含即可\n\n注意：这个匹配是从正则左边的分支条件开始的，如果左边满足了，那么右边就不会在对比！\nvar reg = /\\d&#123;4&#125;|\\d&#123;2&#125;/var str = &quot;12345&quot;str.replace(reg,&#x27;X&#x27;);// &quot;X5&quot;\n\nvar reg = /\\d&#123;2&#125;|\\d&#123;4&#125;/var str = &quot;12345&quot;str.replace(reg,&#x27;X&#x27;);//&quot;X345&quot;\n\n前瞻&#x2F;后顾sometimes，我们要找寻的字符可能还要依靠前后字符来确定比如说我要替换连续的2个数字，而且它的前面要连着是2个英文字母，这样的数字我才要你可能会疑惑,这样写不就完事了吗？\n\\d&#123;2&#125;\\w&#123;2&#125;\n\n上面匹配的是2个数字和2个字母，虽然是连着的，但是匹配了是4个字符，如果我要替换匹配文本的话，那就替换了4个字符，而我们只想替换2个数字！这个时候就需要用到断言了首先我们需要明白几个点\n\n正则表达式从文本头部到尾部开始解析，文本尾部方向叫做‘前’，也就是往前走，就是往尾巴走\n前瞻就是正则表达式匹配到规则（此例中的‘2个数字’）的时候，向前看看，看看是否符合断言（此例中的‘前面连着2个字母’），后瞻&#x2F;后顾的规则则相反。（javascript不支持后顾）\n\n上表格！\n\n根据表格内容，我们就可以解决这个问题了，注意\\w包括数字哦~题目要求是连着2个字母\n\nvar reg = /\\d&#123;2&#125;(?=[a-zA-Z]&#123;2&#125;)/;var str = &quot;1a23bc456def&quot;;str.replace(reg,&#x27;X&#x27;);//&quot;1aXbc456def&quot;\n\n只替换了数字，没有替换后面的断言哦！\n顺便把这个负向前瞻看看吧\n\n看到这个not followed by 我想你应该知晓用法了\n分组当我们要匹配一个出现三次的单词而不是数字的时候，会怎么写呢？你可能会这样写\nhello&#123;3&#125;\n\n然后你打开可视化工具\n\n妈耶，居然只重复了我的o字母！死渣则，好过分\n其实，我们只要使用（）就可以达到分组的目的，使量词作用于分组，上面分支条件中的括号亦是如此\n\n分组以后怎么使用分组内容呢？首先看一个问题，如何匹配8个不连续的数字？如果你不使用分组，你会发现根本无从下手，因为你不能判断出有无重复！我们先公布答案，再来分析一波\n\n\n首先，这个(?!负向前瞻断言A)表达式B，这里使用的是负向前瞻，也就是说断言A前面的内容（表达式B），不能符合表达式A，这个说法很拗口，我嘴巴都拗不过来了。能听明白吧，这个设计就是，我这个断言是“出现重复的数字”，然后表达式是“8个数字”，”8个数字“不能复合“出现重复的数字”\n然后，这个 .(\\d). 呢，是先找到一个在任意位置出现的数字，为什么是任意位置呢？因为我们判断的重复可能出现在任何位置；看上面的可视化也就可以明白，\\d前后有0-n个字符,所以说他是任意位置的。\n最关键的来了，这个\\1代表什么呢？仔细看你可以发现，\\d加了一个括号，这个括号就代表着分组，那么是几号分组呢？第一个括号就是分组1（默认情况下），如果还有第二个括号，那就是分组2，前瞻的括号是不算的噢，而这个\\1呢就代表着引用这个分组1，使用\\2引用分组2。你也许会好奇，我引用它是相当于在这个位置写了一个\\d吗？NOP，不仅仅这么简单，它引用的是这个\\d的内容，也就是说他会和\\d是一样的值！这不就是重复了吗？！！！这个 .(\\d).\\1 就代表着**任意位置出现了任意次数的重复\n最后，我们把这些整合在一起就是，匹配8个数字不能出现任意的重复。(?!出现任意重复)8个数字，因为这个(?!)是负向前瞻，所以。。。emmm。。这样就理解了吧。\n\n分组还有其他更详细的内容，但是篇幅有限，马上就到30分钟了。只好捡一些有价值常用的讲了~\n嘿嘿嘿正则就介绍到这里啦~\n"},{"title":"Arthas直接执行Spring Context中的函数","url":"/2023/04/13/17/","content":"简介Arthas提供了非常丰富的关于调用拦截的命令，比如 trace&#x2F;watch&#x2F;monitor&#x2F;tt 。但是很多时候我们在排查问题时，需要更多的线索，并不只是函数的参数和返回值。比如在一个Spring应用里，想获取到Spring context里的其它bean。如果能随意获取到Spring bean，那就可以“为所欲为”了。下面介绍如何利用Arthas获取到Spring context。\n操作步骤Spring MVC应用，请求会经过一系列的Spring Bean处理，那么我们可以在Spring MVC的类里拦截到一些请求。使用Arthas Attach成功之后，执行tt命令来记录RequestMappingHandlerAdapter#invokeHandlerMethod的请求，可以直接获取到Spring context了就\n1. 使用tt命令监控Spring RequestMappingHandlerAdapter中的类执行下列命令后，你需要做两件事\n\n触发http请求，让tt命令监听到RequestMappingHandlerAdapter\n监控控制台，看到下方有一条记录即可。获取到下列结果的INDEX值，比如1000[arthas@1]$ tt -t org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter invokeHandlerMethodPress Q or Ctrl+C to abort.结果：Affect(class count: 1 , method count: 1) cost in 323 ms, listenerId: 2 INDEX      TIMESTAMP                  COST(ms)      IS-RET     IS-EXP     OBJECT               CLASS                                    METHOD                                  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- 1000       2023-04-13 10:47:54        19.308463     true       false      0x39ae0bff           RequestMappingHandlerAdapter             invokeHandlerMethod                      1001       2023-04-13 10:47:54        34.872646     true       false      0x39ae0bff           RequestMappingHandlerAdapter             invokeHandlerMethod                     \n目前已经监听到了spring的bean执行了，下面来获取spring的context\n\n2. 获取Spring的context可以用tt命令的-i参数来指定index，并且用-w参数来执行ognl表达式来获取spring context：\ntt -i INDEX -w &#x27;target.getApplicationContext()&#x27;..........结果忽略Affect(row-cnt:1) cost in 7 ms.\n3. 从Spring context里获取任意bean获取到spring context之后，就可以获取到任意的bean了，比如获取到logBusinessOperateServiceImpl，并调用reloadEntityFields()函数：\n[arthas@1]$ tt -i 1000 -w &#x27;target.getApplicationContext().getBean(&quot;logBusinessOperateServiceImpl&quot;).reloadEntityFields()&#x27;结果：nullAffect(row-cnt:1) cost in 30 ms.\n如果需要传递参数的函数，则直接在调用方法的参数里直接写即可(按照方法重载的规则)，例如reloadEntityFields(&#39;admin&#39;,123,3.7F)\n更多的思路在很多代码里都有static函数或者static holder类，顺滕摸瓜，可以获取很多其它的对象。比如在Dubbo里通过SpringExtensionFactory获取spring context：\n$ ognl &#x27;#context=@com.alibaba.dubbo.config.spring.extension.SpringExtensionFactory@contexts.iterator.next, #context.getBean(&quot;userServiceImpl&quot;).findUser(1)&#x27;@User[    id=@Integer[1],    name=@String[Deanna Borer],]\n帮助地址\n\n原链接，GitHub issues： https://github.com/alibaba/arthas/issues/482，具体详细内容可在里面讨论。\n官方提供Demo： https://github.com/hengyunabc/spring-boot-inside/tree/master/demo-arthas-spring-boot\nArthas快速开始：https://arthas.aliyun.com/doc/quick-start.html\n\n","tags":["arthas","springboot","spring"]},{"title":"FastThreadLocal是不是真的快？","url":"/2025/04/13/27/","content":"什么是FastThreadLocal？FastThreadLocal是Netty中对JDK提供的ThreadLocal优化改造版本，从名称上来看，它应该比ThreadLocal更快了，以应对Netty处理并发量大、数据吞吐量大的场景。\nFastThreadLocal 是 ThreadLocal 的一种特殊变体，只有在使用 FastThreadLocalThread 线程类访问时，才能获得更高的访问性能。  \n\n 体现了程序员的一种极致“Geek精神”，连多循环几次都不愿意，必须做到最快。  \n\n\n概括来讲，FastThreadLocal快的核心其实是一个index。有了这个index，在get和remove方法的时候，就直接通过下标获取。可以看到下图中的get方法，拿对象是直接通过index获取的。\n源码分析源码分析从三个地方进行分析,new FastThreadLocal() 、fastThreadLocal.get()、fastThreadLocal.set(value)来进行分析，从增删改查来看它的实现。\nnew() -分配当前对象的index索引FastThreadLocal() 函数其实这一步，是为这个对象申请InternalThreadLocalMap索引的过程，相当于注册当前对象，得到一个唯一id\n\nnextVariableIndex() 申请索引单线程中都存在一个InternalThreadLocalMap实例，用来区分线程变量。这一步跟ThreadLocal很像。\n但是要保证绝对的线程安全，所以使用了CAS来进行比较并交换，保证了可靠性\n\nget() - 从FastThreadLocal中获取值get这一步看起来也比较简单，就是在new这个对象时获得这个索引，去用这个index查找数据\n\nInternalThreadLocalMap#get() 获得当前线程的InternalThreadLocalMap 这一步存在一个关键限制：必须使用 Netty 封装的线程类，普通 Thread 无法使用 FastThreadLocal  \n\nInternalThreadLocalMap#indexedVariable() 通过下标索引读取内容这一步其实更明了，就是通过先前new对象出来的index，来get这个InternalThreadLocalMap中数组的值，O(1)的时间复杂度即可\n\nset() 按照分配索引，放入该线程的数组中set 操作其实就是根据分配好的 index，将值存入当前线程的数组中。下面的remove（）逻辑其实是netty自己的清理逻辑，不需要我们处理\nsetKnownNotUnset(threadLocalMap, value) 往数组中放入值这一步其实就是拿当前被分配到的index，去调用当前线程内InternalThreadLocalMap的数组直接赋值。\n\nthreadLocalMap.setIndexedVariable(index, value) 真正执行的地方到这，进行一个可能会有old值替换过程，这一步是netty自己的业务逻辑，我们无需关注。\n\n总结为啥说是个噱头？\n✅ 在 Netty 这种 IO 框架里，它是真的快。\n❌ 自己搞不合适\n为什么？\n线程是自己控制的（全用 FastThreadLocalThread）\n用到的变量多，访问频率高\n线程复用频繁，GC 敏感\n\n\n\n是否真的变快了?\n变快肯定是变快的，因为并不需要有ThreadLocalMap的线性探测，直接O(1)就可以取到数据\n实现方式总结一下就是\n每个 FastThreadLocal 实例会分配一个全局唯一的 index（自增）\n每个线程维护一个 InternalThreadLocalMap，本质上是个 Object[]\n每个变量都直接按 index 存到 values[index] 上\n\n\n老的实现方式就是\n每一个线程都有一个ThreadLocalMap\n在get、set时，这个Map就根据当前key来往map中放入值\n当前key就是这个ThreadLocal对象，例UserThreadLocal、TraceThreadLocal\n\n\n如果遇到hash冲突了怎么办\n会进行线性查找，这也就是netty不满意的地方，怕万一线程中有几十个ThreadLocal的情况下，会有hash撞槽的可能，带来额外的性能开销。（其实就是多循环几次）\n\n\n\n\n本质上，就是一个是数组，一个是map。一个遇到冲突会线性的从头找到尾，另一个天然隔绝了这种问题的存在。只是用起来比较费劲，需要是专门的FastThreadLocalThread对象\n\n是否真的存在自动清理？没有，在Netty框架内确实实现了自动清理，但是我们自己使用的话还是老样子要调用remove()函数\n对照\n\n\n对比点\nThreadLocal\nFastThreadLocal\n\n\n\n存储结构\nThreadLocalMap（哈希表）\n数组（按 index 存取）\n\n\n是否有冲突\n有，线性探测\n无，直接数组定位\n\n\n清理机制\n没有自动清理\n支持 removeAll()，但需手动调用\n\n\n性能（变量多时）\n探测成本高，容易卡\n始终 O(1)，访问极快\n\n\n使用门槛\n低，开箱即用\n高，需要特殊线程配合\n\n\n","tags":["Java","多线程","ThreadLocal","FastThreadLocal"]},{"title":"QQ邮箱如何绑定自己的域名，制作自己的独特邮箱！","url":"/2018/09/07/6/","content":"​    今天下午无聊，北京的天气今天清爽了下来，洗个澡坐到了电脑前，一个QQ消息过来。哦~ 我朋友他们公司注册了一个域名，想问我怎么自定义自己的公司的域名，看着更有专业性呢? 碰巧，也今天下午闲下来了。就直接跟他说拉~ 下面看下这个教程说不定对你有帮助哦~\n​     闲话不多说，我们开始吧！\n​     首先第一步，先进去QQ邮箱为我们提供专业的域名邮箱入口。(这里注意一点:登陆的账号建议就是你之后管理这个域名的账号) ，进去之后直接点创建域名邮箱\n​      点我\n​     第二步: 进去之后，他会根据傻瓜式操作的告诉你，下一步怎么办。现在我们需要做的就是，填写上你的域名(注意必须要有操作的权限，后期需要绑定解析 )\n\n​     第三步: 选择你的域名归属，这里直接点其他吧\n​      \n​      第四步: 直接进去验证域名所有权，然后解析到腾讯的服务器上。\n\n这里就需要进去你的云计算的控制面板了，这里腾讯云为例。\n\n当然了，第一个也就是cname的记录了，注意下第二个我们选择 mx记录\n随后操作完之后，域名验证好了，现在就可以直接进入账号的控制面板了。\n\n\n  第五步:然后我们就可以管理了，点击管理进入这个页面\n\n  \n好了，设置完了，到这里就好啦。赶紧去叫你的朋友发个邮件帮你看看吧~\n有什么问题不懂的，可以练习我哦\n","tags":["技术"]},{"title":"Kafka学习笔记","url":"/2022/12/25/15/","content":"Kafka基本的概念Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，**基于zookeeper协调的分布式日志系统(也可以当做MQ系统)**，常见可以用于web&#x2F;nginx日志、访问日志，消息服务等等，Linkedin于2010年贡献给了Apache基金会并成为顶级开源项目。\nKafka部分名词解释如下\nBroker：消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。\nTopic：一类消息，例如page view日志、click、短信、日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。\nPartition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。相当于分区\nSegment：partition物理上由多个segment组成\noffset：每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息.\nProducer：消息的生产者，负责向Broker中投递消息\nConsumer：消息的消费者，负责从Broker中拉取消息\nConsumer Group：消费者组，在同一个消费者组中是不能够消费同一个分区中的消息的。在多个不同的消费组中，多个不同的消费者可以消费同一条消息\n\n\nKafka消息队列模型kafka的消息队列一般分为两种模式：点对点和订阅模式\n点对点模式Kafka 是支持消费者群组的，也就是说 Kafka 中会有一个或者多个消费者，如果一个生产者生产的消息由一个消费者进行消费的话，那么这种模式就是点对点模式\n发布订阅模式如果一个生产者或者多个生产者产生的消息能够被多个消费者同时消费的情况，这样的消息队列成为发布订阅模式的消息队列\nKafka有哪些特性致使它性能这么高？\n顺序读写\n零拷贝\n消息压缩\n分批发送\n\nKafka的设计亮点\n高吞吐、低延迟：kakfa 最大的特点就是收发消息非常快，kafka 每秒可以处理几十万条消息，它的最低延迟只有几毫秒；\n高伸缩性：每个主题(topic) 包含多个分区(partition)，主题中的分区可以分布在不同的主机(broker)中；\n持久性、可靠性：Kafka 能够允许数据的持久化存储，消息被持久化到磁盘，并支持数据备份防止数据丢失，Kafka 底层的数据存储是基于 Zookeeper 存储的，Zookeeper 我们知道它的数据能够持久存储；\n容错性：允许集群中的节点失败，某个节点宕机，Kafka 集群能够正常工作；\n高并发：支持数千个客户端同时读写。\n\nkafka的设计原理kafka文件存储原理\nKafka实际上就是日志消息存储系统， 根据offset获取对应的消息，消费者获取到消息之后\n\n该消息不会立即从mq中移除。\n\n将topic分成多个不同的分区、每个分区中拆分成多个不同的segment文件存储日志。\n每个segment文件会有\n.index 消息偏移量索引文件\n.log文件 消息物理存放的位置\n\n\n\n在默认的情况下，每个segment文件容量最大是为500mb，如果超过500mb的情况下依次内推，产生一个新的segment文件\ntopic中partition存储分布假设一个kafka集群中只有一个broker，opt/keshao/message-data为数据文件存储目录，在Kafka broker中的server.properties文件配置（参数：log.dirs），例如创建2个topic名称分别为report_pushlaunch_info,partitions数量都为partitions=4 \n|--report_push-0|--report_push-1|--report_push-2|--report_push-3|--launch_info-0|--launch_info-1|--launch_info-2|--launch_info-3\n在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 如果是多broker分布情况，请参考kafka集群partition分布原理分析\npartition中文件存储方式\n每个partiton中相当于一个巨型文件被平均分配到多个大小相等segment（段）数据中。但每个段segment file消息数量不一定相等，这种特性方便old segment被快速检索删除\n每个partiton只需要顺序读写就可以了，segment文件生命周期由服务端配置参数决定。\n这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。\n\n\npartiton中segment文件存储结构\nsegment file由2大部分组成，分别index file和data file，两个文件一一对应，成对出现。后缀.index和.log分别表示为segment索引文件、数据文件；\nsegment文件命名规则：partiton全局的第一个segment0开始，后续每个setment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充；\n\n文件的是Kafka broker中做的一个实验，创建一个topicXXX包含1 partiton，设置每个segment大小为500MB，并启动producer向Kafka broker写入大量数据segment file中，index与file的物理关系如下：查找顺序：查找offset&#x3D;6的消息的流程如下\n\n二分查找算法：查找到该分区中所有的Segment文件 list排序  每个Segment文件都是有一个命名规范，offset&#x3D;7在我们的Segment文件中，此处定位到index文件\n先访问该index文件，根据offset值查询到物理存放位置，Offset&#x3D;7&gt;6&lt;9 所以定位到offset&#x3D;6 获取到物理存放位置1407\n根据该物理存放位置9807 去对应的log文件查找消息，依次向下查找+1次 获取到offset&#x3D;7的消息。\n\n为什么kafka中的 索引文件没有对每个消息建立索引呢？\n\n目的是为了节约我们空间的资源\n稀疏索引算法+二分查找算法，定位到位置，在根据顺序遍历查找。如果该offset消息 没有对应的索引的情况下，时间复杂度是为多少：（ON）如果该offset消息 有对应的索引的情况下，时间复杂度是为多少：（O1）\n\nmessage的存储方式表格中列出了message的物理结构：\n\n\n\n关键字\n解释说明\n\n\n\n8 byte offset\n在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message\n\n\n4 byte message size\nmessage大小\n\n\n4 byte CRC32\n用crc32校验message\n\n\n1 byte “magic”\n表示本次发布Kafka服务程序协议版本号\n\n\n1 byte “attributes”\n表示为独立版本、或标识压缩类型、或编码类型。\n\n\n4 byte key length\n表示key的长度,当key为-1时，K byte key字段不填\n\n\nK byte key\n可选\n\n\nvalue bytes payload\n表示实际消息数据。\n\n\n小结Kafka运行时很少有大量读磁盘的操作，主要是定期批量写磁盘操作，因此操作磁盘很高效。这跟Kafka文件存储中读写message的设计是息息相关的。Kafka中读写message有如下特点:写message\n\n消息从java堆转入page cache(即物理内存)。\n由异步线程刷盘,消息从page cache刷入磁盘。\n\n读message\n\n消息直接从page cache转入socket发送出去。\n当从page cache没有找到相应数据时，此时会产生磁盘IO,从磁 盘Load消息到page cache,然后直接从socket发出去\n\nKafka高效文件存储设计特点\n\nKafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。\n通过索引信息可以快速定位message和确定response的最大大小。\n通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。\n通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。\n\n无论消息是否被消费，kafka都会保存所有的消息，旧消息删除策略1、 基于时间，默认配置是168小时（7天）。2、 基于大小，默认配置是1073741824。\nKafka如何保证生产消息可靠性副本机制副本机制（Replication）：也可以称之为备份机制，通常是指分布式系统在多台互联网的机器上保存相同的数据拷贝，副本机制有什么好处？\n\n提供数据冗余：即使系统部分组件失效，系统依然能够继续运转，因而增加了整体可用性以及数据持久性\n提供高伸缩性：支持横向扩展，能够通过添加机器的方式来提升读的性能，进而提高读操作吞吐量\n改善数据局部性：允许将数据放入与用户地理位置相近的地方，从而降低系统延时\n\nkafka是有主题概念的，而每一个主题又进一步划分成若干个分区。副本的概念实际上是在分区层级下定义的，每个分区配置有多若干个副本。所谓的副本，本质上就是一个只能追加写消息的提交日志，根据kafka副本机制的定义，同一个分区下的所有副本保存有相同的消息序列，这些副本分散的保存在不同的Broker上，从而能够对抗部分Broker宕机带来的数据不可用。\nkafka有了副本机制是否会发生数据丢失？会。写入数据都是往某个Partition的Leader写入的，然后那个Partition的Follower会从Leader同步数据，但是这个同步过程是异步的。也就是说如果此时1条数据刚写入Leader Partition1，还没来得及同步给Follower，Leader Partiton1所在机器突然就宕机了的话，此时就会选举Partition1的Follower作为新的Leader对外提供服务，然后用户就读不到刚才写入的那条数据了。因为Partition0的Follower上是没有同步到最新的一条数据的，这个时候就会造成数据丢失的问题。\nKafka的ISR机制\n此处的Leader是Partition的Leader，而不是Broker的Leader\n\n这个机制简单来说，就是会自动给每个Partition维护一个ISR列表，这个列表里一定会有Leader，然后还会包含跟Leader保持同步的Follower。也就是说，只要Leader的某个Follower一直跟他保持数据同步，那么就会存在于ISR列表里。但是如果Follower因为自身发生一些问题，导致不能及时的从Leader同步数据过去，那么这个Follower就会被认为是“out-of-sync”，从ISR列表里移除。\n怎么保证Kafka写入的数据不丢失？\n每个Partition都至少得有1个Follower在ISR列表里，跟上了Leader的数据同步\n每次写入数据的时候，都要求至少写入Partition Leader成功，同时还有至少一个ISR里的Follower也写入成功，才算这个写入是成功了\n如果不满足上述两个条件，那就一直写入失败，让生产系统不停的尝试重试，直到满足上述两个条件，然后才能认为写入成功\n这个时候万一leader宕机，就可以切换到那个follower上去，那么Follower上是有刚写入的数据的，此时数据就不会丢失了。\n\n关于第二点就需要去配置相应ack参数，才能保证写入Kafka的数据不会丢失。\nKafka的ack消息模式acks参数，是在Kafka Producer，也就是生产者里设置的。这个参数实际上有三种常见的值可以设置，分别是：0、1 和 all。\n\n0： Producer 不等待 Broker 的 ACK，这提供了最低延迟，Broker 一收到数据还没有写入磁盘就已经返回，当 Broker 故障时有可能丢失数据。\n1:   Producer 等待 Broker 的 ACK，Partition 的 Leader 落盘成功后返回 ACK，如果在 Follower 同步成功之前 Leader 故障，那么将会丢失数据。\n-1 （all）： Producer 等待 Broker 的 ACK，Partition 的 Leader 和 Follower 全部落盘成功后才返回 ACK。但是在 Broker 发送 ACK 时，Leader 发生故障，则会造成数据重复。\n\n副本选举实现原理当Leader副本宕机之后，会从ISR同步副本列表中剔除，然后取出剩下的ISR列表中第一个为Lader副本，显然还有可能数据没有及时同步完成，当选择为Leader副本之后，数据还会可能存在丢失的情况。\n副本故障处理机制\nLEO:每个副本数据最后一个的offset或者最大的offset值。\nHW 消费者能够看见到的最大offset值。\n\n\nFollower节点发生故障原理当我们follower2节点如果宕机之后，就会从ISR列表中剔除，有突然恢复了，则开始同步Leader 节点的数据。如何同步：如果follower 的leo不等于Leader 节点的leo，则开始截取高于当前hw位置的log，从该hw位置开始同步Leader 节点数据，如果该follower的leo大于该分区的hw，则从新加入isr列表中。Kafka实现集群，保证每个副本的数据一致性问题，但是不能保证消息不会丢失\n\n发生该问题如何解决？生产者投递消息采用日志形式记录下来，如果消费者消费成功之后，在可以将该消息给删除。\n\nLeader节点发生故障原理如果Leader节点宕机之后，会从新在剩余的isr列表中，选举一个新的Leader节点。为了保证每个节点中副本一致性的问题，会将高与hw位置的log给截取掉。所以我们kafka为了严格意义上，保证每个节点副本数据一致性问题，但是不能保证数据不丢失。概率：—-非常低。解决办法：生产者投递消息的时候采用日志记录的方式，如果发生Leader变为follower 部分的消息被丢失的情况下，我们可以使用生产投递日志实现补偿。\nKafka选举原理控制器原理控制器组件（Controller），是 Apache Kafka 的核心组件。它的主要作用是在 Apache ZooKeeper 的帮助下管理和协调整个 Kafka 集群。在分布式系统中，通常需要有一个协调者，该协调者会在分布式系统发生异常时发挥特殊的作用。在Kafka中该协调者称之为控制器(Controller),其实该控制器并没有什么特殊之处，它本身也是一个普通的Broker，只不过需要负责一些额外的工作(追踪集群中的其他Broker，并在合适的时候处理新加入的和失败的Broker节点、Rebalance分区、分配新的leader分区等)。值得注意的是：Kafka集群中始终只有一个Controller Broker。\nController Broker是如何被选出来的Broker 在启动时，会尝试去 ZooKeeper 中创建 &#x2F;controller 节点。Kafka 当前选举控制器的规则是：第一个成功创建 &#x2F;controller 节点的 Broker 会被指定为控制器。Kafka的Broker Controller通过注册一个controller临时节点节点进行竞选，如果未set成功，则watch该节点，等待成为controller\nController Broker的具体作用是什么Controller Broker的主要职责有很多，主要是一些管理行为，主要包括以下几个方面：\n\n创建、删除主题，增加分区并分配leader分区\n集群Broker管理（新增 Broker、Broker 主动关闭、Broker 故障)\npreferred leader选举\n分区重分配\n\n主题管理这里的主题管理，就是指控制器帮助我们完成对 Kafka 主题的创建、删除以及分区增加的操作。换句话说，当我们执行kafka-topics 脚本时，大部分的后台工作都是控制器来完成的。\n分区重分配分区重分配主要是指，kafka-reassign-partitions 脚本提供的对已有主题分区进行细粒度的分配功能。这部分功能也是控制器实现的。\n集群成员管理自动检测新增 Broker、Broker 主动关闭及被动宕机。这种自动检测是依赖于前面提到的 Watch 功能和 ZooKeeper 临时节点组合实现的。比如，控制器组件会利用Watch 机制检查 ZooKeeper 的 /brokers/ids 节点下的子节点数量变更。目前，当有新 Broker 启动后，它会在 /brokers 下创建专属的 znode 节点。一旦创建完毕，ZooKeeper 会通过 Watch 机制将消息通知推送给控制器，这样，控制器就能自动地感知到这个变化，进而开启后续的新增 Broker 作业。侦测 Broker 存活性则是依赖于刚刚提到的另一个机制：临时节点。每个 Broker 启动后，会在 /brokers/ids 下创建一个临时 znode。当 Broker 宕机或主动关闭后，该 Broker 与 ZooKeeper 的会话结束，这个 znode 会被自动删除。同理，ZooKeeper 的 Watch 机制将这一变更推送给控制器，这样控制器就能知道有 Broker 关闭或宕机了，从而进行“善后”。\n数据服务控制器的最后一大类工作，就是向其他 Broker 提供数据服务，控制器上保存了最全的集群元数据信息，其他所有 Broker 会定期接收控制器发来的元数据更新请求，从而更新其内存中的缓存数据。当控制器发现一个 broker 离开集群（通过观察相关 ZooKeeper 路径），控制器会收到消息：这个 broker 所管理的那些分区需要一个新的 Leader。控制器会依次遍历每个分区，确定谁能够作为新的 Leader，然后向所有包含新 Leader 或现有 Follower 的分区发送消息，该请求消息包含谁是新的 Leader 以及谁是 Follower 的信息。随后，新的 Leader 开始处理来自生产者和消费者的请求，Follower 用于从新的 Leader 那里进行复制。\n消费者Rebalance机制（再平衡）rebalance就是说如果消费组里的消费者数量有变化或消费的分区数有变化，kafka会重新分配消费者消费分区的关系。比如consumer group中某个消费者挂了，此时会自动把分配给他的分区交给其他的消费者，如果他又重启了，那么又会把一些分区重新交还给他。注意：rebalance只针对subscribe这种不指定分区消费的情况，如果通过assign这种消费方式指定了分区，kafka不会进行rebanlance。如下情况可能会触发消费者rebalance:\n\n消费组里的consumer增加或减少了\n动态给topic增加了分区\n消费组订阅了更多的topic\n\n主要有三种rebalance的策略：range()、round-robin(轮询)、sticky(粘性)。Kafka 提供了消费者客户端参数partition.assignment.strategy 来设置消费者与订阅主题之间的分区分配策略。默认情况为range分配策略。\n\nrange策略就是按照分区序号排序(范围分配)，假设 n＝分区数／消费者数量 &#x3D; 3， m＝分区数%消费者数量 &#x3D; 1，那么前 m 个消费者每个分配 n+1 个分区，后面的（消费者数量－m ）个消费者每个分配 n 个分区。比如分区03给一个consumer，分区46给一个consumer，分区7~9给一个consumer。\nround-robin策略就是轮询分配，比如分区0、3、6、9给一个consumer，分区1、4、7给一个consumer，分区2、5、8给一个consumer\nsticky策略初始时分配策略与round-robin类似，但是在rebalance的时候，需要保证如下两个原则。\n分区的分配要尽可能均匀\n分区的分配尽可能与上次分配的保持相同。\n当两者发生冲突时，第一个目标优先于第二个目标 。这样可以最大程度维持原来的分区分配的策略。比如对于第一种range情况的分配，如果第三个consumer挂了，那么重新用sticky策略分配的结果如下：\nconsumer1除了原有的0~3，会再分配一个7\nconsumer2除了原有的4~6，会再分配8和9\n\n\n\n\n\n同步副本(in-sync replica ,ISR)列表ISR中的副本都是与Leader进行同步的副本，所以不在该列表的follower会被认为与Leader是不同步的. 那么，ISR中存在是什么副本呢？首先可以明确的是：Leader副本总是存在于ISR中。 而follower副本是否在ISR中，取决于该follower副本是否与Leader副本保持了“同步”。始终保证拥有足够数量的同步副本是非常重要的。要将follower提升为Leader，它必须存在于同步副本列表中。每个分区都有一个同步副本列表，该列表由Leader分区和Controller进行更新。选择一个同步副本列表中的分区作为leader 分区的过程称为clean leader election。注意，这里要与在非同步副本中选一个分区作为leader分区的过程区分开，在非同步副本中选一个分区作为leader的过程称之为unclean leader election。由于ISR是动态调整的，所以会存在ISR列表为空的情况，通常来说，非同步副本落后 Leader 太多，因此，如果选择这些副本作为新 Leader，就可能出现数据的丢失。毕竟，这些副本中保存的消息远远落后于老 Leader 中的消息。在 Kafka 中，选举这种副本的过程可以通过Broker 端参数 *_*unclean.leader.election.enable _控制是否允许 Unclean 领导者选举。开启 Unclean 领导者选举可能会造成数据丢失，但好处是，它使得分区 Leader 副本一直存在，不至于停止对外提供服务，因此提升了高可用性。反之，禁止 Unclean Leader 选举的好处在于维护了数据的一致性，避免了消息丢失，但牺牲了高可用性。分布式系统的CAP理论说的就是这种情况。不幸的是，unclean leader election*的选举过程仍可能会造成数据的不一致，因为同步副本并不是*完全同步的。由于复制是异步**完成的，因此无法保证follower可以获取最新消息。比如Leader分区的最后一条消息的offset是100，此时副本的offset可能不是100，这受到两个参数的影响：\n\nreplica.lag.time.max.ms：同步副本滞后与leader副本的时间\nzookeeper.session.timeout.ms：与zookeeper会话超时时间\n\n脑裂现象如果controller Broker 挂掉了，Kafka集群必须找到可以替代的controller，集群将不能正常运转。这里面存在一个问题，很难确定Broker是挂掉了，还是仅仅只是短暂性的故障。但是，集群为了正常运转，必须选出新的controller。如果之前被取代的controller又正常了，他并不知道自己已经被取代了，那么此时集群中会出现两台controller。其实这种情况是很容易发生。比如，某个controller由于GC而被认为已经挂掉，并选择了一个新的controller。在GC的情况下，在最初的controller眼中，并没有改变任何东西，该Broker甚至不知道它已经暂停了。因此，它将继续充当当前controller，这是分布式系统中的常见情况，称为脑裂。假如，处于活跃状态的controller进入了长时间的GC暂停。它的ZooKeeper会话过期了，之前注册的&#x2F;controller节点被删除。集群中其他Broker会收到zookeeper的这一通知。由于集群中必须存在一个controller Broker，所以现在每个Broker都试图尝试成为新的controller。假设Broker 2速度比较快，成为了最新的controller Broker。此时，每个Broker会收到Broker2成为新的controller的通知，由于Broker3正在进行”stop the world”的GC，可能不会收到Broker2成为最新的controller的通知。等到Broker3的GC完成之后，仍会认为自己是集群的controller，在Broker3的眼中好像什么都没有发生一样。现在，集群中出现了两个controller，它们可能一起发出具有冲突的命令，就会出现脑裂的现象。如果对这种情况不加以处理，可能会导致严重的不一致。所以需要一种方法来区分谁是集群当前最新的Controller。Kafka是通过使用epoch number（纪元编号，也称为隔离令牌）来完成的。epoch number只是单调递增的数字，第一次选出Controller时，epoch number值为1，如果再次选出新的Controller，则epoch number将为2，依次单调递增。每个新选出的controller通过Zookeeper 的条件递增操作获得一个全新的、数值更大的epoch number 。其他Broker 在知道当前epoch number 后，如果收到由controller发出的包含较旧(较小)epoch number的消息，就会忽略它们，即Broker根据最大的epoch number来区分当前最新的controller。\n上图，Broker3向Broker1发出命令:让Broker1上的某个分区副本成为leader，该消息的epoch number值为1。于此同时，Broker2也向Broker1发送了相同的命令，不同的是，该消息的epoch number值为2，此时Broker1只听从Broker2的命令(由于其epoch number较大)，会忽略Broker3的命令，从而避免脑裂的发生。\nKafka优化策略常见核心配置内存缓冲的大小：buffer.memory\n生产者投递消息先存放在本地缓冲区中，将消息组装成n多个不同的Batch，在通过send线程将缓冲区的数据批量的形式发送给kafka服务器端存放。\n生产者本地内存缓冲区如果设置太小了，在高并发情况下有可能会发生内存溢出，导致生产者无法继续写入消息到缓冲区卡死。\n实际生产环境中，根据压力测试情况下，合理设置内存缓冲区大小。\n\n参数：buffer.memory\n最大请求大小：“max.request.size”该参数kafkamq服务器端限制接受的消息\n重试策略“retries”和“retries.backoff.ms”该参数设置定重试的次数、间隔时间\n确认机制：acks建议设置为1ACK 参数配置：\n\n0：Producer 不等待 Broker 的 ACK，这提供了最低延迟，Broker 一收到数据还没有写入磁盘就已经返回，当 Broker 故障时有可能丢失数据。\n1：Producer 等待 Broker 的 ACK，Partition 的 Leader 落盘成功后返回 ACK，如果在 Follower 同步成功之前 Leader 故障，那么将会丢失数据。\n-1（all）：Producer 等待 Broker 的 ACK，Partition 的 Leader 和 Follower 全部落盘成功后才返回 ACK。但是在 Broker 发送 ACK 时，Leader 发生故障，则会造成数据重复。\n\n具体看业务要求：-1 all  延迟概率一定很低 0 延迟概率为适中1\n消费者分区的个数消费者怎么知道我应该从哪个位置开始消费呢？应该有一个记录分组对应消费分区offset位置。\n\n在老的版本kafka中是记录在zk上，记录在zk上频繁读写操作，性能不是很好。\n在新的版本kafka中，消费者消费分区中的消息是记录在topic主题日志文件中，默认的情况下分成50个文件记录。\n\n为什么消费者需要使用50个文件记录消费者消费记录呢？\n\n如果消费者（分组）比较多的，都记录在同一个日志文件中，读写操作就非常麻烦。\n\n消费者怎么知道我应该读取那个日志文件 知道从那个offset开始消费呢？\n\n消费者消费消息的时候：key&#x3D;group-id.topic.partition\ngroup-id.topic.partition&#x3D;mayikt.mttopic.0\n(key&#x3D;group-id.topic.partition)%consumer_offsets.size(50)&#x3D;12\nOffset消费记录 记录在consumer_offsets-12文件夹\n记录Offset消费记录 的是consumer分组对应消费记录 不是记录单个消费者消费记录。\n\n–记录当前分组消费的记录—offsets.topic.replication.factor 参数的约束，默认值为3（注意：该参数的使用限制在0.11.0.0版本发生变化），分区数可以通过 offsets.topic.num.partitions 参数设置，默认值为50。\n优化策略Broker优化\nreplica复制配置\n\nfollow从leader拉取消息进行同步数据\nnum.replica.fetchers  拉取线程数 配置多可以提高follower的I/O并发度，单位时间内leader持有更多请求，相应负载会增大，需要根据机器硬件资源做权衡replica.fetch.min.bytes=1  拉取最小字节数 默认配置为1字节，否则读取消息不及时replica.fetch.max.bytes= 5 * 1024 * 1024 拉取最大字节数  默认为1MB，这个值太小，5MB为宜，根据业务情况调整replica.fetch.wait.max.ms follow 最大等待时间 \n\n\n压缩速度 compression.type：压缩的速度上lz4&#x3D;snappy&lt;gzip。\n\nProduer优化幂等性：enable.idempotence是否使用幂等性。如果设置为true，表示producer将确保每一条消息只会存放一份；如果设置为false，则表示producer因发送数据到broker失败重试使，可能往数据流中写入多分重试的消息。注意：如果使用idempotence，即enable.idempotence为true，那么要求配置项max.in.flight.requests.per.connection的值必须小于或等于5；配置项retries的值必须大于0；acks配置项必须设置为all。如果这些值没有被用户明确地设置，那么系统将自动选择合适的值。如果设置　　的值不合适，那么会抛出ConfigException异常。\n网络和IO线程配置优化1.num.network.threads：Broker处理消息的最大线程数2.num.io.threads：Broker处理磁盘IO的线程数一般num.network.threads主要处理网络io，读写缓冲区数据，配置线程数量为cpu核数加1num.io.threads主要进行磁盘io操作，高峰期可能会发生IO等待，因此配置需要大些，配置线程数量为cpu核数2倍，最大不超过3倍.\n日志保留策略配置生产者投递消息到kafka的mq中，消费者获取到消息之后不会立即被删除，会有一个日志保留策略。\n\n减少日志保留时间，建议三天或则更多时间。log.retention.hours&#x3D;72\n分段文件配置1GB， 默认是500mb 有利于快速回收磁盘空间，重启kafka加载也会加快(如果文件过小，则文件数量比较多，kafka启动时是单线程扫描目录(log.dir)下所有数据文件)，文件较多时性能会稍微降低。log.segment.bytes&#x3D;1073741824##日志滚动的周期时间，到达指定周期时间时，强制生成一个新的segmentlog.roll.hours=72##segment的索引文件最大尺寸限制，即时log.segment.bytes没达到，也会生成一个新的segmentlog.index.size.max.bytes=10*1024*1024##控制日志segment文件的大小，超出该大小则追加到一个新的日志segment文件中（-1表示没有限制）log.segment.bytes=1014*1024*1024\n\nlog数据文件刷盘策略当我们把数据写入到文件系统之后，数据其实在操作系统的page cache里面，并没有刷到磁盘上去。如果此时操作系统挂了，其实数据就丢了。\n\n每当producer写入10000条消息时，刷数据到磁盘 配置为：log.flush.interval.messages&#x3D;10000\n每间隔1秒钟时间，刷数据到磁盘。log.flush.interval.ms&#x3D;1000\n\n配置优化案例（重要）Brokernum.replica.fetchers： 适量提高同步leader副本线程Produceringer.ms 0 或者1 定时将批量消息发送到Broker中Consumerauto.commit.enable —配置为手动提交offset\nDocker环境下的Kafka环境搭建（单机）https://www.lixueduan.com/posts/kafka/01-install/\nversion: &quot;3&quot;services:  zookeeper:    image: &#x27;bitnami/zookeeper:latest&#x27;    ports:      - &#x27;2181:2181&#x27;    environment:      # 匿名登录--必须开启      - ALLOW_ANONYMOUS_LOGIN=yes    #volumes:      #- ./zookeeper:/bitnami/zookeeper  # 该镜像具体配置参考 https://github.com/bitnami/bitnami-docker-kafka/blob/master/README.md  kafka:    image: &#x27;bitnami/kafka:2.8.0&#x27;    ports:      - &#x27;9092:9092&#x27;      - &#x27;9999:9999&#x27;    environment:      - KAFKA_BROKER_ID=1      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092      # 客户端访问地址，更换成自己的      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181      # 允许使用PLAINTEXT协议(镜像中默认为关闭,需要手动开启)      - ALLOW_PLAINTEXT_LISTENER=yes      # 关闭自动创建 topic 功能      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=false      # 全局消息过期时间 6 小时(测试时可以设置短一点)      - KAFKA_CFG_LOG_RETENTION_HOURS=6      # 开启JMX监控      - JMX_PORT=9999    #volumes:      #- ./kafka:/bitnami/kafka    depends_on:      - zookeeper  # Web 管理界面 另外也可以用exporter+prometheus+grafana的方式来监控 https://github.com/danielqsj/kafka_exporter  kafka_manager:    image: &#x27;hlebalbau/kafka-manager:latest&#x27;    ports:      - &quot;9000:9000&quot;    environment:      ZK_HOSTS: &quot;zookeeper:2181&quot;      APPLICATION_SECRET: letmein    depends_on:      - zookeeper      - kafka\nSpringBoot启动和配置kafkaSpringBoot中很多kafka的使用技巧，简单记录和探索\n1. 引入依赖引入基本的maven的pom文件，包含kafka的Server\n&lt;dependencies&gt;  &lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;  &lt;/dependency&gt;  &lt;dependency&gt;    &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;    &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;  &lt;/dependency&gt;  &lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;    &lt;scope&gt;test&lt;/scope&gt;  &lt;/dependency&gt;  &lt;dependency&gt;    &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;    &lt;artifactId&gt;spring-kafka-test&lt;/artifactId&gt;    &lt;scope&gt;test&lt;/scope&gt;  &lt;/dependency&gt;  &lt;dependency&gt;    &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;    &lt;artifactId&gt;lombok&lt;/artifactId&gt;  &lt;/dependency&gt;  &lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;  &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt;  &lt;plugins&gt;    &lt;plugin&gt;      &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;      &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;      &lt;version&gt;3.8.1&lt;/version&gt;      &lt;configuration&gt;        &lt;encoding&gt;UTF-8&lt;/encoding&gt;      &lt;/configuration&gt;    &lt;/plugin&gt;    &lt;plugin&gt;      &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;      &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt;    &lt;/plugin&gt;  &lt;/plugins&gt;&lt;/build&gt;\n2. 使用Spring Test启动一个kafka只要maven项目中引入spring-kafka-test就可以直接使用springboot启动一个kafka的Server，非常方便在开发阶段\n/** * 启动kafka dev环境的实例 */@SpringBootTest(classes = ApplicationTests.class)@EmbeddedKafka(count = 4, ports = &#123;9092, 9093, 9094, 9095&#125;)public class ApplicationTests &#123;    /**     * 启动kafka     *     * dev test模式     *     * @throws IOException     */    @Test    public void startServer() throws IOException &#123;        System.in.read();    &#125;&#125;\n3. 初步测试3.1 编写生产者和消费者的demo代码/** * kafka的demo * * @author zhangshuaike */@SpringBootApplication@RestControllerpublic class KafkaDemoApplication &#123;    public static void main(String[] args) &#123;        SpringApplication.run(KafkaDemoApplication.class, args);    &#125;    @Autowired    private KafkaTemplate&lt;Object, Object&gt; template;    @GetMapping(&quot;/send/&#123;input&#125;&quot;)    public void sendFoo(@PathVariable String input) &#123;        this.template.send(&quot;topic_input&quot;, input);    &#125;    @KafkaListener(id = &quot;webGroup&quot;, topics = &quot;topic_input&quot;)    public void listen(String input) &#123;        System.out.println(&quot;input value:&quot; + input);    &#125;&#125;\n3.2 发送测试curl http://127.0.0.1:8080/send/test\n4. 带回调函数的生产者kafkaTemplate提供了一个回调方法addCallback，我们可以在回调方法中监控消息是否发送成功 或 失败时做补偿处理，有两种写法。\n4.1 第一种@GetMapping(&quot;/one/&#123;message&#125;&quot;)public void sendMessage2(@PathVariable(&quot;message&quot;) String callbackMessage) &#123;    kafkaTemplate.send(&quot;topic1&quot;, callbackMessage).addCallback(success -&gt; &#123;        // 消息发送到的topic        String topic = success.getRecordMetadata().topic();        // 消息发送到的分区        int partition = success.getRecordMetadata().partition();        // 消息在分区内的offset        long offset = success.getRecordMetadata().offset();        System.out.println(&quot;发送消息成功:&quot; + topic + &quot;-&quot; + partition + &quot;-&quot; + offset);    &#125;, failure -&gt; &#123;        System.out.println(&quot;发送消息失败:&quot; + failure.getMessage());    &#125;);&#125;\n4.2 第二种@GetMapping(&quot;/two/&#123;message&#125;&quot;)public void sendMessage3(@PathVariable(&quot;message&quot;) String callbackMessage) &#123;    kafkaTemplate.send(&quot;topic1&quot;, callbackMessage).addCallback(new ListenableFutureCallback&lt;SendResult&lt;String, Object&gt;&gt;() &#123;        @Override        public void onFailure(Throwable ex) &#123;            System.out.println(&quot;发送消息失败：&quot;+ex.getMessage());        &#125;        @Override        public void onSuccess(SendResult&lt;String, Object&gt; result) &#123;            System.out.println(&quot;发送消息成功：&quot; + result.getRecordMetadata().topic() + &quot;-&quot;                    + result.getRecordMetadata().partition() + &quot;-&quot; + result.getRecordMetadata().offset());        &#125;    &#125;);&#125;\n5. 自定义分区器kafka中每个topic被划分为多个分区，那么生产者将消息发送到topic时，具体要追加到哪个分区？这就是分区策略，Kafka 为我们提供了默认的分区策略，同时它也支持自定义分区策略。其路由机制为：\n\n若发送消息时指定了分区（即自定义分区策略），则直接将消息append到指定分区；\n若发送消息时未指定 patition，但指定了 key（kafka允许为每条消息设置一个key），则对key值进行hash计算，根据计算结果路由到指定分区，这种情况下可以保证同一个 Key 的所有消息都进入到相同的分区；\npatition 和 key 都未指定，则使用kafka默认的分区策略，轮询选出一个 patition；\n\n我们自定义一个分区策略，将消息发送到我们指定的partition，首先新建一个分区器类实现Partitioner接口，重写方法，其中partition方法的返回值就表示将消息发送到几号分区。\npublic class CustomizePartitioner implements Partitioner &#123;    @Override    public int partition(String s, Object o, byte[] bytes, Object o1, byte[] bytes1, Cluster cluster) &#123;        //自定义分区规则（这里假设全部发到0号分区）        return 0;    &#125;    @Override    public void close() &#123;    &#125;    @Override    public void configure(Map&lt;String, ?&gt; map) &#123;    &#125;&#125;\n在application.propertise中配置自定义分区器，配置的值就是分区器类的全路径名\n# 自定义分区器spring.kafka.producer.properties.partitioner.class=com.felix.kafka.producer.CustomizePartitioner\n6.kafka事务提交如果在发送消息时需要创建事务，可以使用 KafkaTemplate 的 executeInTransaction 方法来声明事务\n/** * 发送事务消息，需要处理以下事项： * 1. 设置spring *    kafka: *     producer: *       retries: 3 #重试次数 *       acks: all *       # 加事务前缀，自动给producer开启事务，所有加 *       transaction-id-prefix: tx_ * 2.方法上增加  @Transactional */@GetMapping(&quot;/send&quot;)@SuppressWarnings(&quot;all&quot;)public void sendMessageTransaction()&#123;    //生命事务，后面报错消息不会发出去    kafkaTemplate.executeInTransaction(operations -&gt;&#123;        operations.send(&quot;transaction&quot;,&quot;慢慢沉淀&quot;);        throw new RuntimeException(&quot;fail&quot;);    &#125;);    //不声明事务，后面保存但前端消息已经发送成功了    kafkaTemplate.send(&quot;transaction&quot;,&quot;慢慢沉淀，但是我不带事务&quot;);    throw new RuntimeException(&quot;fail&quot;);&#125;\n7. 消费者指定参数指定topic、partition、offset消费前面我们在监听消费topic1的时候，监听的是topic1上所有的消息，如果我们想指定topic、指定partition、指定offset来消费呢？也很简单，@KafkaListener注解已全部为我们提供。\n/** * @Title 指定topic、partition、offset消费 * @Description 同时监听topic1和topic2，监听topic1的0号分区、 * topic2的 &quot;0号和1号&quot; 分区，指向1号分区的offset初始值为8 * @param record */@KafkaListener(id=&quot;consumer1&quot;,groupId = &quot;felix-group&quot;,topicPartitions = &#123;        @TopicPartition(topic = &quot;topic1&quot;,partitions = &#123;&quot;0&quot;&#125;),        @TopicPartition(topic = &quot;topic2&quot;,partitions = &quot;0&quot;,                partitionOffsets = @PartitionOffset(partition = &quot;1&quot;,initialOffset = &quot;8&quot;))&#125;)public void onMessage2(ConsumerRecord&lt;?,?&gt; record)&#123;    System.out.println(&quot;topic:&quot;+record.topic()+&quot;partition:&quot;+record.partition()+&quot;offset:&quot;+record.offset()+&quot;value:&quot;+record.value());&#125;\n属性解释：\n\nid：消费者ID；\ngroupId：消费组ID；\ntopics：监听的topic，可监听多个；\ntopicPartitions：可配置更加详细的监听信息，可指定topic、parition、offset监听。\n\n上面onMessage2监听的含义：监听topic1的0号分区，同时监听topic2的0号分区和topic2的1号分区里面offset从8开始的消息。注意：topics和topicPartitions不能同时使用；\n8.消费者批量消费8.1设置application.properties开启批量消费即可# 设置批量消费spring.kafka.listener.type=batch# 批量消费每次最多消费多少条消息spring.kafka.consumer.max-poll-records=50\n8.2 接收消息时用List来接收，监听代码如下：@KafkaListener(id=&quot;consumer2&quot;,groupId = &quot;felix-group&quot;,topics = &quot;topic1&quot; )public void onMesssage(List&lt;ConsumerRecord&lt;?,?&gt;&gt; records)&#123;    System.out.println(&quot;&gt;&gt;&gt;批量消费一次，records.size()=&quot;+records.size());    for(ConsumerRecord&lt;?,?&gt; record:records)&#123;        System.out.println(record.value());    &#125;&#125;\n9.ConsumerAwareListenerErrorHandler异常处理器通过异常处理器，我们可以处理consumer在消费时发生的异常。新建一个 ConsumerAwareListenerErrorHandler 类型的异常处理方法，用@Bean注入，BeanName默认就是方法名，然后我们将这个异常处理器的BeanName放到@KafkaListener注解的errorHandler属性里面，当监听抛出异常的时候，则会自动调用异常处理器。\n//异常处理// 新建一个异常处理器，用@Bean注入@Beanpublic ConsumerAwareListenerErrorHandler consumerAwareErrorHandler()&#123;    return (message,exception,consumer)-&gt;&#123;        System.out.println(&quot;消费异常：&quot;+message.getPayload());        return null;    &#125;;&#125;//将这个异常处理器的BeanName放到@KafkaListener注解的errorHandler属性里面@KafkaListener(topics = &#123;&quot;topic1&quot;&#125;,errorHandler = &quot;consumerAwareErrorHandler&quot;)public void onMessage4(ConsumerRecord&lt;?,?&gt; record) throws Exception&#123;    throw new Exception(&quot;简单消费-模拟异常&quot;);&#125;// 批量消费也一样，异常处理器的message.getPayload()也可以拿到各条消息的信息@KafkaListener(topics = &quot;topic1&quot;,errorHandler=&quot;consumerAwareErrorHandler&quot;)public void onMessage5(List&lt;ConsumerRecord&lt;?,?&gt;&gt; records) throws Exception&#123;    System.out.println(&quot;批量消费一次...&quot;);    throw new Exception(&quot;批量消费-模拟异常&quot;);&#125;\n10.消费过滤器消息过滤器可以在消息抵达consumer之前被拦截，在实际应用中，我们可以根据自己的业务逻辑，筛选出需要的信息再交由KafkaListener处理，不需要的消息则过滤掉。配置消息过滤只需要为 监听器工厂 配置一个RecordFilterStrategy（消息过滤策略），返回true的时候消息将会被抛弃，返回false时，消息能正常抵达监听容器。\npackage com.example.kafka.consumer;import org.apache.kafka.clients.consumer.ConsumerRecord;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.context.annotation.Bean;import org.springframework.kafka.annotation.KafkaListener;import org.springframework.kafka.annotation.PartitionOffset;import org.springframework.kafka.annotation.TopicPartition;import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;import org.springframework.kafka.core.ConsumerFactory;import org.springframework.kafka.listener.ConsumerAwareListenerErrorHandler;import org.springframework.messaging.handler.annotation.SendTo;import org.springframework.stereotype.Component;import java.util.List;@Componentpublic class KafkaConsumer &#123;    @Autowired    ConsumerFactory consumerFactory;    //消息过滤器    @Bean    public ConcurrentKafkaListenerContainerFactory filterContainerFactory()&#123;        ConcurrentKafkaListenerContainerFactory factory=new ConcurrentKafkaListenerContainerFactory();        factory.setConsumerFactory(consumerFactory);        //被过滤器的消息将被丢弃        factory.setAckDiscarded(true);        //消息过滤策略        factory.setRecordFilterStrategy(consumerRecord -&gt; &#123;            if(Integer.parseInt(consumerRecord.value().toString())%2==0)&#123;                return false;            &#125;            //返回true消息则被过滤            return true;        &#125;);        return factory;    &#125;    //消息过滤监听    @KafkaListener(topics = &#123;&quot;topic1&quot;&#125;,containerFactory = &quot;filterContainerFactory&quot;)    public void onMessage6(ConsumerRecord&lt;?,?&gt; record)&#123;        System.out.println(record.value());    &#125;&#125;\n上面实现了一个”过滤奇数、接收偶数”的过滤策略，我们向topic1发送0-99总共100条消息，看一下监听器的消费情况，可以看到监听器只消费了偶数，\n11. 消息转发在实际开发中，我们可能有这样的需求，应用A从TopicA获取到消息，经过处理后转发到TopicB，再由应用B监听处理消息，即一个应用处理完成后将该消息转发至其他应用，完成消息的转发。在SpringBoot集成Kafka实现消息的转发也很简单，只需要通过一个@SendTo注解，被注解方法的return值即转发的消息内容，如下\n/**  * @Title 消息转发  * @Description 从topic1接收到的消息经过处理后转发到topic2  * @param record  * @return  */ @KafkaListener(topics = &#123;&quot;topic&quot;&#125;) @SendTo(&quot;topic2&quot;) public String onMessage7(ConsumerRecord&lt;?,?&gt; record)&#123;     return record.value()+&quot;-forward message&quot;; &#125;\n12. 定时启动，停止监听器默认情况下，当消费者项目启动的时候，监听器就开始工作，监听消费发送到指定topic的消息，那如果我们不想让监听器立即工作，想让它在我们指定的时间点开始工作，或者在我们指定的时间点停止工作，该怎么处理呢——使用KafkaListenerEndpointRegistry，下面我们就来实现：① 禁止监听器自启动；② 创建两个定时任务，一个用来在指定时间点启动定时器，另一个在指定时间点停止定时器；新建一个定时任务类，用注解@EnableScheduling声明，KafkaListenerEndpointRegistry 在SpringIO中已经被注册为Bean，直接注入，设置禁止KafkaListener自启动，\n@EnableScheduling@Componentpublic class CronTimer &#123;    /**     * @KafkaListener注解所标注的方法并不会在IOC容器中被注册为Bean，     * 而是会被注册在KafkaListenerEndpointRegistry中，     * 而KafkaListenerEndpointRegistry在SpringIOC中已经被注册为Bean     **/    @Autowired    private KafkaListenerEndpointRegistry registry;        @Autowired    private ConsumerFactory consumerFactory;    // 监听器容器工厂(设置禁止KafkaListener自启动)    @Bean    public ConcurrentKafkaListenerContainerFactory delayContainerFactory() &#123;        ConcurrentKafkaListenerContainerFactory container = new ConcurrentKafkaListenerContainerFactory();        container.setConsumerFactory(consumerFactory);        //禁止KafkaListener自启动        container.setAutoStartup(false);        return container;    &#125;    // 监听器    @KafkaListener(id=&quot;timingConsumer&quot;,topics = &quot;topic1&quot;,containerFactory = &quot;delayContainerFactory&quot;)    public void onMessage1(ConsumerRecord&lt;?, ?&gt; record)&#123;        System.out.println(&quot;消费成功：&quot;+record.topic()+&quot;-&quot;+record.partition()+&quot;-&quot;+record.value());    &#125;    // 定时启动监听器    @Scheduled(cron = &quot;0 42 11 * * ? &quot;)    public void startListener() &#123;        System.out.println(&quot;启动监听器...&quot;);        // &quot;timingConsumer&quot;是@KafkaListener注解后面设置的监听器ID,标识这个监听器        if (!registry.getListenerContainer(&quot;timingConsumer&quot;).isRunning()) &#123;            registry.getListenerContainer(&quot;timingConsumer&quot;).start();        &#125;        //registry.getListenerContainer(&quot;timingConsumer&quot;).resume();    &#125;    // 定时停止监听器    @Scheduled(cron = &quot;0 45 11 * * ? &quot;)    public void shutDownListener() &#123;        System.out.println(&quot;关闭监听器...&quot;);        registry.getListenerContainer(&quot;timingConsumer&quot;).pause();    &#125;&#125;\n启动项目，触发生产者向topic1发送消息，可以看到consumer没有消费，因为这时监听器还没有开始工作\n参考文章Kafka文件存储机制那些事\nKafka如何保证消息的可靠性_我是你亲爱的航哥的博客-CSDN博客_kafka保证消息可靠性\n谈谈你对Kafka副本Leader选举原理的理解？-51CTO.COM\nkafka的实现原理_kafka_八两_InfoQ写作社区\nSpringBoot整合kafka - 掘金\nKafka的Controller Broker是什么\nspring boot集成kafka之spring-kafka深入探秘 - 凯京科技的个人空间 - OSCHINA - 中文开源技术交流社区\n","tags":["springboot","kafka","mq","分布式","Docker"]},{"title":"SLS 存储分级功能介绍与成本分析","url":"/2025/09/14/29/","content":"介绍阿里云日志服务（SLS）是一站式海量日志大数据平台，支持实时采集、弹性存储、秒级查询与分析，并可灵活投递至多种大数据与AI引擎。通过冷热分层、流式计算和智能告警，SLS帮助企业实现全链路可观测、智能运维与安全审计，在云原生与大数据场景中提供高性能、低成本、免运维的日志全生命周期管理能力。\n该篇文章主要使用的是阿里云 SLS，但是腾讯云的 cls 等也都是同样的原理。均为实现降低存储成本的同时还要额外增长存储天数的行为\n成本核算\n\n\n存储类型\n单价（元&#x2F;GB&#x2F;天）\n每GB每月成本（元）\n\n\n\n热存储\n0.0115\n0.345\n\n\n低频存储\n0.0050\n0.150\n\n\n归档存储\n0.0017\n0.051\n\n\n场景推算假如的所有后端日志均为热存储，因热存储的费用是归档存储的 6 倍，是低频存储的 2.3 倍。我们可以通过当前的分层存储来降低存储成本的同时来拉长存储时间。\n文章接下来的场景全部计算单位为 1G 日志数据\n1. 热存储+低频\n假如所有服务提高到 30 天，则为热存储 7 天+ 低频存储 23天，则最终的价格是 0.0805+  0.115&#x3D;0.1955，降本了约百分之 45\n\n\n\n组成\n单价&#x2F;天\n天数\n小计\n\n\n\n热存储\n0.0115\n7\n0.0805    (0.0115×7)\n\n\n低频\n0.0050\n23\n0.1150    (0.005×23)\n\n\n合计\n\n30\n0.1955\n\n\n阿里云需要调整的配置如下：\n\n2. 热存储+低频+归档\n假如所有服务按照归档服务，战线拉超长 97 天，则是这样计算  0.0805  +    0.15     +   0.102      &#x3D;  0.3325\n\n\n\n组成\n单价&#x2F;天\n天数\n小计\n\n\n\n热存储\n0.0115\n7\n0.0805       (0.0115×7)\n\n\n低频\n0.0050\n30\n0.1500       (0.005× 30)\n\n\n归档\n0.0017\n60\n0.102         (0.0017×30)\n\n\n合计\n\n97\n0.3325\n\n\n阿里云的配置如下：\n\n落地影响面\n服务如果共享一个 Project，用Logstore来区分不同的项目。参考阿里云的官方文档，在检索条件为热存储之外时，qps 会被限制为 10，如果在归档条件内（起码37 天外）， qps 会被限制为 1。\n\n\n\n钉钉告警群（P0、P1）依赖 SLS 的定时检索来查询，需要咨询阿里云这样调整后会不会对告警的精度有所动荡\n初步阅读官方文档后，发现无感知。\n\n\n\n","tags":["云计算","SLS","日志查询"]},{"title":"MySQL亿级数据如何快速增加字段？","url":"/2024/01/07/19/","content":"今天主要介绍一下MySQL 8.0.19 instant add column的新特性，基于亿级数据秒速增加字段，下面一起来看看吧～\nMySQL DDL 的方法MySQL 在大型表上的 DDL 会带来耗时较久、负载较高、额外空间占用、MDL、主从同步延时等情况。需要特别引起重视，而MySQL 的 DDL 有很多种方法。\nMySQL 本身自带三种方法，分别是：copy、inplace、instant。\n\ncopy 算法为最古老的算法，在 MySQL 5.5 及以下为默认算法。\n从 MySQL 5.6 开始，引入了 inplace 算法并且默认使用。inplace 算法还包含两种类型：rebuild-table 和 not-rebuild-table。MySQL 使用 inplace 算法时，会自动判断，能使用 not-rebuild-table 的情况下会尽量使用，不能的时候才会使用 rebuild-table。当 DDL 涉及到主键和全文索引相关的操作时，无法使用 not-rebuild-table，必须使用 rebuild-table。其他情况下都会使用 not-rebuild-table。\n从 MySQL 8.0.12 开始，引入了 instant 算法并且默认使用。目前 instant 算法只支持增加列等少量 DDL 类型的操作，其他类型仍然会默认使用 inplace。\n\n有一些第三方工具也可以实现 DDL 操作，最常见的是 percona 的 pt-online-schema-change 工具（简称为 pt-osc），和 github 的 gh-ost 工具，均支持 MySQL 5.5 以上的版本。\n各类工具的对比\n一般情况下的建议：\n\n如果使用的是 MySQL 5.5 或者 MySQL 5.6，推荐使用 gh-ost\n如果使用的是 MySQL 5.7，索引等不涉及修改数据的操作，建议使用默认的 inplace 算法。如果涉及到修改数据（例如增加列），不关心主从同步延时的情况下使用默认的 inplace 算法，关心主从同步延时的情况下使用 gh-ost\n如果使用的是 MySQL 8.0，推荐使用 MySQL 默认的算法设置，在语句不支持 instant 算法并且在意主从同步延时的情况下使用 gh-ost\n\nMySQL DDL 的原理简析（1）copy算法较简单的实现方法，MySQL 会建立一个新的临时表，把源表的所有数据写入到临时表，在此期间无法对源表进行数据写入。MySQL 在完成临时表的写入之后，用临时表替换掉源表。这个算法主要被早期（&lt;&#x3D;5.5）版本所使用。\n（2）inplace 算法从 5.6 开始，常用的 DDL 都默认使用这个算法。inplace 算法包含两类：inplace-no-rebuild 和 inplace-rebuild，两者的主要差异在于是否需要重建源表。\ninplace 算法的操作阶段主要分为三个：\n\nPrepare阶段： - 创建新的临时 frm 文件(与 InnoDB 无关)。 - 持有 EXCLUSIVE-MDL 锁，禁止读写。 - 根据 alter 类型，确定执行方式（copy，online-rebuild，online-not-rebuild）。 更新数据字典的内存对象。 - 分配 row_log 对象记录数据变更的增量（仅 rebuild 类型需要）。 - 生成新的临时ibd文件 new_table（仅rebuild类型需要）。\nExecute 阶段：降级EXCLUSIVE-MDL锁，允许读写。扫描old_table聚集索引（主键）中的每一条记录 rec。遍历new_table的聚集索引和二级索引，逐一处理。根据 rec 构造对应的索引项。将构造索引项插入 sort_buffer 块排序。将 sort_buffer 块更新到 new_table 的索引上。记录 online-ddl 执行过程中产生的增量（仅 rebuild 类型需要）。重放 row_log 中的操作到 new_table 的索引上（not-rebuild 数据是在原表上更新）。重放 row_log 中的DML操作到 new_table 的数据行上。\nCommit阶段：当前 Block 为 row_log 最后一个时，禁止读写，升级到 EXCLUSIVE-MDL 锁。重做 row_log 中最后一部分增量。更新 innodb 的数据字典表。提交事务（刷事务的 redo 日志）。修改统计信息。rename 临时 ibd 文件，frm文件。变更完成，释放 EXCLUSIVE-MDL 锁。\n\n（3）instant 算法MySQL 8.0.12 才提出的新算法，目前只支持添加列等少量操作，利用 8.0 新的表结构设计，可以直接修改表的 metadata 数据，省掉了 rebuild 的过程，极大的缩短了 DDL 语句的执行时间。\n（4）pt-online-schema-change借鉴了 copy 算法的思路，由外部工具来完成临时表的建立，数据同步，用临时表替换源表这三个步骤。其中数据同步是利用 MySQL 的触发器来实现的，会少量影响到线上业务的 QPS 及 SQL 响应时间。\nMySQL 8.0特性instant add columnMySQL 数据库针对亿级别的大表加字段是痛苦的，需要对表进行重建，MySQL 5.7 支持 Online DDL，大部分 DDL 不影响对表的读取和写入，但是依然会消耗非常多的时间，且占用额外的磁盘空间，并会造成主从延迟。所以大表 DDL 仍是一件令 DBA 头痛的事。而mysql8.0使用instant ADD COLUMN特性，只需很短的时间，字段就加好了，享受MongoDB那样的非结构化存储的灵活方便,无形中减少了开发的工作量。\n快速加列采用的是 instant 算法，使得添加列时不再需要 rebuild 整个表，只需要在表的 metadata 中记录新增列的基本信息即可。在 alter 语句后增加 ALGORITHM&#x3D;INSTANT 即代表使用 instant 算法， 如果未明确指定，则支持 instant 算法的操作会默认使用。如果 ALGORITHM&#x3D;INSTANT 指定但不支持，则操作立即失败并显示错误。\n\n关于列的 DDL 操作，是否支持 instant 等算法，官方文档给出了一个表格，整理如下，星号表示不是全部支持，有依赖项。\ninstant 算法使用最广泛的应该是添加列了，可以看到使用该算法还是有些限制的，一些限制如下：\n\n如果 alter 语句包含了 add column 和其他的操作，其中有操作不支持 instant 算法的，那么 alter 语句会报错，所有的操作都不会执行。\n如果指定了AFTER，字段必须是在最后一列，否则需要重建表；\n只能顺序加列, 仅支持在最后添加列，而不支持在现有列的中间添加列。\n不支持压缩表，即该表行格式不能是 COMPRESSED。\n不支持包含全文索引的表。\n不支持临时表。\n不支持那些在数据字典表空间中创建的表。\nDROP COLUMN需要重建表；\nmodify修改字段属性需要重建表。\n\n效果对比与演示（1）初始化测试数据创建数据的过程使用存储过程\n#创建表CREATE TABLE `large_user` (  `id` bigint(20) DEFAULT NULL,  `name` varchar(64) DEFAULT NULL,  `age` int(11) DEFAULT NULL) #创建日志表CREATE TABLE `large_user_log` (  `id` int(11) DEFAULT NULL,  `msg` varchar(1000) DEFAULT NULL COMMENT &#x27;提交信息记录&#x27;) ENGINE=InnoDB DEFAULT CHARSET=utf8;#创建存储过程DELIMITER ;;CREATE PROCEDURE `insert_large_user`(id_begin INT, id_end INT)BEGINDECLARE i int;SET i = id_begin;SET AUTOCOMMIT = 0;WHILE i &gt;= id_begin &amp;&amp; i &lt;= id_end DO    INSERT INTO large_user(id, name, age) VALUES (i, concat(&#x27;user_&#x27;, i % 100000), i % 100);    SET i = i + 1;    IF MOD(i, 100000) &lt;=0 THEN        INSERT INTO large_user_log (id, msg) VALUES(i, &#x27;ready to commit&#x27;);        COMMIT;    END IF;END WHILE;END;;DELIMITER ;#调用存储过程，产生一亿条数据call insert_large_user(1, 100000000);\n\n（2）使用效果对比我们在Demo中分别使用两种数据库，一种是MySQL8.0，另一种是MySQL5.7做相关的测试。\nselect version()\n\n8.0的版本为：8.0.33\n5.7的版本为：5.7.18-txsql-log\n（3）新增字段的SQLALTER TABLE `large_user` ADD COLUMN `id_card` varchar(255) NULL COMMENT &#x27;身份证号&#x27; AFTER `age`\n\nMySQL8.0执行效果：Time: 0.068s\nMySQL5.7执行效果：Time: 158.898s\n（4）删除字段的SQLALTER TABLE `large_user` DROP COLUMN `id_card`\n\nMySQL8.0执行效果：Time: 0.063s\nMySQL5.7执行效果：Time: 282.09s\n更多和参考文档在本次测试中，测试结果可能受到CPU、内存、SSD的影响从而执行时长不固定，但MySQL8.0 instant  算法带来的算法更新可以很直观的看出来执行时长的进步。\n本次文章参考文档\n\nhttps://dev.mysql.com/doc/refman/8.0/en/innodb-online-ddl-operations.html\nhttps://www.toutiao.com/article/6933566079608439308/\nhttps://blog.csdn.net/qq_25138909/article/details/103781012\n\n","tags":["MySQL","数据库","大数据"]},{"title":"SpringMVC源码的分析","url":"/2025/07/22/16/","content":"SpringMVC执行过程流程图Spring MVC 的四大组件:前端控制器（DispatcherServlet）、处理器映射器（HandlerMapping）、处理器适配器（HandlerAdapter）以及视图解析器（ViewResolver） 的角度来看一下 Spring MVC 对用户请求的处理过程，过程如下图所示:其中，DispatcherServlet是前端控制器，它负责接收用户的请求，并调用其他组件来处理。HandlerMapping是处理器映射器，它负责根据请求的URL找到对应的Controller对象，并返回给DispatcherServlet。HandlerAdapter是处理器适配器，它负责根据Controller对象的类型，找到合适的适配器对象，并调用Controller对象的方法，返回ModelAndView对象。ModelAndView对象包含了模型数据和视图名称。ViewResolver是视图解析器，它负责根据视图名称找到对应的视图对象，并返回给DispatcherServlet。View是视图对象，它负责渲染模型数据，并返回响应给用户。这样一来，springmvc就实现了一个基于MVC模式的web框架，将请求处理分成了不同的步骤和组件，提高了可扩展性和灵活性。\n\n执行过程和组件介绍\n用户向服务器发送请求，请求会先达到 SpringMVC 前端控制器 DispatcherServlet ;\nDispatcherServlet 根据该 URI，调用 HandlerMapping 获得该 Handler 配置的所有相关的对象（包括 Handler 对象以及 Handler 对象对应的拦截器），最后以 HandlerExecutionChain 执行链对象的形式返回\nDispatcherServlet 根据获得的 Handler，获取对应的 HandlerAdapter;\n获取到 HandlerAdapter，将开始执行拦截器的 preHandler(…)方法;\n提取 Request 中的模型数据，填充 Handler 入参，开始执行 Handler（Controller)方法，处理请求，在填充 Handler 的入参过程中，根据你的配置，Spring 将帮你做一些额外的工作：\nHttpMessageConveter：将请求消息（如 Json、xml 等数据）转换成一个对象，将对象转换为指定的类型信息\n数据转换：对请求消息进行数据转换。如 String 转换成 Integer、Double 等\n数据格式化：对请求消息进行数据格式化。如将字符串转换成格式化数字或格式化日期等\n数据验证：验证数据的有效性（长度、格式等），验证结果存储到 BindingResult 或 Error 中\n\n\nHandler 方法执行完成后，向 DispatcherServlet 返回一个 ModelAndView 对象。\n开始执行拦截器的 postHandle(…)方\n根据返回的 ModelAndView（此时会判断是否存在异常：如果存在异常，则执行 HandlerExceptionResolver 进行异常处理）选择一个适合的 ViewResolver 进行视图解析，根据 Model 和 View，来渲染视图\n渲染视图完毕执行拦截器的 afterCompletion(…)方法\n将渲染结果返回给客户端，本次http请求完成\n\n分析源码准备工作1. 创建Filter@Componentpublic class BaseFilter implements Filter &#123;    @Override    public void doFilter(ServletRequest servletRequest, ServletResponse servletResponse, FilterChain filterChain) throws IOException, ServletException &#123;        System.out.println(this.getClass().getName()+&quot;:1&quot;);        filterChain.doFilter(servletRequest, servletResponse);    &#125;&#125;\n2. 创建HandlerInterceptor\u0000\u0000拦截器@Componentpublic class BaseInterceptor implements HandlerInterceptor, WebMvcConfigurer &#123;    @Override    public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123;        System.out.println(this.getClass().getName()+&quot;:2&quot;);        return HandlerInterceptor.super.preHandle(request, response, handler);    &#125;    @Override    public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123;        System.out.println(this.getClass().getName()+&quot;:4&quot;);        HandlerInterceptor.super.afterCompletion(request, response, handler, ex);    &#125;    @Override    public void addInterceptors(InterceptorRegistry registry) &#123;        registry.addInterceptor(new BaseInterceptor());    &#125;&#125;\n3. 编写Controller的代码@RequestMapping(&quot;index/&#123;message&#125;&quot;)public String get(@PathVariable String message) &#123;    System.out.println(this.getClass().getName()+&quot;:3&quot;);    return &quot;hello&quot; + &quot;&lt;br/&gt;&quot; + message + &quot;&lt;br&gt;&quot; + LocalDateTime.now();&#125;\n开始分析首先当我们访问页面的时候，将会把请求发送到前端控制器 DispatcherServlet，DispatcherServlet 是一个 Servlet，我们知道在 Servlet 在处理一个请求的时候会交给 service 方法进行处理，这里也不例外，DispatcherServlet 继承了 FrameworkServlet，并且在收到请求后会调用我们的doService()函数\n1. 进入doService(request, response)@Overrideprotected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123;\tlogRequest(request);       // 给 request 中的属性做一份快照，以便能够恢复原始属性\tMap&lt;String, Object&gt; attributesSnapshot = null;\tif (WebUtils.isIncludeRequest(request)) &#123;\t\tattributesSnapshot = new HashMap&lt;&gt;();\t\tEnumeration&lt;?&gt; attrNames = request.getAttributeNames();\t\twhile (attrNames.hasMoreElements()) &#123;\t\t\tString attrName = (String) attrNames.nextElement();\t\t\tif (this.cleanupAfterInclude || attrName.startsWith(DEFAULT_STRATEGIES_PREFIX)) &#123;\t\t\t\tattributesSnapshot.put(attrName, request.getAttribute(attrName));\t\t\t&#125;\t\t&#125;\t&#125;    // 如果没有配置本地化或者主题的处理器之类的，SpringMVC 会使用默认的配置文件，即 DispatcherServlet.properties\trequest.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext());\trequest.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver);\trequest.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver);\trequest.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource());\tif (this.flashMapManager != null) &#123;\t\tFlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response);\t\tif (inputFlashMap != null) &#123;\t\t\trequest.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap));\t\t&#125;\t\trequest.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap());\t\trequest.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager);\t&#125;\ttry &#123;           //真正的开始处理请求\t\tdoDispatch(request, response);\t&#125;\tfinally &#123;\t\tif (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123;\t\t\t// Restore the original attribute snapshot, in case of an include.\t\t\tif (attributesSnapshot != null) &#123;\t\t\t\trestoreAttributesAfterInclude(request, attributesSnapshot);\t\t\t&#125;\t\t&#125;\t&#125;&#125;\n2. doDispatch(request, response)真正开始处理请求接下来 DispatcherServlet 开始真正的处理，让我们来看下 doDispatch 方法，首先会获取当前请求的 Handler 执行链，然后找到合适的 HandlerAdapter（此处为 RequestMappingHandlerAdapter），接着调用 RequestMappingHandlerAdapter 的 handle 方法，如下为 doDispatch 方法：\nprotected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123;\t\tHttpServletRequest processedRequest = request;\t\tHandlerExecutionChain mappedHandler = null;\t\tboolean multipartRequestParsed = false;\t\tWebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request);\t\ttry &#123;\t\t\tModelAndView mv = null;\t\t\tException dispatchException = null;\t\t\ttry &#123;                //先检查是不是 Multipart 类型的，比如上传等；如果是 Multipart 类型的，则转换为 MultipartHttpServletRequest 类型\t\t\t\tprocessedRequest = checkMultipart(request);\t\t\t\tmultipartRequestParsed = (processedRequest != request);\t\t\t\t// 获取当前请求的 Handler 执行链，这一部分就是处理器映射器\t\t\t\tmappedHandler = getHandler(processedRequest);\t\t\t\tif (mappedHandler == null) &#123;\t\t\t\t\tnoHandlerFound(processedRequest, response);\t\t\t\t\treturn;\t\t\t\t&#125;\t\t\t\t//获取当前的处理器适配器\t\t\t\tHandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler());\t\t\t\t// 对于 header 中 last-modified 的处理\t\t\t\tString method = request.getMethod();\t\t\t\tboolean isGet = &quot;GET&quot;.equals(method);\t\t\t\tif (isGet || &quot;HEAD&quot;.equals(method)) &#123;\t\t\t\t\tlong lastModified = ha.getLastModified(request, mappedHandler.getHandler());\t\t\t\t\tif (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123;\t\t\t\t\t\treturn;\t\t\t\t\t&#125;\t\t\t\t&#125;                // 遍历所有定义的 interceptor，执行 preHandle 方法，如果它们返回了false,                // 那么就不再往下执行了\t\t\t\tif (!mappedHandler.applyPreHandle(processedRequest, response)) &#123;\t\t\t\t\treturn;\t\t\t\t&#125;\t\t\t\t// 实际调用Handler的地方，其实就是Controller的对象了\t\t\t\tmv = ha.handle(processedRequest, response, mappedHandler.getHandler());\t\t\t\tif (asyncManager.isConcurrentHandlingStarted()) &#123;\t\t\t\t\treturn;\t\t\t\t&#125;                //处理成默认视图名，也就是添加前缀和后缀等\t\t\t\tapplyDefaultViewName(processedRequest, mv);                // 拦截器postHandle方法进行处理\t\t\t\tmappedHandler.applyPostHandle(processedRequest, response, mv);\t\t\t&#125;\t\t\tcatch (Exception ex) &#123;\t\t\t\tdispatchException = ex;\t\t\t&#125;\t\t\tcatch (Throwable err) &#123;\t\t\t\tdispatchException = new NestedServletException(&quot;Handler dispatch failed&quot;, err);\t\t\t&#125;            //处理最后的结果，渲染之类的都在这里\t\t\tprocessDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException);\t\t&#125;\t\tcatch (Exception ex) &#123;\t\t\ttriggerAfterCompletion(processedRequest, response, mappedHandler, ex);\t\t&#125;\t\tcatch (Throwable err) &#123;\t\t\ttriggerAfterCompletion(processedRequest, response, mappedHandler,\t\t\t\t\tnew NestedServletException(&quot;Handler processing failed&quot;, err));\t\t&#125;\t\tfinally &#123;\t\t\tif (asyncManager.isConcurrentHandlingStarted()) &#123;\t\t\t\t// Instead of postHandle and afterCompletion\t\t\t\tif (mappedHandler != null) &#123;\t\t\t\t\tmappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response);\t\t\t\t&#125;\t\t\t&#125;\t\t\telse &#123;\t\t\t\t// Clean up any resources used by a multipart request.\t\t\t\tif (multipartRequestParsed) &#123;\t\t\t\t\tcleanupMultipart(processedRequest);\t\t\t\t&#125;\t\t\t&#125;\t\t&#125;\t&#125;\n3. 查找对应的 Handler 对象 getHandler(processedRequest)让我们去探索下是如何获取当前请求的 Handler 执行链，对应着这句代码 mappedHandler &#x3D; getHandler(processedRequest);，看下 DispatcherServlet 具体的 getHandler 方法，该方法主要是遍历所有的 handlerMappings 进行处理，handlerMappings 是在启动的时候预先注册好的，handlerMappings 包含 RequestMappingHandlerMapping、BeanNameUrlHandlerMapping、RouterFunctionMapping、SimpleUrlHandlerMapping 以及 WelcomePageHandlerMapping，在循环中会调用 AbstractHandlerMapping 类中的 getHandler 方法来获取 Handler 执行链，若获取的 Handler 执行链不为 null，则返回当前请求的 Handler 执行链，DispatcherServlet 类的 getHandler 方法如下：\n@Nullableprotected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123;\tif (this.handlerMappings != null) &#123;           //遍历所有的handlerMappings进行处理，handlerMappings是在启动的时候注册好的\t\tfor (HandlerMapping mapping : this.handlerMappings) &#123;\t\t\tHandlerExecutionChain handler = mapping.getHandler(request);\t\t\tif (handler != null) &#123;\t\t\t\treturn handler;\t\t\t&#125;\t\t&#125;\t&#125;\treturn null;&#125;\n3.1 在循环中，根据 mapping.getHandler(request);，继续往下看 AbstractHandlerMapping 类中的 getHandler 方法：public final HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception &#123;       //根据request获取handler\tObject handler = getHandlerInternal(request);\tif (handler == null) &#123;\t\thandler = getDefaultHandler();\t&#125;\tif (handler == null) &#123;\t\treturn null;\t&#125;\t// 如果Handler时String，表明时一个bean名称，需要寻找对应bean\tif (handler instanceof String) &#123;\t\tString handlerName = (String) handler;\t\thandler = obtainApplicationContext().getBean(handlerName);\t&#125;       // 封装 Handler 执行链返回\tHandlerExecutionChain executionChain = getHandlerExecutionChain(handler, request);\treturn executionChain;\n3.2 getHandlerInternal(request)AbstractHandlerMapping 类中的 getHandler 方法中首先根据 requrst 获取 handler，主要是调用了 AbstractHandlerMethodMapping 类中的 getHandlerInternal 方法，该方法首先获取 request 中的 url，即 &#x2F;hello&#x2F;{message}，用来匹配 handler 并封装成 HandlerMethod，然后根据 handlerMethod 中的 bean 来实例化 Handler 并返回。\n@Overrideprotected HandlerMethod getHandlerInternal(HttpServletRequest request) throws Exception &#123;       // 获取 request 中的 url，用来匹配 handler ： index/‘haha’\tString lookupPath = getUrlPathHelper().getLookupPathForRequest(request);\tthis.mappingRegistry.acquireReadLock();\ttry &#123;           // 根据路径寻找 Handler，并封装成 HandlerMethod\t\tHandlerMethod handlerMethod = lookupHandlerMethod(lookupPath, request);         // 根据 handlerMethod 中的 bean 来实例化 Handler，并添加进 HandlerMethod\t\treturn (handlerMethod != null ? handlerMethod.createWithResolvedBean() : null);\t&#125;\tfinally &#123;\t\tthis.mappingRegistry.releaseReadLock();\t&#125;&#125;\n接下来，我们看 lookupHandlerMethod 的逻辑，主要逻辑委托给了 mappingRegistry 这个成员变量来处理：\n@Nullableprotected HandlerMethod lookupHandlerMethod(String lookupPath, HttpServletRequest request) throws Exception &#123;\tList&lt;Match&gt; matches = new ArrayList&lt;&gt;();       //通过 lookupPath属性中查找。如果找到了，就返回对应的RequestMappingInfo\tList&lt;T&gt; directPathMatches = this.mappingRegistry.getMappingsByUrl(lookupPath);\tif (directPathMatches != null) &#123;           // 如果匹配到了，检查其他属性是否符合要求，如请求方法，参数，header 等\t\taddMatchingMappings(directPathMatches, matches, request);\t&#125;\tif (matches.isEmpty()) &#123;\t\t// 没有直接匹配到，则遍历所有的处理方法进行通配符匹配，这里大部分处理path类的           // 请求，例如: /user/&#123;userId&#125;等请求\t\taddMatchingMappings(this.mappingRegistry.getMappings().keySet(), matches, request);\t&#125;\tif (!matches.isEmpty()) &#123;           // 如果方法有多个匹配，不同的通配符等，则排序选择出最合适的一个\t\tComparator&lt;Match&gt; comparator = new MatchComparator(getMappingComparator(request));\t\tmatches.sort(comparator);\t\tMatch bestMatch = matches.get(0);           // 如果有多个匹配的，会找到第二个最合适的进行比较\t\tif (matches.size() &gt; 1) &#123;\t\t\tif (logger.isTraceEnabled()) &#123;\t\t\t\tlogger.trace(matches.size() + &quot; matching mappings: &quot; + matches);\t\t\t&#125;\t\t\tif (CorsUtils.isPreFlightRequest(request)) &#123;\t\t\t\treturn PREFLIGHT_AMBIGUOUS_MATCH;\t\t\t&#125;\t\t\tMatch secondBestMatch = matches.get(1);\t\t\tif (comparator.compare(bestMatch, secondBestMatch) == 0) &#123;\t\t\t\tMethod m1 = bestMatch.handlerMethod.getMethod();\t\t\t\tMethod m2 = secondBestMatch.handlerMethod.getMethod();\t\t\t\tString uri = request.getRequestURI();                   // 不能有相同的最优 Match\t\t\t\tthrow new IllegalStateException(\t\t\t\t\t\t&quot;Ambiguous handler methods mapped for &#x27;&quot; + uri + &quot;&#x27;: &#123;&quot; + m1 + &quot;, &quot; + m2 + &quot;&#125;&quot;);\t\t\t&#125;\t\t&#125;\t\trequest.setAttribute(BEST_MATCHING_HANDLER_ATTRIBUTE, bestMatch.handlerMethod);           // 设置 request 参数（RequestMappingHandlerMapping 对其进行了覆写）\t\thandleMatch(bestMatch.mapping, lookupPath, request);\t\treturn bestMatch.handlerMethod;\t&#125;\telse &#123;           // 调用 RequestMappingHandlerMapping 类的 handleNoMatch 方法再匹配一次\t\treturn handleNoMatch(this.mappingRegistry.getMappings().keySet(), lookupPath, request);\t&#125;&#125;\n通过上面的过程，我们就获取到了 Handler，就开始封装执行链了，就是将我们配置的拦截器加入到执行链中去，getHandlerExecutionChain 方法如下：\n3.3 (封装执行链、重要) getHandlerExecutionChain\u0000()这里会传入业务的handler，也就是我们的业务controller。或者是以beanName生成的Handlermappings，反正就是具体的业务方法，然后把拦截器放在执行链的前面就可以\nprotected HandlerExecutionChain getHandlerExecutionChain(Object handler, HttpServletRequest request) &#123;\t// 如果当前 Handler 不是执行链类型，就使用一个新的执行链实例封装起来       HandlerExecutionChain chain = (handler instanceof HandlerExecutionChain ?\t\t\t(HandlerExecutionChain) handler : new HandlerExecutionChain(handler));       //获取请求的path\tString lookupPath = this.urlPathHelper.getLookupPathForRequest(request);\t// 遍历拦截器，找到跟当前 url 对应的，添加进执行链中去       for (HandlerInterceptor interceptor : this.adaptedInterceptors) &#123;\t\tif (interceptor instanceof MappedInterceptor) &#123;\t\t\tMappedInterceptor mappedInterceptor = (MappedInterceptor) interceptor;\t\t\tif (mappedInterceptor.matches(lookupPath, this.pathMatcher)) &#123;\t\t\t\tchain.addInterceptor(mappedInterceptor.getInterceptor());\t\t\t&#125;\t\t&#125;\t\telse &#123;\t\t\tchain.addInterceptor(interceptor);\t\t&#125;\t&#125;\treturn chain;&#125;\n到此为止，我们就获取了当前请求的 Handler 执行链，接下来看下是如何获取请求的 Handler 适配器，主要依靠 DispatcherServlet 类的 getHandlerAdapter 方法，该方法就是遍历所有的 HandlerAdapter，找到和当前 Handler 匹配的就返回，在这里匹配到的为 RequestMappingHandlerAdapter。DispatcherServlet 类的 getHandlerAdapter 方法如下：\nprotected HandlerAdapter getHandlerAdapter(Object handler) throws ServletException &#123;    if (this.handlerAdapters != null) &#123;        // 遍历所有的 HandlerAdapter，找到和当前 Handler 匹配的就返回        for (HandlerAdapter adapter : this.handlerAdapters) &#123;            if (adapter.supports(handler)) &#123;                return adapter;            &#125;        &#125;    &#125;    throw new ServletException(&quot;No adapter for handler [&quot; + handler +            &quot;]: The DispatcherServlet configuration needs to include a HandlerAdapter that supports this handler&quot;);&#125;\n4. （调用业务）HandlerAdapter 执行当前的 Handler再获取完当前请求的 Handler 适配器后，接着进行缓存处理，也就是对 last-modified 的处理，然后调用 applyPreHandle 方法执行拦截器的 preHandle 方法，即遍历所有定义的 interceptor，执行 postHandle 方法，然后就到了实际执行 handle 的地方，doDispatch 方法中 handle 方法是执行当前 Handler，我们这里使用的是 RequestMappingHandlerAdapter，首先会进入 AbstractHandlerMethodAdapter 的 handle 方法：\n@Override@Nullablepublic final ModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler)\t\tthrows Exception &#123;\treturn handleInternal(request, response, (HandlerMethod) handler);&#125;\n4.1  AbstractHandlerMethodAdapter 的 handle 方法中又调用了 RequestMappingHandlerAdapter 类的 handleInternal 方法：@Override\tprotected ModelAndView handleInternal(HttpServletRequest request,\t\t\tHttpServletResponse response, HandlerMethod handlerMethod) throws Exception &#123;\t\tModelAndView mav;\t\tcheckRequest(request);\t\t// Execute invokeHandlerMethod in synchronized block if required.\t\tif (this.synchronizeOnSession) &#123;\t\t\tHttpSession session = request.getSession(false);\t\t\tif (session != null) &#123;\t\t\t\tObject mutex = WebUtils.getSessionMutex(session);\t\t\t\tsynchronized (mutex) &#123;\t\t\t\t\tmav = invokeHandlerMethod(request, response, handlerMethod);\t\t\t\t&#125;\t\t\t&#125;\t\t\telse &#123;\t\t\t\t// No HttpSession available -&gt; no mutex necessary\t\t\t\tmav = invokeHandlerMethod(request, response, handlerMethod);\t\t\t&#125;\t\t&#125;\t\telse &#123;            // 执行方法，封装 ModelAndView\t\t\tmav = invokeHandlerMethod(request, response, handlerMethod);\t\t&#125;\t\tif (!response.containsHeader(HEADER_CACHE_CONTROL)) &#123;\t\t\tif (getSessionAttributesHandler(handlerMethod).hasSessionAttributes()) &#123;\t\t\t\tapplyCacheSeconds(response, this.cacheSecondsForSessionAttributeHandlers);\t\t\t&#125;\t\t\telse &#123;\t\t\t\tprepareResponse(response);\t\t\t&#125;\t\t&#125;\t\treturn mav;\t&#125;\n在执行完 handle 方法后，然后调用 applyDefaultViewName 方法组装默认视图名称，将前缀和后缀名都加上，接着调用 applyPostHandle 方法执行拦截器的 preHandle 方法，也就是遍历所有定义的 interceptor，执行 postHandle 方法。\n5. 处理最终结果以及渲染最后调用 DispatcherServlet 类中的 processDispatchResult 方法，此方法是处理最终结果的，包括异常处理、渲染页面和发出完成通知触发拦截器的 afterCompletion() 方法执行等，processDispatchResult()方法代码如下：\nprivate void processDispatchResult(HttpServletRequest request, HttpServletResponse response,\t\t\t@Nullable HandlerExecutionChain mappedHandler, @Nullable ModelAndView mv,\t\t\t@Nullable Exception exception) throws Exception &#123;\t\tboolean errorView = false;\t\tif (exception != null) &#123;\t\t\tif (exception instanceof ModelAndViewDefiningException) &#123;\t\t\t\tlogger.debug(&quot;ModelAndViewDefiningException encountered&quot;, exception);\t\t\t\tmv = ((ModelAndViewDefiningException) exception).getModelAndView();\t\t\t&#125;\t\t\telse &#123;\t\t\t\tObject handler = (mappedHandler != null ? mappedHandler.getHandler() : null);\t\t\t\tmv = processHandlerException(request, response, handler, exception);\t\t\t\terrorView = (mv != null);\t\t\t&#125;\t\t&#125;\t\t// 渲染\t\tif (mv != null &amp;&amp; !mv.wasCleared()) &#123;\t\t\trender(mv, request, response);\t\t\tif (errorView) &#123;\t\t\t\tWebUtils.clearErrorRequestAttributes(request);\t\t\t&#125;\t\t&#125;\t\telse &#123;\t\t\tif (logger.isTraceEnabled()) &#123;\t\t\t\tlogger.trace(&quot;No view rendering, null ModelAndView returned.&quot;);\t\t\t&#125;\t\t&#125;\t\tif (WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123;\t\t\t// Concurrent handling started during a forward\t\t\treturn;\t\t&#125;\t\tif (mappedHandler != null) &#123;\t\t\tmappedHandler.triggerAfterCompletion(request, response, null);\t\t&#125;\t&#125;\n接下来让我们看下 DispatcherServlet 类的 render 方法是如何完成渲染的，DispatcherServlet 类的 render 方法渲染过程如下：\n\n判断 ModelAndView 中 view 是否为 view name，没有获取其实例对象：如果是根据 name，如果是则需要调用 resolveViewName 从视图解析器获取对应的视图(View)对象；否则 ModelAndView 中使用 getview 方法获取 view 对象。\n然后调用 View 类的 render 方法。\n\nDispatcherServlet 类的 render 方法如下：\nprotected void render(ModelAndView mv, HttpServletRequest request, HttpServletResponse response) throws Exception &#123;    // 设置本地化    Locale locale = (this.localeResolver != null ? this.localeResolver.resolveLocale(request) : request.getLocale());    response.setLocale(locale);    View view;    String viewName = mv.getViewName();    if (viewName != null) &#123;        // 解析视图名，得到视图        view = resolveViewName(viewName, mv.getModelInternal(), locale, request);        if (view == null) &#123;            throw new ServletException(&quot;Could not resolve view with name &#x27;&quot; + mv.getViewName() +                    &quot;&#x27; in servlet with name &#x27;&quot; + getServletName() + &quot;&#x27;&quot;);        &#125;    &#125;    else &#123;        view = mv.getView();        if (view == null) &#123;            throw new ServletException(&quot;ModelAndView [&quot; + mv + &quot;] neither contains a view name nor a &quot; +                    &quot;View object in servlet with name &#x27;&quot; + getServletName() + &quot;&#x27;&quot;);        &#125;    &#125;    if (logger.isTraceEnabled()) &#123;        logger.trace(&quot;Rendering view [&quot; + view + &quot;] &quot;);    &#125;    try &#123;        if (mv.getStatus() != null) &#123;            response.setStatus(mv.getStatus().value());        &#125;        // 委托给视图进行渲染        view.render(mv.getModelInternal(), request, response);    &#125;    catch (Exception ex) &#123;        if (logger.isDebugEnabled()) &#123;            logger.debug(&quot;Error rendering view [&quot; + view + &quot;]&quot;, ex);        &#125;        throw ex;    &#125;&#125;\n","tags":["Spring","SpringMVC","源码分析"]},{"title":"Using filesort 还是Using index condition？","url":"/2025/03/18/26/","content":"开篇\n是一个弱智的操作产生了下面的分析，如果一会最后忍不住喷还是先行ctrl+w为上\n\n在看运营提的需求池时，发现有一个列表接口检索过慢。但实际未使用任何条件检索，也就是说一个普通的百万级表，查询需要10s+，激起了我的好奇心\n在使用Arthas的Trace分析后，发现竟然是真的，原本以为是数据后处理过多的判断逻辑导致。。\n$ trace com.demo.modules.mk.service.impl.QuestionnaireAnswerServiceImpl pagePress Ctrl+C to abort.Affect(class-cnt:1 , method-cnt:1) cost in 90 ms, listenerId: 1`---ts=2025-03-19 16:08:45;thread_name=main-thread;id=1;is_daemon=false;priority=5;TCCL=sun.misc.Launcher$AppClassLoader@18b4aac2    `---[99960ms] com.demo.modules.mk.service.impl.QuestionnaireAnswerServiceImpl#page()        `---[91101ms] com.demo.modules.mk.service.dao.BaseDao#selectPage()\n\n显然，慢查询是罪魁祸首。接下来深入分析具体原因。\n分析过程SQL下面是具体执行出的SQL\nSELECT id, #........此处省略10+字段 is_deleted FROM mk_questionnaire_answer WHERE is_deleted = 0  AND (   approve_status not IN (10,11,12,13)) ORDER BY approve_status ASC, commit_date ASC  LIMIT 10\n\nExplan执行计划分析\n\n\n列名\n值\n备注或说明\n\n\n\nid\n1\n查询的序列号，简单查询通常为 1。\n\n\nselect_type\nSIMPLE\n查询类型，表示这是一个简单查询，没有子查询或联合查询。\n\n\ntable\nmk_questionnaire_answer\n目标查询的表名。\n\n\ntype\nrange\n访问类型，range表示基于索引的范围扫描。\n\n\npossible_keys\nindex_6\n查询可能用到的索引名称。\n\n\nkey\nindex_6\n实际使用的索引名称。\n\n\nkey_len\n2\n表示索引中实际参与查询的字段长度，单位为字节。\n\n\nrows\n294439\n估算需要扫描的行数，行数较高表明查询范围较大。\n\n\nfiltered\n10.00\n查询条件的过滤率，表示约 10% 的行符合查询条件。\n\n\nExtra\nUsing index condition; Using MRR; Using filesort\nUsing index condition：使用了索引下推优化。Using MRR（多范围读取）：优化范围查询的 I&#x2F;O。Using filesort：使用文件排序（可能带来性能影响）。\n\n\n看这个执行计划，是走了索引的。仅是排序时使用了 Using filesort这个内存大户，所以看看能不能使用索引优化即可，先去掉order by看看。\nSELECT id, #........此处省略10+字段 is_deleted FROM mk_questionnaire_answer WHERE is_deleted = 0  AND (   approve_status not IN (10,11,12,13))  LIMIT 10 #执行时间：0.3s\n\n从执行时间上看，绝对是排序打乱了索引的有序性，被迫把数据拉到sort buffer中重新计算了。换句话说，查询部分使用了索引，但排序部分没有走索引。\n看下这个表的index6索引有什么东西\nCREATE TABLE `mk_questionnaire_answer`  (  `id` bigint NOT NULL AUTO_INCREMENT,  //忽略部分创建表信息  PRIMARY KEY (`id`) USING BTREE,  //这里就是关键的index6  INDEX `index_6`(`approve_status`, `is_deleted`, `type`, `gmt_create`, `commit_date`) USING BTREE) \n\n解决问题好吧。。。索引竟然不是按照最左前缀去加的，把索引换成严格的最左前缀匹配\nINDEX `index_6`(`approve_status`, `commit_date`, `is_deleted`, `type`, `gmt_create`) USING BTREE\n\n再执行这个SQL看看，最后仅需要0.73s。优化 改BUG成功，是的 就是这么的丝滑\n再Explan执行计划分析最后再看下这个优化后的执行计划，Extra字段中提示不再使用sort buffer了，速度也快了起来\n\n\n\n列名\n值\n备注或说明\n\n\n\nid\n1\n查询的序列号，简单查询通常为 1。\n\n\nselect_type\nSIMPLE\n查询类型，表示这是一个简单查询，没有子查询或联合查询。\n\n\ntable\nmk_questionnaire_answer\n目标查询的表名。\n\n\ntype\nrange\n访问类型，range 表示基于索引的范围扫描。\n\n\npossible_keys\nindex_6\n查询可能用到的索引名称。\n\n\nkey\nindex_6\n实际使用的索引名称。\n\n\nkey_len\n2\n表示索引中实际参与查询的字段长度，单位为字节。\n\n\nrows\n294423\n估算需要扫描的行数，行数略微减少，可能因查询条件优化所致。\n\n\nfiltered\n10.00\n查询条件的过滤率，表示约 10% 的行符合查询条件。\n\n\nExtra\nUsing index condition\nUsing index condition：使用了索引下推优化。文件排序 filesort 已优化掉，性能提升。\n\n\n总结为什么会慢？简单理解就是要不要再次排序的问题，正常的asc、desc 排序，从索引结构上用不着单独进行排序。发生上边的filesort后，会出现索引检索数据几乎无意义的情况，数据会慢很多。\n上面情况是使用了两个字段同时进行参照排序，所以需要使用复合索引才能做到使用索引排列，前面index6虽然把字段都加上了，因顺序不同，所以MySQL无法按照最左原则去匹配，这时候我们只需要调换下索引顺序，等MySQL去重建索引就好了。\n\n小声BB：大家之后建索引还是要看一下顺序~~\n\n什么是sort buffer?mysql的排序用到了sort buffer，sort buffer是一个内存块。\nmysql会先取出需要排序的数据，然后把数据放入sort buffer，当所有数据都放入sort buffer或者sort buffer满了就开始排序，然后将排序好的结果返回给客户端。\n参数sort_buffer_size显示的就是sort buffer的大小。\n如果数据量超过sort buffer，那么就会通过磁盘临时文件辅助进行排序，如果数据量比较小，则可以直接在内存中进行。\n在内存中排序会使用快排算法，而通过磁盘临时文件则会使用归并排序算法。\n排序步骤可以分为以下几步：\n\n取出select的数据存入sort buffer。\n在sort buffer中进行快排或者归并排序算法。\n如果有limit按照limit取相应的结果集进行返回。\n\n参考文章\nMySQL 查询性能优化：处理“Using index condition； Using temporary； Using filesort”-CSDN博客\nhttps://juejin.cn/post/7144911491454468103\n\n","tags":["MySQL","Arthas","慢查询"]},{"title":"Spring如何解决循环依赖(三级缓存)","url":"/2025/07/22/18/","content":"大纲整篇文章大纲如下：\n基础知识\n1.1什么是循环依赖？一个或多个对象之间存在直接或间接的依赖关系，这种依赖关系构成一个环形调用，有下面 3 种方式我们看一个简单的 Demo，对标“情况 2”。\n@Componentpublic class AService &#123;    @Autowired    private BService bService;&#125;@Componentpublic class BService &#123;    @Autowired    private AService aService;&#125;\n这是一个经典的循环依赖，它能正常运行，后面我们会通过源码的角度，解读整体的执行流程。\n1.2 三级缓存概念spring就是通过三级缓存去解决的循环依赖的问题。\n\n第一级缓存：singletonObjects，用于保存实例化、注入、初始化完成的Bean实例\n第二级缓存：earlySingletonObjects，用于保存实例化完成的Bean\n第三级缓存：singletonFactories，用于保存Bean创建工厂，以便后面有机会创建对象\n\n我们查看下具体的获取单例源码：执行逻辑：\n\n先从“第一级缓存”中找对象，有就返回，没有就找“二级缓存”\n找“二级缓存”，有就返回，没有就找“三级缓存”\n找“三级缓存”，找到了就获取对象，放到“二级缓存”，从“三级缓存”中移除\n\n1.3 原理执行流程\n\n在第一层中，先去获取 A 的 Bean，发现没有就准备去创建一个，然后将 A 的代理工厂放入“三级缓存”（这个 A 其实是一个半成品，还没有对里面的属性进行注入），但是 A 依赖 B 的创建，就必须先去创建 B；\n在第二层中，准备创建 B，发现 B 又依赖 A，需要先去创建 A；\n在第三层中，去创建 A，因为第一层已经创建了 A 的代理工厂，直接从“三级缓存”中拿到 A 的代理工厂，获取 A 的代理对象，放入“二级缓存”，并清除“三级缓存”；\n回到第二层，现在有了 A 的代理对象，对 A 的依赖完美解决（这里的 A 仍然是个半成品），B 初始化成功；\n回到第一层，现在 B 初始化成功，完成 A 对象的属性注入，然后再填充 A 的其它属性，以及 A 的其它步骤（包括 AOP），完成对 A 完整的初始化功能（这里的 A 才是完整的 Bean）。\n将 A 放入“一级缓存”。\n\n源码分析我们以AService、BService之间的依赖关系，来调试这一段代码。\n找到代码入口在Spring的AbstractApplicationContext#refresh函数中，重点查看 finishBeanFactoryInitialization(beanFactory)函数我们在跳进去finishBeanFactoryInitialization(beanFactory)函数后内的preInstantiateSingletons函数，这个函数的作用是预创建所有的单例Bean。也就是说容器启动时加载Bean的地方。找到AService\n第一层\ncreateBean 创建AService\n又去调用doCreateBean\n调用 addSingletonFactory() (放入三级缓存)进入 doCreateBean() 后，调用 addSingletonFactory()\n\n在这个函数中还做了创建Bean的实例，createBeanInstance\n\n\n查看是否放入三级缓存我们查看下是否放入到了三级缓存中了通过条件表达式可以看出来确实放入了三级缓存中\n\naddSingletonFactory函数如下：\n\n\n进入populateBean属性注入\n执行 postProcessProperties()进入到 populateBean()，执行 postProcessProperties()，这里是一个策略模式，找到下图的策略对象。找到org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor#postProcessProperties后，我们简单查看下元数据\n执行依赖注入属性 inject()下面的流程都是为了获取AService的成员对象，然后进行注入到这里，基本第一层就执行完了。应该去执行getBea(BService)的逻辑了\n第二层\ndoResolveDependency()获取BService的bean，从doGetBean()，到doResolveDependency()，和第一层逻辑完全一样，好到 BService依赖的对象名 AService前面流程一样的，直接到org.springframework.beans.factory.support.DefaultListableBeanFactory#doResolveDependency就可以啦\n再次获取AService的Bean到这里，第二层基本已经完成了。需要再次进入到第三次套娃，啊哈哈\n第三层在第一层和第二层中，我们每次都会从 getSingleton() 获取对象，但是由于之前没有初始化 louzai1 和 louzai2 的三级缓存，所以获取对象为空。\n\n到了第三层，由于第三级缓存有 AService 数据，这里使用三级缓存中的工厂，为 AService 创建一个代理对象，塞入二级缓存。这里就拿到了AService的对象，解决了BService的依赖关系，返回到第二层。\n返回第二层返回到第二层后，BService的初始化结束，这里就结束了么？二级缓存的数据，啥时候会给到一级呢？答案来了，在 doGetBean() 中，我们会通过 createBean() 创建一个 BService 的 bean，当 BService 的 bean 创建成功后，我们会执行 getSingleton()，它会对 BService 的结果进行处理。\n返回第一层在AServic初始化完成后，会把AService的二级缓存清除，并将对象放入一级缓存中。至此依赖循环解决完毕\n原理总结\n为什么要有三级缓存？我们先说“一级缓存”的作用，变量命名为 singletonObjects，结构是 Map&lt;String, Object&gt;，它就是一个单例池，将初始化好的对象放到里面，给其它线程使用，如果没有第一级缓存，程序不能保证 Spring 的单例属性。“二级缓存”先放放，我们直接看“三级缓存”的作用，变量命名为 singletonFactories，结构是 Map&lt;String, ObjectFactory&lt;?&gt;&gt;，Map 的 Value 是一个对象的代理工厂，所以“三级缓存”的作用，其实就是用来存放对象的代理工厂。那这个对象的代理工厂有什么作用呢，我先给出答案，它的主要作用是存放半成品的单例 Bean，目的是为了“打破循环”，可能大家还是不太懂，这里我再稍微解释一下。我们回到文章开头的例子，创建 A 对象时，会把实例化的 A 对象存入“三级缓存”，这个 A 其实是个半成品，因为没有完成 A 的依赖属性 B 的注入，所以后面当初始化 B 时，B 又要去找 A，这时就需要从“三级缓存”中拿到这个半成品的 A（这里描述，其实也不完全准确，因为不是直接拿，为了让大家好理解，我就先这样描述），打破循环。那我再问一个问题，为什么“三级缓存”不直接存半成品的 A，而是要存一个代理工厂呢 ？答案是因为 AOP。在解释这个问题前，我们看一下这个代理工厂的源码，让大家有一个更清晰的认识。直接找到创建 A 对象时，把实例化的 A 对象存入“三级缓存”的代码，直接用前面的两幅截图。下面我们主要看这个对象工厂是如何得到的，进入 getEarlyBeanReference() 方法。\n最后一幅图太重要了，我们知道这个对象工厂的作用：\n\n如果 A 有 AOP，就创建一个代理对象；\n如果 A 没有 AOP，就返回原对象。\n\n那“二级缓存”的作用就清楚了，就是用来存放对象工厂生成的对象，这个对象可能是原对象，也可能是个代理对象。我再问一个问题，为什么要这样设计呢？把二级缓存干掉不行么 ？我们继续往下看。\n能去掉第二级缓存吗？@Servicepublic class A &#123;    @Autowired    private B b;    @Autowired    private C c;&#125;@Servicepublic class B &#123;    @Autowired    private A a;&#125;@Servicepublic class C &#123;    @Autowired    private A a;&#125;\n根据上面的套娃逻辑，A 需要找 B 和 C，但是 B 需要找 A，C 也需要找 A。假如 A 需要进行 AOP，因为代理对象每次都是生成不同的对象，如果干掉第二级缓存，只有第一、三级缓存：\n\nB 找到 A 时，直接通过三级缓存的工厂的代理对象，生成对象 A1。\nC 找到 A 时，直接通过三级缓存的工厂的代理对象，生成对象 A2。\n\n看到问题没？你通过 A 的工厂的代理对象，生成了两个不同的对象 A1 和 A2，所以为了避免这种问题的出现，我们搞个二级缓存，把 A1 存下来，下次再获取时，直接从二级缓存获取，无需再生成新的代理对象。所以“二级缓存”的目的是为了避免因为 AOP 创建多个对象，其中存储的是半成品的 AOP 的单例 bean。如果没有 AOP 的话，我们其实只要 1、3 级缓存，就可以满足要求。\n最后总结我们再回顾一下 3 级缓存的作用：\n\n一级缓存：为“Spring 的单例属性”而生，就是个单例池，用来存放已经初始化完成的单例 Bean；\n二级缓存：为“解决 AOP”而生，存放的是半成品的 AOP 的单例 Bean；\n三级缓存：为“打破循环”而生，存放的是生成半成品单例 Bean 的工厂方法。\n\n\n附件腾讯文档\n","tags":["Spring","源码分析"]},{"title":"insert SQL并发插入导致的阻塞问题","url":"/2025/03/01/25/","content":"在一个正常的业务中，MySQL报错了阻塞。第一反应是死锁，但是发现是一条insert语句，语句如下:\nINSERT INTO make_card_info (  id,  open_id,  md_order_no,  apollo_user_id,  project_name,  task_type,  task_pass_time,  amount,  app_mark) VALUES (  1876546101549854799,  &#x27;1851452288917311489&#x27;,  1870999825732784129,  &#x27;1859075883175706625&#x27;,  &#x27;模糊处理&#x27;,  &#x27;模糊处理&#x27;,  &#x27;2025-01-07 16:27:09.737&#x27;,  10,  &#x27;mark-cloud&#x27;);\n\n此时有点懵圈，手动执行了下这条SQL，发现确实被阻塞了。思路步骤如下\n\n分析表结构\n查看MySQL状态\n查看代码,分析调用链\n\n现在咱们挨个分析一下\n分析表结构一般出现这种问题，要么就是ID重复，要么就是索引里面存在唯一索引。ID重复倒是可以排除了，这个表的id是自增的，那么就是唯一索引的问题\n\n看完这个表结构，猜想大概率是唯一索引问题，造成阻塞的想必也一定是这个唯一索引。\n我的思路是mk_order_no字段难道重复了吗？走了一个这个sql，发现并没有查询出来\nselect * from make_card_info where mk_order_no=1870999825732784129\n\n问题已经出来了，应该是并发插了。这时候去看看MySQL的状态就行了，看看可不可以kill掉它\n查看MySQL状态SHOW PROCESSLIST;#这将显示当前所有连接的状态，包括正在运行的 SQL 语句。SELECT * FROM information_schema.innodb_trx;#这个查询将返回当前所有活动的 InnoDB 事务，包括事务 ID、开始时间、状态等信息。\n\n查看代码,分析调用链@Override@Transactionalpublic String uploadChain(MakeCardInfoDTO dto) &#123;    if(dto.getMdOrderNo() == null)&#123;        throw new RuntimeException(&quot;订单id不能为空&quot;);    &#125;    //查看用户是否创建过钱包    UserWalletDTO userWalletDTO = ConvertUtils.sourceToTarget(dto, UserWalletDTO.class);    dto.setApolloUserId(createUserWallet(userWalletDTO).getUserId());    //防重查看制卡任务是否存在    MakeCardInfoEntity makeCardInfoEntity = makeCardInfoDao.selectOne(new QueryWrapper&lt;MakeCardInfoEntity&gt;()            .lambda()            .eq(MakeCardInfoEntity::getMdOrderNo, dto.getMdOrderNo()));    if(makeCardInfoEntity != null)&#123;        if(makeCardInfoEntity.getTradeStatus().compareTo(WalletStatusEnum.SUCCESS.getStatus()) == 0)&#123;            return makeCardInfoEntity.getTaskId();        &#125;else&#123;            return StringUtils.EMPTY;        &#125;    &#125;    //构建上链请求体    String uploadChainBody = getUploadChainBody(dto);    //先落库    MakeCardInfoEntity entity = ConvertUtils.sourceToTarget(dto, MakeCardInfoEntity.class);    makeCardInfoDao.insert(entity);    //请求上链    JSONObject userWallet = walletRequestInfo.uploadChain(uploadChainBody);    //上链任务ID    String taskId = userWallet.getStr(&quot;taskId&quot;);    entity.setTaskId(taskId);    entity.setReqBody(uploadChainBody);    makeCardInfoDao.updateById(entity);    return taskId;&#125;\n\n大概能猜出来了，该方法问题点如下:\n\n该方法开启了事务，未手动提交，交给spring进行管理\n该方法的执行流程为，先insert拿到id，后http请求一个接口，得到结果后再进行修改\n注意: 该步骤也是在事务下进行的，会有可能出现A事务没提交，B事务又开启的情况\n\n\n该接口未做任何幂等性的限制\n\n综上所述，做个复盘，从请求侧确实能看出来两次请求（MQ重试）\n也就是A线程在执行未提交事务前的逻辑，已经insert。B线程又进来请求insert，两个线程虽主键不冲突，但有唯一索引限制，A事务无法提交update SQL，B事务则被insert SQL阻塞，故出现死锁。\n\n解决方案kill掉，让业务正常运转找到阻塞的SQL，kill掉就ok了。kill掉后通知业务方进行重试  and 人工介入处理\n\ntrx_id\ttrx_state\ttrx_started\ttrx_requested_lock_id\ttrx_wait_started\ttrx_weight\ttrx_mysql_thread_id\ttrx_query\ttrx_operation_state\ttrx_tables_in_use\ttrx_tables_locked\ttrx_lock_structs\ttrx_lock_memory_bytes\ttrx_rows_locked\ttrx_rows_modified\ttrx_concurrency_tickets\ttrx_isolation_level\ttrx_unique_checks\ttrx_foreign_key_checks\ttrx_last_foreign_key_error\ttrx_adaptive_hash_latched\ttrx_adaptive_hash_timeout\ttrx_is_read_only\ttrx_autocommit_non_locking19739684\nRUNNING\n2025-01-07 14:41:15\n(NULL)\t(NULL)3\n269085273\n(NULL)\t(NULL)0\n1\n2\n\nKILL 269085273;\n\n改造代码改造代码的落地方式分为多种，简单说下两种比较实用的思路\n\n给订单号做分布式锁，防止单订单号重复提交，也就是单订单号串行化处理\n这段代码核心就是需要得到insert的id，后修改造成的。 可以改造为不需要数据库返回id，使用雪花等id的分配方式，这样还可以减少一次修改\n\n","tags":["分布式","MySQL","高并发","幂等性","死锁"]},{"title":"video玩出新花样当background","url":"/2018/05/20/1/","content":"hi,好久不见啊。转眼间端午节了，不知道你们又在干嘛呢？别的不扯了，端午节快乐啊哈~\n因为前几天我们要进行网页设计大赛，所以我们小组又焦头烂额的坐在了一起：“又做东西啊，烦死了”！最终我们还是做出来了。也得了个第一名，虽然不怎么好。但也是对自己努力的一个肯定。\n学习中遇见了一些问题就不说了，我们做的页面是支付宝全站，但是支付宝个人中心有一个拿video当做视频背景的元素，我们一直没搞懂。但是恍然大悟，我们可以使用z-index属性来定义啊。好那么我们详细欣赏下面的教程！\n下面先给大家截图一下这个效果吧，大神勿喷~ 但是截图的又不能动~~\n多了话不多说我们先来分析一波，首先我们这个东西用视频当做背景，所以背景一定在下面的，所以设置他的层级代码肯定为负的最多的，因为上面要放内容。那么我们既然弄肯定要全屏还有把他固定在哪里，即使我们不设置数值。然后设置他的显示亮度。多了不说了css代码上了。\n那么看到这里就有疑问了，有人问 -webkit-filter:grayscale(100%) 这段代码是干嘛的，显示的亮度。但是需要支付浏览器，webkit是兼容谷歌的，如果展示在用户面前需要兼容多个浏览器，具体的自行百度下。\n好那么我们css设置完成了，好我们展示下我们html的代码吧。这个代码其实就是正常的插入一段视频。无需多的理解。\n好一个视频已经展现在我们的面前，总是感觉视频的速度可能播放太快或者太慢没有视觉感觉。好我们定义一下他的视频背景，首先这个 id&#x3D;v1 v1是自己取得名字，这个根据开发者自己来定。其中的0.5自行改数值。\n\n就这么简单我们的一个页面已经制作完成啦。现在我们看下效果吧！有什么不懂得可以QQ问我哦~\n","tags":["技术"]},{"title":"Nginx http_secure_link模块使用","url":"/2025/07/15/28/","content":"什么是http_secure_link模块？secure_link机制简单来说就是来保护你的http资源被受控访问的，类似于防盗链、http授权访问的Token。假如你有一些私有化的、需验证身份才能访问的资源，就可以使用这个机制。\n下面是一个简单的流程，分为两步\n\n请求授权：携带目标uri，在后端鉴权通过后计算签名。\n签名的规则通常是uri+secret+expire来生成\n\n\n携签名访问资源： 携带url?sign=xxxx&amp;expire=1731313131来访问\n参数说明：\nsign携带API返回的签名\nexpire 过期时间戳\n\n\n\n\n\n下面是简单的流程图\n\n核心代码代码涉及两个部分，一个是nginx网关部分。另一部分则为核心的API。\nAPI代码使用Go语言，出入参非常简单。\n注意：sign在全链路下是无状态的，也就是泄漏后依然会有风险，推荐expire时间设置为分钟级，过期后拒绝请求可再签来实现安全性。\npackage serviceimport (\t&quot;crypto/md5&quot;\t&quot;encoding/base64&quot;\t&quot;fmt&quot;\t&quot;strings&quot;)// GenerateSignature 生成URI访问签名//// 参数://   uri    - 请求的URI路径//   expire - 签名过期时间戳（秒）//   secret - 签名密钥//// 返回值://   string - 生成的签名字符串//// 实现原理:// 1. 按照Nginx兼容格式拼接expire、uri和secret// 2. 对拼接字符串进行MD5哈希计算// 3. 哈希结果进行Base64 URL编码并去除末尾等号func GenerateSignature(uri string, expire int64, secret string) string &#123;\t// 严格匹配 Nginx 的字符串格式：$&#123;expire&#125;$&#123;\t//\t//uri&#125; $&#123;secret&#125;\traw := fmt.Sprintf(&quot;%d%s %s&quot;, expire, uri, secret)\thash := md5.Sum([]byte(raw))\t// Base64 编码 + 替换特殊字符（与 Nginx 兼容）\treturn strings.TrimRight(\t\tbase64.URLEncoding.EncodeToString(hash[:]), &quot;=&quot;,\t)&#125;\n\nNginxnginx无需任何插件即可实现该功能，而且http_secure_link是nginx的worker内特性。还是无状态check，所以不存在太多的性能问题，不会拖慢nginx的响应速度。\n\nsecret 必须与 GenerateSignature 函数中的secret参数完全一致，否则无法过签名\n\nworker_processes 1;events &#123;    worker_connections 1024;&#125;http &#123;    include       mime.types;    default_type  application/octet-stream;    sendfile      on;    keepalive_timeout 65;    server &#123;        listen 80;        server_name localhost;        location /protected/ &#123;            secure_link $arg_st,$arg_exp;            # secret 记得更换             secure_link_md5 &quot;$arg_exp$uri secret&quot;;            if ($secure_link = &quot;&quot;) &#123;                return 403; # Bad hash            &#125;            if ($secure_link = &quot;0&quot;) &#123;                return 410; # Expired            &#125;            # 明确指定文件物理路径别名            alias /usr/share/nginx/static/;        &#125;        location / &#123;            root /usr/share/nginx/html;            index index.html;        &#125;    &#125;&#125;\n\n总结该场景下可以实现部分api、static、html的保护访问，可以做到http访问保护。但是一定要注意时间问题，尽量保证泄露的风险最小。\n本例如有demo、技术支持需求，欢迎在下面评论～\n参考文章\nhttps://nginx.org/en/docs/http/ngx_http_secure_link_module.html\nhttps://nginx.ac.cn/en/docs/http/ngx_http_secure_link_module.html\nhttps://www.cnblogs.com/panwenbin-logs/p/8728327.html\n\n","tags":["Nginx","GO","防盗链"]},{"title":"不小心提交到Git远程仓库的文件，怎么完全从仓库中清除?","url":"/2022/12/22/14/","content":"使用 Git 做代码版本控制时，有时候会不小心把某些敏感的文件提交到 Git 仓库，可能过后很久才发现。或者是以前提交的文件，现在发现不合适，需要从仓库中清理。 如果但是删除文件，然后提交的话，还是可以从仓库的历史记录中找出这个文件，这个问题就比较严重了。\n所以，如果要彻底从 Git 仓库中删除某个文件可以用如下操作：\ngit filter-branch --index-filter &#x27;git rm -rf --cached --ignore-unmatch path_to_file&#x27; HEAD\n\n其中 path_to_file 就是你要删除的文件在项目中的相对路径，例如：src&#x2F;main&#x2F;resource&#x2F;application.yml 。\n执行此命令后，git 会遍历整个仓库的历史记录找出这个文件，清理，然后重新构造 git 的历史链条。\n接下来强推就行了git push -f，这样远程仓库上也不会再存在这个文件了。\n\n本文章转载于OSCHINA用户红薯：不小心提交到 Git 的敏感文件，怎么完全从仓库中清除 - Java自由人 - OSCHINA - 中文开源技术交流社区\n\n","tags":["Git","远程仓库"]},{"title":"个人主页折腾记","url":"/2019/06/07/9/","content":"活跃一下,不然可能以为我sei了！\n端午节在家无聊,所以把之前的个人主页更新了一下,毕竟这是我的第一个域名也是我的第一个网站,怀念当年的感觉也很喜欢那时候的感觉,所以决定更新一手！\n话不多说,先上图！\n(开局一张图,故事全靠编)\n网站图片首页：\n关于我\n网站的小功能List:1：依旧存在 中英繁 国际化页面\n2：增加一言： 一些可以鼓励自己的心灵鸡汤之类的\n3：更简洁大气的页面\n4：增加邮件联系的方式\n这个网站是我很喜欢的一个网站,并且我有精力就努力的维护下去,很有纪念意义！\n———————–中场抱怨一手————————-\n嗯今天看了好多朋友的网站,发现有的可能已经宕机，有的做的越来越帅,也发现了很多人已经停更。可能都来不及说一声再见把！\n———————–中场抱怨结束————————-\n更新之后的设计使用了现在很流行的弧形UI，配色使用了贵族很喜欢用的紫色！嗯 自我感觉良好\n关于源代码:我在挂别的地方测试的时候就已经很多小伙伴找我要了,反正静态的网站,很喜欢我们就一起用 hhh\n全球最大的同性交友网站链接奉上:\nhttps://github.com/a2501521908/homedemo\n嗯！ 最后祝各位老朋友:\n天天快乐，幸福安康。\n","tags":["技术"]},{"title":"Hexo Keep主题修改首页文章展示时间 version< 4.1.0","url":"/2024/04/01/20/","content":"Note新的时间查阅官网文档后，主题更新版本到 4.1.0 就可以支持这个特性啦，不需要再修改源码。如果你的版本低于4.1.0，那么下面的文章可以帮到你。\n\nhttps://keep-docs.xpoet.cn/basis/configuration-guide/home.html#post-datetime\n\n原因在批量更新hexo的markdown文章文件后，发现首页的时间是以更新文章为准。\n\n个人喜好，此处的文章想展示为createTime ,看了一遍Keep的文档，发现没得到什么解决方案，看起来只能去改源码了\n在修改源码之前，其实还有一些注意事项：\n\n\n不建议直接修改源代码，可能会在更新模板版本时遇见的问题\n如使用NPM更新，则修改后的文件会被新的NPM版本直接覆盖\n如使用Git更新，则可能会出现pull失败，因为代码冲突或者版本的原因\n\n\n\n\n权衡利弊+强迫症考虑后，还是决定改一下\n修改源码找到  /themes/keep/layout/_partial/home-article-meta-info.ejs 文件，修改头部的三目运算符变量规则即可。\n&lt;%//const target_date = post.date !== post.updated ? post.updated : post.dateconst target_date = post.date !== post.updated ? post.date : post.updated%&gt;\n\n","tags":["hexo"]},{"title":"免费CDN jsDelivr使用","url":"/2022/11/14/12/","content":"jsDelivr是什么？jsDelivr是一种特殊的CDN。 它旨在让用户下载npm和Github上托管JavaScript库。 (如果它们托管在Wordpress.org上，则还可以加载Wordpress插件)。也就是说它可以镜像以下服务\n\nwordpress的插件、模板\nGitHub开放仓库\nNPM公共镜像仓库加速\n\n也就是说，我们可以不再自建或自维护CDN！你是NPM的包，它可以同步后为你分发到全球的各个CDN高速节点。你是Github仓库同样也可以使用全球的CDN静态加速。即使你是wordpress的站长，你还可以用它来同步你所使用的模板、插件等。不再需要维护CDN，我们只需要在以上平台上发布，jsDelivr会自动镜像并且分发。\n简单demohttps://cdn.jsdelivr.net/gh/a2501521908/jsdelivr-cdn@1.0/getlanguage.js\n\n我在GitHub上发布了一个仓库jsdelivr-cdn，并且创建了一个版本为1.0，我只需要根据路径取出我所需要的文件即可\n应用应用中暂时不演示其他，只演示GitHub镜像CDN的使用，别的均是换汤不换药，使用官方规定的网络路径访问即可\nGithub规则https://cdn.jsdelivr.net/gh/github用户名/仓库名@版本号/文件路径\n\n0.准备工作\n分支不限，核心原理就是创建一个仓库的release版本，然后jsDriver会同步你的创建的release版本\n\n\n先在Github上创建一个公开的仓库\n使用Git推送至相关分支仓库的代码,也就是你要使用CDN加速的代码\n\n1.创建仓库的Release\n点击Create new release\n\n\n2.填写版本号\ntag如果不理解在这里可以理解为版本号\ntarget是你这个tag需要引用的分支，选择你需要引用的分支，我这里直接选择了main\n点击 new create tag即可\n确认都没问题了，我们选择最下面的Publish release，创建完成版本即可\n\n\n3.测试和应用\n原版本    https://cdn.jsdelivr.net/gh/a2501521908/jsdelivr-cdn@1.0/getlanguage.js\n压缩版本 https://cdn.jsdelivr.net/gh/a2501521908/jsdelivr-cdn@1.0/getlanguage.min.js\n\nWordPress从 WordPress.org 插件 SVN 存储库加载任何插件：\nhttps://cdn.jsdelivr.net/wp/plugins/project/tags/version/file\n\n加载文件的确切版本：\nhttps://cdn.jsdelivr.net/wp/plugins/wp-slimstat/tags/4.6.5/wp-slimstat.js\n\n加载最新版本（不推荐用于生产用途）：\nhttps://cdn.jsdelivr.net/wp/plugins/wp-slimstat/trunk/wp-slimstat.js\n\n请求最新版本是危险的，因为新版本可能会带来重大更改。\n从 WordPress.org 主题 SVN 存储库加载任何主题：\nhttps://cdn.jsdelivr.net/wp/themes/project/version/file\n\n加载文件的确切版本：\nhttps://cdn.jsdelivr.net/wp/themes/twenty-eightteen/1.7/assets/js/html5.js\n\n将“.min”添加到任何 JS&#x2F;CSS 文件以获得缩小版本 - 如果不存在，我们将为您生成它。所有生成的文件都带有源映射，可以在开发过程中轻松使用：\nhttps://cdn.jsdelivr.net/wp/themes/twenty-eightteen/1.7/assets/js/html5.min.js\n\nNPM发布至官方NPM仓库即可\n官方提供的访问路径格式为: https://cdn.jsdelivr.net/npm/包名@版本号/目录\n","tags":["cdn","jsDelivr"]},{"title":"关于停止使用 957xx.cc域名","url":"/2022/11/29/13/","content":"关于停止使用 957xx.cc域名详情因原域名到期并且备案被撤销，所以转向新的域名。即：52xk.cc，寓意为：我爱小轲，新的域名更好记更简单，是本人珍藏了很久的一个域名，最近终于得以机会使用\n同时，域名邮箱也进行了下架，幸好没使用老域名邮箱注册什么平台，不然换绑也够头疼喽！\n收到网站备案IP整改通知解决方案同时在这里给大家说一下，如果云计算厂商主动联系你，并且说你的解析IP与备案IP不一致，要求你将域名解析IP修改为与备案IP一致或者提交变更备案将域名IP修改为域名实际解析IP，本人是如何解决的\n很多朋友可能都是备案后原云主机会被回收，IP自然也不存在无法找回了，变更又得提交一大堆程序并且还得买云服务器！如果遇到此情况还请你不要惊慌，在今天（2022-11-29）目前还是可以使用云计算厂商的CDN产品，CDN再转发到你现有源站即可。\n如果客服的态度很严格，那你依旧有权利要求客服去找他的上级沟通，最终达成一致。\n","tags":["通知","暂停"]},{"title":"关于博客被恶意镜像这件事","url":"/2024/09/25/22/","content":"前言从来没想过会有人开辟这种赛道，竟然有人把我的博客内容完全clone下来。文字内容转换为繁体，然后完全复刻我的博客出来…. \n\n\n冷静下来思考，这其中究竟有什么利益？值得花一个域名的价钱来clone我一个小站？而且还屏蔽大陆的IP访问，大陆直接404\n\n利用部分文章，来获得Google、Bing搜索引擎的排名\n\n盗版我的博客文章，为了后期的劣币驱逐良币\n\n我写的太好了，就想留着看\n\n\n从上面这些可获益情况来看，都是属于为了吃醋而包一顿饺子，虽然暂时想不明白，但还是得着手解决这个问题\n落地过程跟松哥聊天解闷时，他帮我找到了解决问题的思路以及别人的落地过程。在此处特别致谢一下，一起帮我解决了这个棘手问题\n\n我: 有个人克隆了我的博客。。\n我：这是出于啥目的😯\n松哥：😀哇哦\n松哥：我打开是404\n我：dns被污染了？\n我：不对，国内IP一访问就是404；国外IP一访问就是我的博客\n松哥：搞不懂一点\n我：但为啥盯着我的博客呢？难不成是我写的太好了，非得国外镜像一个？\n松哥：😂好家伙，采集现在这么明目张胆了\n我：而且不知道咋做到的，还原度这么高… 我理解hexo g后，在GitHub仓库里应该是纯html才对吧？他竟然能顺路翻译成繁体，还自动发布\n松哥：6666，确实，这个人怕是对你有点意思 哈哈哈\n一段时间过后……….\n松哥：https://yfzhu.cn/posts/1014/，https://xyzbz.cn/archives/1214/  好像你站点也是这样，被恶意镜像了\n\n解决方案1. 增加跳转代码验证该域名是否为localhost、52xk.cc ，不是的话粗暴跳转到https://52xk.cc 即可\n&lt;script&gt;(function() &#123;    // Base64编码的合法域名1，对应域名是 &#x27;52xk.cc&#x27;    var encodedValidDomain1 = &#x27;NTJ4ay5jYw==&#x27;; // 52xk.cc    // Base64编码的合法域名2，对应域名是 &#x27;localhost&#x27;    var encodedValidDomain2 = &#x27;bG9jYWxob3N0&#x27;; // localhost    // Base64编码的跳转URL，对应的URL是 &#x27;https://52xk.cc&#x27;    var encodedRedirectUrl = &#x27;aHR0cHM6Ly81MnhrLmNj&#x27;; // https://52xk.cc    // 解码Base64编码的字符串    function decodeBase64(encodedStr) &#123;        return atob(encodedStr);    &#125;    // 解码后的合法域名1，应该等于 &#x27;52xk.cc&#x27;    var validDomain1 = decodeBase64(encodedValidDomain1);    // 解码后的合法域名2，应该等于 &#x27;localhost&#x27;    var validDomain2 = decodeBase64(encodedValidDomain2);    // 解码后的跳转URL，应该等于 &#x27;https://52xk.cc&#x27;    var redirectUrl = decodeBase64(encodedRedirectUrl);    // 获取当前页面的域名    var hostname = document.location.hostname;    // 如果当前域名不是合法域名1或合法域名2，则跳转到指定的URL    if (hostname !== validDomain1 &amp;&amp; hostname !== validDomain2) &#123;        window.location.href = redirectUrl;    &#125;&#125;)();&lt;/script&gt;\n\n2. 向Google投诉国外的搜索引擎中，Google是主要的搜索来源。玩这套的大概率也就图个Google搜索了，所以肯定收录了，直接举报这个域名即可\nhttps://support.google.com/legal/answer/3110420\n3. 启用防盗链博客尽量屏蔽掉陌生的referer，尤其静态资源。可在Gateway部分去做白名单限制，这里不做详细解释\n4. 锁定IP，拉黑处理找到相关爬取日志，拉黑IP处理即可，也是在Gateway下手即可\n5. 最后的最后收手吧兄弟,这条街上全是阿祖😅\n参考\nhttps://yfzhu.cn/posts/1014/\nhttps://xyzbz.cn/archives/1214/\nhttps://cloud.tencent.com/developer/article/1724145\n\n","tags":["博客","镜像"]},{"title":"半年了，你的梦想又去了哪里?","url":"/2018/08/10/5/","content":"  不知不觉，半年时间已经过去了。时光如梭，怎么样? 你的梦想呢? 是不是更远了一步?\n\n​     昨天无意间，无意间看到了以前的小米手机(红  红 红..红米)手机还是以前上高中上面的照片，看到了以前跟同学们的一些合照，想到了上一年同学聚会的场面，谁谁谁都变了许多呀。哎呀，你看看我，想到了这些就又开始怀旧了起来，不知不觉的，时间总是不等人。总是感觉还有时间，可是回头才发现，真的是不珍惜 什么都没有了。\n​    正当还犯牢骚的时候，忽然想看看春节还有多长时间，似乎出来这么长时间总是感觉时间过的很快。结果一百度，瞬间吓蒙了。自己似乎都不知道过年实在几月，还有多长时间。还得自己百度。\n​    回忆半年前的那些在一块玩耍，放炮的照片，再看看外面北京这个大城市的蓝天白云，楼下的车辆一辆又一辆的过。光阴似箭，离开村子也半年了。\n​    然而，我思想还是我刚刚走进社会。还是一个吚吖学语的幼儿，还是一个不谙人世的少年，还是一个刚刚立志的青年。我就应学习东西的才刚刚尝到一滴甘露，应对知识的海洋，我才初涉小溪，刚刚湿脚；应对社会这个大学校，我才刚刚进门，刚刚看到科学的殿堂，才刚刚听到最基础的ABC、AOE；应对人生这道考题，我才刚刚思考，刚刚有了一点初浅的认识，刚刚看到前进的方向，刚刚看到一丝追求目标的亮光…… \n​    但是，这一切容不得你思考，容不得你学习，容不得你偿试，容不得你练习、演习，岁月却已经催人老了，你的人生却已走过少年、青年,转眼间已经走过五分之一了。\n​    或许人生就是这样，也许人也是这样，总是在不知道珍惜的时候，总以为时间和她(他)都还在，却或过头来，我们都已经分道扬镳了。没办法，眼下只能好好的使劲拼呀,加油 朋友们。\n​    \n"},{"title":"忽然想说说阿里云盘的BUG","url":"/2024/09/17/21/","content":"前言中秋假期期间，有一件事冲上了热搜。让人一惊，阿里云盘忽然出了一个BUG，用户相册数据竟能越过鉴权和隔离策略直接展示在其他用户面前！这件事对所有阿里云盘用户来说，无异于说是灾难的，相当于多人的隐私直接展现在一个大屏幕上，谁都可以看。\n\n\n\n特别感谢我的习惯，哦不，应该感谢我的倔强？在20年前的安卓手机打开网盘APP后默认就会访问访问我的 通讯录/相册/短信 等信息，直接默认我同意就自动上传上去了！因此我也被迫的学会了权限管理，在ios平台虽然没有默认同意，也要经常点击拒绝才可以。\n\n但是具体的记录看了下，过程大概如下：\n\n创建一个新的相册\n筛选过滤数据仅展示相册，就出来了😂\n\nBUG猜想阿里的技术通常使用Java方案，在Java中很流行一种判空策略，下面简单的使用MP的一个例子做一个简单的猜想，仅猜想哈。并没有什么内部消息，是自己的意淫。\n通常我们会在程序中严格的指定用户ID，但是很多人的代码是这样写的：\n// 形参忽略，主要有 userId、albumId、statuspublic List&lt;DriveAlbum&gt; selectDriveAlbum(...) &#123;    LambdaQueryWrapper&lt;DriveAlbum&gt; queryWrapper = new LambdaQueryWrapper&lt;&gt;();    //用户id精确eq，判断不为空再拼接    queryWrapper.eq(StringUtils.isNotBlank(userId), &quot;user_id&quot;, userId);    //相册id精确eq，判断不为空再拼接    queryWrapper.eq(StringUtils.isNotBlank(albumId), &quot;album_id&quot;, albumId);    queryWrapper.eq(status != null, &quot;status&quot;, status);    return driveAlbumMapper.selectList(queryWrapper);&#125;\n\n看完上面的策略，假如实参传递来userId和albumId，则SQL可能会是如下的效果：\n#查询10086用户权限下的10010相册SELECT *  FROM drive_album WHERE  user_id =&quot;10086&quot; and album_id=&quot;10010&quot; ORDER BY create_time DESC;\n\n但是假如实参传的都为null或者不命中判断呢？\n#查询所有用户的所有相册SELECT *  FROM drive_album ORDER BY create_time DESC;\n\n这样就会直接越权访问别人的数据了，为什么为null就不清楚了\n当然，我认为阿里绝对不可能没有预案，肯定有更复杂的检查和策略来杜绝这种跨账号之间的资产访问。\n我上面仅仅是用来猜想的，仅仅是简单的举例子，仅用来参考。阿里的响应速度还是很快的，看到后很快就做了拦截和修复，最快的保证了用户的数字资产安全，点赞👍\n应对策略\n完善的自动化测试：不能仅做接口测试，还需要每次都进行模拟点击测试\n\n满足测试的覆盖率，核心功能的通过率\n在CI&#x2F;CD流程上增加测试覆盖率check，通过后再进入部署流程\n\n\n常态化的code review：code review的过程可以让队友帮我们发现问题，降低问题的发生概率\n\n在编程方面严格限制，一旦检测到重要的数据被越权访问，直接拒绝本次请求。简单例子如下\npublic List&lt;DriveAlbum&gt; selectDriveAlbum(...) &#123;    // 判断 userId 或 albumId 是否为 null    if (StringUtils.isBlank(userId) || albumId == null) &#123;        throw new BizException(&quot;网络有问题，请稍后再试哦&quot;);    &#125;    // 构建 MyBatis Plus 的 LambdaQueryWrapper    LambdaQueryWrapper&lt;DriveAlbum&gt; queryWrapper = new LambdaQueryWrapper&lt;&gt;();    // 忽略  &#125;\n\n完善报警机制，一旦抛出相关报警发送到相关群聊中@相关值班同学，做到最快发现问题、最快响应问题\n\n统一登录系统 &amp; 统一拦截系统在重要的操作中，如果相关隔离标识异常，尽量不做放行。宁可被投诉，也不可把数据越权输出\n\n学习阿里云盘，如果实在因为不可抗力因素不能快速的修复，则直接把数据全部拦截，影响面做到最小\n\n\n","tags":["云盘","BUG","测试"]},{"title":"我的一个道姑朋友-背景","url":"/2018/07/03/4/","content":"陌生人，你好。欢迎来到这里！\n也许你也听过我的一个道姑朋友，感受也许很深，也许很浅。当然这并不重要，让我们一起来了解这首歌的背景，我相信，你看完后会有些感受。\n\n\n由于我找很长时间没找到以冬的链接，只能搜索播放器的链接奉上想听的朋友可以点击下：点我\n \n女主：​        我是个孤儿，那天我上山采药,突然下暴雨。不 知所措的我躲在屋檐下等雨停恍然看见一个 与我年龄相仿的少年。 他身着白衣撑着伞朝我走来，说要送我回家，而 我早已陷入他那对深邃的眼眸中心内一阵 悸动。他一把拉我入伞下。   我得知他是山上的道士，也刚好下山采药，他把  伞赠予我，一个人回去了。  还在茅屋门口目送他的我愣在原地。后来我采 药卖钱买了一盒桂花糕去山上和他道谢，他也很开心的样子收下了。我和他走在山路上像是孩子一样嬉戏，他送我   下山。之后我便总去给他送东西，不论是自己   做的还是买的，他也都会很开心的收下  ，之后我们聊着天。\n​          每次他都送我下山，我们一 起骑着马一起游玩。 记得我送给他马具的时候他特别开心，他扶着在 我耳边说一直会保护我。 这天突然下暴雨，我住的茅屋破烂,于是决定去 山上当一个道姑，这样就能永远陪着他了。 \n​         我带着他送给我的伞，还有为数不多的家产上 山去当了道姑，但他好像不在。另一个道长再三问我是不是确定好了要当道姑 ，当了道姑后要断红尘，即使这能永远陪在他身   边也好。 于是就换了一身素装，成了一名道姑，之后的日 子每天都能与他遇见，他还是原来的样子总是   和我嬉闹。   但我总是发现他老是下山而且越来越频繁，但   每次回来都会给我带来胭脂红妆什么的送给  我，我开心极了。    \n​      这天夜晚，他刚从山下回来，敲我房门，要送给我一个胭脂,说这是最新款的，女孩子用了后特别漂亮，我开心极了。对他说，这样好像不太好,总是送我东西，被其他人看见就不好了，他说只管收着就好了。他问我，思念一个人是什么感觉，还没等我回答，他就转身离去了，我心里一阵悸动。就这样过了三年。\n​       这天他突然说要还俗，因为他和乡下一个卖胭脂的姑娘私定了终身，并且答应要娶她,于是道长带着大家的祝福收拾收拾就下山了，我呆呆的愣在原地，眼眶渐渐湿润。原来三年前他喜欢的就不是我，那送我的那些胭脂只是为了讨好她，思念的人自然也不是我，想到这我泪水再也忍不住。不久就传来道长喜宴的消息，我假装偶然赶上他们的喜宴，他看见是我先是 愣了一下。\n \n​      他还是一身白衣如旧，依附在他身旁的佳人有如花的颜容。   她问他我是谁，他说是以前在山上当道士的   时候的   一个道姑朋友，身边佳人有露出了甜美的笑 容。此时不知为什么决定他们很是般配。但是   我还是想上前问他，是不是我送的马具不够好   看，是不是那天的桂花糕我没捂热，  是不是世上的人都是这样连自己的承诺都可 以随 意的收回。 好想一把上前抱住他在他白皙的侧脸留一个唇印，任旁人惊讶，可是我不能，只能强忍微笑给了 他们祝福,并把当年道长送我的伞送给了他们当作贺礼。 我在角落里独自饮酒，转身离去谁也没看见 我转身后的泪如雨下。\n​      后来我一个人去了很多地方 从春天一直走到冬天，那个时候的那件事和事 里的那个人，就好像我做的一场梦。 现在梦醒了，什么都没了。    \n道士：我是一个道士，却爱上了一个经常上山探药的 姑娘，她不知道我怕她发生危险经常在远处保护着她。 那天突下暴雨，见她却无带伞连忙寻伞去接她， 我一身白衣见她檐下避雨，一把拉入伞下， 我说送她回家。 姑娘面色绯红眼里闪着光，那是我这辈子见 到的最动人的眼神。我把伞赠予了她,独自一人回了山上。\n \n后来她 到观中送来一盒桂花糕 与我道谢，以表感激我 送她下山，我和她走在山路上，她一直专心倾听   着我说的话。之后她总是会到观里来送我不同的东西，每次我都会收下后，和她一起聊天，送她下山。我们 一起骑马游玩,记得有一次她送我一具马鞍,我   那天很开心，把她拥入怀下伏在她耳边许道   ,我会一直保护她。  原本心中定了还俗的念想，  这天天下暴雨，我一个道友告知我，有位姑娘在   观中当了道姑，她一身素衣看见我后好像很开   心的样子，但我却心中忧郁万千，后来才知道她 是为了我才当了道姑。   还是，原来那样我们每天都有很多话聊为了讨   她欢心，我每次都会从山下挑一盒胭脂红妆回来送她，她也都会很开心的收下。 \n​     这天晚上我挑了一个非常贵重的胭脂给她送了 过去她怕被人看见了不好，我说只管收着就好，问她，你知道思念是什么感觉么?她深思，我却   没等她回我便离开了去。   就这样过去了三年，一天我在山下胭脂摊挑选 胭脂，卖胭脂的是一位姑娘身着艳丽,却也没   过多注意。   不料下起暴雨，傍无伞具。姑娘见状便留我小   坐略饮薄茶等雨后天晴再走不迟。       \n​      半饮过后，头感昏沉，心中情起。翌日早上，姑娘 |给我看榻上落红，并要我负责，虽尽是愧疚与悲 戚，但也不能推辞责任，免姑娘日后难见他人，那 天晚上我一人楼中独饮,心中她的身影若影若 现，不敢再去与她相见。 后来我还俗与姑娘大喜当天，她身红衣红唇， 美得凄凉，我却不敢正眼看她 身旁佳人问到她是谁，我只轻声回她一句那是我的一个道姑朋友。 \n​     这天她一个人在角落不停地喝酒,最后也不知道她是怎么离开的。 傍晚,在满布红烛的房间里，佳人一人独坐红 榻。我抚摸着她送我的那具马鞍， 从袖中拿出那温热的桂花糕送入嘴中，眼中湿 润已久心里想的却不是眼前人。  \n胭脂姑娘：​      我是个卖胭脂的姑娘，他是山上的道士我偷偷 留意他很久了。 在一个暴雨天我在我的摊位旁看到了他，为另 一个模样清秀的姑娘撑伞， 他们轻轻相拥，他说着好听的诺言说想一直拥 着她，那姑娘面色绯红，眼里闪着光一那是我这 辈子见到的最动人的眼神，但是我却嫉妒得发 狂,我一定要阻止他们在一起!  !不久后，听说那姑娘当了道姑，我有些不理解她 的想法。\n \n​     但是,就在她当道姑那一天，我看到了 山下的他，我化着精致的妆容， 一颦一笑皆是风情，莲步轻移眉眼低垂，这是我有意为之的，我对自己的容颜有着十足的自信， 他却熟视无睹， 只是细细询问我各种胭脂的用法，后来他买了 几盒胭脂，脸_上漾着笑，他是在想她吧  \n  不行，他是我爱的男人，我要用我的万种风情得   到他!他走后，我在铜镜前仔细端详自己的容颜，不知为何，却觉得 此刻的我的一双桃花眼竟比不上那个小   道姑的一对眸子..后来他常来我的店铺挑选胭   脂，但也只是选胭脂而已。   终于，有一天，他挑完胭脂后下起了雨，他没带伞，   我留他小坐略饮薄酒，并在他的酒中加了   催情，药，他浑然不觉…  \n​     那个夜晚，我的阁中春色浓。翌日早上，我给他看榻上的落红，并要他负责，他的眼中尽是愧疚与悲戚，良久，他同意了。   大婚那日锣鼓喧天，好不热闹，我却看到了红唇   红衣的她，她明明只是中上之姿，比起我可差远   了,此刻我却觉得她有一种惊心动魄的美，我故   意在她面前问他这是谁，他怔了一刻随即平静   1地说道:”她是我的一个道姑朋友。”  \n​      我乖巧地点了点头，拉他到一旁后来我看到她在角落里不停地饮酒 ，她是怎么离开的我也不得而知。傍晚,在布满 红烛的房间里，他并没有碰我，让我先睡我却在 深夜看到他抚摸着一副马 鞍,往嘴里塞着桂花 糕，压抑着自己的哭声。 房间的角落静静地躺着一把油纸伞.  \n婚礼上的一名宾客：​      我是婚礼上的一位宾客，至于叫什么就不便透 露了。卖胭脂的姑娘结婚了邀请我们这些邻 里去参加婚礼。 那个姑娘啊，我看着她从蹒跚学步到如今的豆 蔻年华，不知不觉居然要嫁人了。 外 面锣鼓喧天，屋里张灯结彩，好派喜庆的模 样。 可新郎生硬 的抱着新娘一脸落 寞看不 出一 丝的欢喜。\n​    直到一位红衣道姑的出现才让他眼中焕发出惊 人的神采，我才觉得事情好像不是那么简单。 胭脂姑娘 问他那女子是谁，他愣了一下故作平 淡的说:“她是我的一个道姑朋友”。 多么平淡的一句话，可是我从里面听出了多么 复杂的情感一 不甘、愧疚、心灰意冷 于是我便留意起那位道姑朋友她看着淡然的 新郎红了眼眶，只是不停的饮酒.. 心里突然堵得慌。\n​    我原来并不是一个多愁善感的人。是道长那一   瞬间仿佛活了过来的神采 ，还是道姑踉踉跄跄离去时眼角挂着的泪珠触   动了我?我不得而知，我只是觉得他们才是本应   该在起的人…  犹记那日天降大雨，我坐在门前屋檐下看见一   位道长撑伞雨中前行，白衣淡然,翩若惊鸿。   他把一位姑娘拥入怀中脸上溢着淡淡的笑容，   或许才这是抱住了幸福的模样吧..  \n结局：  道士走后，留下胭脂姑娘，没人知道道姑走到哪里，道士只想一直找 下去他相信总有一天会找到她…道姑走过海边，这是她第一次见到海，可她想与心中的那人一起倾听大海的声音想到这,她自嘲的笑笑，几天后路过这里，一位白衣少年驾着马,他心想:她一定会喜欢大海。\n​     可少年没有停下，他不敢停下，害怕再次错过 她。道姑来到瀑布下，站在一块石头 上感受 到冰凉的水珠轻吻着她的脸颊她不禁想起那年道士大婚时她很想走_上去 把抱住他,在他白 净的脸留一 个唇印。怎么又想起这件事，道姑自言自语道。 几天后，白衣少年依旧马不停蹄地赶着。 连风景都来不及看。 \n​    五年过去了，少年陪道姑看完了这世间的繁华 和苍凉。 他们近在咫尺却远在天变道姑来到了洛阳，正 值夏季，突然来的暴雨让道姑不知所措，慌乱中 她跌入了一个温暖的怀抱，她吓得-愣， 抬眼一看，一张陌生的脸她有些失望眼底闪 过一抹不经意间的落寞， 1但她很快反应过来的对那个人道歉.  \n   她想着:他应该已经和那位姑娘都有了孩子吧， 怎会来寻我。 水从她脸_上划过，不知是泪水还是雨水。 这时，一双好看的手将她拉入怀中道士在她耳 边轻声说到“我找了你五年,还俗吧，我娶你。\n \n感谢总结： 首先给看完的棒棒的你一个掌声，由于文章的字数较多。可能分段本人分的也不太好，也感谢你，陌生人！能看完这篇文章。 也许我们的人生就是一段旅行，一声会看见很多人可是真正有感觉的人总是那么一两个。走到一起真的不容易，珍惜眼前情义。\n","tags":["谈心"]},{"title":"新的一年,HI!","url":"/2019/02/12/8/","content":"  转眼已经2019了,时间过的可是真快,生活还是那么的苟且，坐在即将出发的路上给自己来一个晚来的新年文章。\n先给自己来一个简单的2018的年总结吧:1：博客折腾折腾我还是到了hexo,并且越来越懒了…​           2018年夏天左右,服务器到期了现在我一个穷鬼想了想还是不要弄服务器的了,毕竟来回都是RMB，能少点阿里就少点吧,还可以学习一点新的东西\n2：来到了我现在的学校,努力的学习,放下了一切的杂念(以后我想也不会了)​           还是清楚的记得去年的我狂妄自大的去做一些事情，结果在3月份出来去宿迁看一个学校的时候，试听期间就感觉到了这里大佬云集,甚至说的东西我都还有点听不懂,才知道自己的三斤八两,不过也还是把自己的性子磨了磨，但是也有一些原因来到了我现在的学校。\n3:   年底了学校给了15天的假期回家,给自己动了个小小的手术,emm​         今年回家回来,不知道是我真的点背还是怎么,脚的指甲老是去肉里探索(瞬间心里wc)，一想到始终还是得做,毕竟到时候上班回来的话时间可能会更少,就去了医院做了这个小小的手术。\n​         当然了,这个年假是我目前活这么长最无聊的年假,真真正正的躺了15天,在上车的前两个小时,抓紧回来更新了这个文章,希望自己以后可以幸运点.\n​        不过时间没有白白浪费掉还好,我和我的小组一起趁着年假，做了一个小小的项目,也学会了很多的业务逻辑\n  哈哈哈，看到这里可能：？？？ 黑人问号，你到底在干了什么\n现在在学习的一些东西:​     1:首要的就是把自己眼前的java学好,以及深入java web开发\n​     2:学习一些框架,自学一些语言等\n​     3:以及一些大数据时代一个bug触发机必须掌握的东西\n  嗯,可能大神看到了这个微微一笑,没错我确实好菜,希望到时候大佬带带我~\nEMMMMMMMMM—————–这可能是最心情复杂的文章\n为什么这么说呢？因为在写上面的时候我还在不慌不忙的写，结果直接告诉我，高速封路了…….\n\n赶紧合起来电脑就去高铁站,就想着期待有一张我的票,慌慌张张的emm,想着只要是别误了我上课怎么都行,售票员小哥哥跟我说,高铁没了就剩下一张站着的火车票,瞬间运气爆棚 站着上了火车。\n当我走到石家庄的时候,有一个人可能是坐累了,叫了我一下他就抱着孩子去卧铺那边了,真的是遇到贵人了,在火车上都不敢睡觉,生怕人家回来了人家站着我在哪坐着,EMMMM\n就这样到了北京站直接就回来了，2点多,被这场封高速已经弄得绝望了。\n扯这么多,给自己的2019立一个flag吧！1.在北京实习找到一份稳定的饭碗。\n2.去一次旅游(见一个人)  不知道能不能\n3.希望在年底学会开车 哈哈~~\n嘿嘿,给大家一声晚来的新年快乐！\n​     \n","tags":["谈心"]},{"title":"来聊(水)下最近hexo备份的思路","url":"/2022/11/11/11/","content":"出现问题最近（2020-2022）博客出现了短暂的停更状态。\n具体原因是因为hexo的文章文件夹在编译过后并不会存储在GitHub的仓库里，而我也同样因为电脑硬盘损坏而失去了我的博客维护权限。\n朋友们也在这期间纷纷到访过博客，尤其是更换友联站点的url、更新文章中的错别字这种强需求无法及时的得到支持。\n解决思路\n把hexo文件夹备份至自己的本地的文件\n把hexo整个文件夹备份至git，并且手动add、commit等\n采用同步盘的思想，hexo文件夹放入同步盘文件夹中。搬设备到各个平台只需要登陆账户即可\n\n重点说一下同步盘的思想\n无论是定期备份、还是每次写完文章后都把自己的文件夹手动的add、commit至git仓库，似乎都不是那么的解放双手？写完忘同步，忘commit都会产生数据丢失的问题。\n然而采用同步盘既可以解决备份，还可以解决以下问题：\n\n多设备编辑文章，在iPhone、Android、Mac、Windows、Pad端都可以做到编辑markdown文件，实现写作自由。有网络的地方就有hexo，就有认真写博客的倔强青年！\n好的同步盘甚至具有协作的功能，这里幻想一下，博客也能跟小伙伴一起协作的快感。\n\n无论如何怎么看，似乎同步盘都可以解决更多的问题。\n同步盘的选型同步盘的产品众多，大厂小厂甚至网盘行业都会来抢这块蛋糕，那我们就从以下几个方面选型市面上产品\n\n跨平台性，针对多设备的兼容\n不限速\nbugfix的数量以及修复的速度\n易用性\n\n简单的来说，入围的产品有OneDrive、iCloud、DropBox、坚果云。\n说搞就搞\n拷贝文件夹到oneDrive\n\ncp hexo /Users/zhangshuaike/OneDrive/hexo/\n\n\n等待同步就好，同步完成后在别的设备下载OneDrive，打开hexo文件夹管理即可\n\n待解决的问题\n如果换了新的设备，怎么初始化npm相关包？好说，搞个init.sh即可\n\necho &#x27;hexo 52xk.cc start&#x27;cd 52xk.ccnpm install -g hexo-cliecho &#x27;hexo init success&#x27;#npm install hexo-theme-keepecho &#x27;hexo init theme:keep,success&#x27;npm install hexo-generator-searchdbecho &#x27;search 插件安装成功&#x27;echo &#x27;deploy插件&#x27;npm install hexo-deployer-git --saveecho &#x27;start localhost:4000&#x27;hexo s\n\n采用同步盘，非常方便稳定的就解决了hexo的数据丢失无法维护的问题。大家可以试一试\n","tags":["hexo","OneDrive"]},{"title":"老朋友，别来无恙啊。","url":"/2018/06/28/3/","content":"Hi，My Friend ，Hello hexo朋友，你好啊~  别来无恙啊。\n没错，你又看到了，我好像又折腾了一波。\n估计这次完事之后不会在折腾了，等着穷到毕业~\n但是这个博客也是挺好的，做的其中遇到了很多的问题，也特别感谢我的表哥@唯美陌阡,因为从来没想过这么多问题。也是做起来比动态博客还要麻烦的一批~帮助我解决了很多问题。\n反正最后磕磕绊绊的还是过来了~  \n\n为何要搬过来？做一个动态博客的成本确实不小，在我这个小小的学生狗面前还是付不起那种高级的服务器费用，同时这个没有那么多限制，可以随心所欲。恩 还得继续 加油！\n","tags":["hexo","hello"]},{"title":"状态设计模式的实践","url":"/2024/11/10/24/","content":"简介状态模式是一种行为设计模式， 让你能在一个对象的内部状态变化时改变其行为， 使其看上去就像改变了自身所属的类一样。\n\n其主要思想是程序在任意时刻仅可处于几种有限的状态中。 在任何一个特定状态中， 程序的行为都不相同， 且可瞬间从一个状态切换到另一个状态。 不过， 根据当前状态， 程序可能会切换到另外一种状态， 也可能会保持当前状态不变。 这些数量有限且预先定义的状态切换规则被称为转移。\n你还可将该方法应用在对象上。 假如你有一个计算任务调度服务平台。 任务可能会处于 创建 、  启动和 已完成等多种状态， 任务运行策略会在不同状态下的行为略有不同：\n\n处于 启动时， 它会将文档转移到完成/失败中状态\n处于 重启时， 它会将文档转移到启动状态\n处于 停止时，它会转移到熔断状态\n\n\n编码阶段（demo code）下面是一个简单的类图，我们以此来进行推进demo。\n\n步骤 1创建一个接口。\npublic interface State &#123;   public void doAction(Context context);&#125;\n\n步骤 2创建实现接口的实体类。\nStartState.java\npublic class StartState implements State &#123;    public void doAction(Context context) &#123;      System.out.println(&quot;Player is in start state&quot;);      context.setState(this);    &#125;    public String toString()&#123;      return &quot;Start State&quot;;   &#125;&#125;\n\nStopState.java：\npublic class StopState implements State &#123;    public void doAction(Context context) &#123;      System.out.println(&quot;Player is in stop state&quot;);      context.setState(this);    &#125;    public String toString()&#123;      return &quot;Stop State&quot;;   &#125;&#125;\n\n步骤 3创建 Context 类。\npublic class Context &#123;   private State state;    public Context()&#123;      state = null;   &#125;    public void setState(State state)&#123;      this.state = state;        &#125;    public State getState()&#123;      return state;   &#125;&#125;\n\n步骤 4使用 Context 来查看当状态 State 改变时的行为变化。\npublic class StatePatternDemo &#123;   public static void main(String[] args) &#123;      Context context = new Context();       StartState startState = new StartState();      startState.doAction(context);       System.out.println(context.getState().toString());       StopState stopState = new StopState();      stopState.doAction(context);       System.out.println(context.getState().toString());   &#125;&#125;\n\n步骤 5执行程序，输出结果：\nPlayer is in start stateStart StatePlayer is in stop stateStop State\n\n上面这些code已经完成了，有一些问题，下一步准备优化\nSpring优化版本上面的demo阶段有一些问题\n\n无法融入到spring容器中\n假如加入到spring容器中，涉及到的类都需要进行一遍注入？\n\n@Resourceprivate StartState startState;@Resourceprivate StopState stopState;\n\n问题也在此暴露了出来，无法真正的实现解耦\n\n无法做熔断，可能会出现过多的try-catch，比如执行start失败后需要处理熔断，执行stop后需要处理重试\n\n改进思路状态设计模式适用于多种状态判断时，进行各状态解耦的前提下，还得注意保护自己的代码\n\n首先并不推荐使用Spring的状态机，个人理解对代码的侵入性太强\n简洁的change方式更能体现出业务代码，切划分更细\n增加一个枚举，来进行参数的传递\n所有的bean全部加入到spring容器中，做单例管理\n增加一个类似于适配器的选择器，传递枚举、context来继续实现状态模式\ncontext需要改造，不应持有状态的对象。应该持有变量来进行规范\n\n\n\n下面是改进后的类图\n\n增加枚举，确定状态传递参数枚举用来作为参数的传递，是本次改造重要的一环\n@Getter@AllArgsConstructorpublic enum StateEnum &#123;    START(1),    STOP(2),    RE_START(3);    private int state;&#125;\n\nContext类进行改造context改造后，仅作为参数类进行传递\n/** * 上下文 */@Data@AllArgsConstructor@NoArgsConstructorpublic class Context &#123;    /**     * 用户ID     */    private Long userId;    /**     * current state，当前状态     */    private StateEnum stateEnum;&#125;\n\nstate类融入spring容器，改造先把相关类加入到容器中@Servicepublic class StartState implements State &#123;   //code&#125;    @Servicepublic class StopState implements State &#123;   //code&#125;        \n\nState类增加hit函数，判定是否命中给State类增加一个是否命中函数，相关实现类指定自己的枚举值进行eq\npublic interface State &#123;    /**     * 判断是否命中     * @param context 上下文对象     * @return true则代表命中     */    boolean isHit(Context context);&#125;\n\n下面再给各个实现类增加实现\nStartState：\n@Servicepublic class StartState implements State &#123;    /**     * 判断是否命中     * @param context 上下文对象     * @return true则代表命中     */    @Override    public boolean isHit(Context context) &#123;        return StateEnum.START.equals(context.getStateEnum());    &#125;&#125;\n\n\u0000StopState:\n@Servicepublic class StopState  implements State &#123;    /**     * 判断是否命中     * @param context 上下文对象     * @return true则代表命中     */    @Override    public boolean isHit(Context context) &#123;        return StateEnum.STOP.equals(context.getStateEnum());    &#125;&#125;\n\n增加一个选择器比较重要的一环出现了，每个类都进行了改造。该标记自己的标记自己，该增加参数的增加参数，那么怎么用起来这些参数？也就是刚刚所说的适配器\n\n注入State下的所有字类用来匹配\n循环使用isHit函数来进行匹配，匹配到后invoke相关函数\n\n@Componentpublic class Change &#123;    @Autowired    private List&lt;State&gt; stateList;    public void doAction(Context context)&#123;        for (State state : stateList) &#123;            if (state.isHit(context)) &#123;                state.doAction(context);            &#125;        &#125;    &#125;&#125;\n\n做个测试Change对象就作为所有State类的入口来进行匹配了，只需要更改相关的参数类（Context），即可调度相关的状态实现\n    @Autowired    private Change change;    @Test    public void testState()&#123;        Context context;        context=new Context(1L, StateEnum.START);        change.doAction(context);        context = new Context(1L, StateEnum.STOP);        change.doAction(context);    &#125;&#125;\n\n参考\n状态模式 | 菜鸟教程\n状态设计模式\n\n","tags":["Java","Spring","设计模式"]},{"title":"那些年我们在北京租过的房子","url":"/2018/07/13/5/","content":"2006年，因为转学到北京，我开始在学校门口租房。我人生第一次自己租住的房子，是学校附近不远的城中村。城中村的入口在清华旁边的一条宽广的马路上，门口看只是一条普通的巷子。走进去1000米，才会看到里面别有洞天。这个城中村全部都是平房以及农民自己搭建的小二楼，住着的都是附近卖菜的送水的或者做小生意的一些社会最底层的劳动人民。我住的房间大约有40平米，平房，一月500块，和三个女生一起合住，每人的租金不到200块钱。没有厕所和厨房，都要去公用的厕所，有时候你在坑上蹲着，面前就飘来一只大狼狗。这里什么都有卖，打电话都便宜的很，瓜果梨桃卖的也很便宜。我在这里住了大约一两个月，因此治安不好，天天有房间被盗，而我跟同屋的女生也不太处的来。他们要早起早睡，而我还是学生要做功课。几次矛盾下来，我就离开了。我再也没有回去过，但是每次看到电视里南方工厂里打工妹生活的纪录片，我就会想起那个地方来，总觉得，那是特别珍贵的一段经历和记忆。\n  \n后来我搬家到学校门口一个房子里的床位，一个三居室，住着14个人，我的小房间有四个女生，另外一个大房间有八个男生，还有一对小情侣在小屋里住着，每人300元每月。我在这里住了大约两年，舍友换了无数无数，中国的外国的，打工的考研的，精神正常的精神不正常的。有个姐姐的老公在对面的学校里读博士，她在门口租一个床位陪读，后来他们毕业后一起去了美国。有个女生在那个屋子里考了三年北大光华MBA，终于梦想成真，考上的那一年她哭的坐不起来。有人在那个屋子里恋爱又失恋，有人在那个屋子里每天摔摔打打。每天晚上排队洗澡洗衣服像一个风景，着急的时候头冲着水龙头随便洗一下就湿漉漉的去上课了。那时候似乎谁都没有想起来还有吹风机，也不觉得人多。14个人一起在客厅看电视的时候，拥挤的场面简直像世界杯直播一样热闹。\n大学最后一学期，我因为实习在CBD区，天天上班要2个小时实在要吐血了，于是在北京大望路地铁站旁边与人合租了一间房。这间房子总共6平米，一人一张床占4平米，还有半平米放衣柜，1.5平米的地面。这间房子价格800元，我和另一个女生一人400元，那时候实习工资是每月1200，后来变成1760。我们两个都是一个行业，因此相处还比较容易。我实习时候经常加班到深夜，每次回来的时候，她已经睡了，只有灯亮着电脑开着收音机在被子里也开着。每次回家都要翻开她被子找收音机关掉，再关灯关电脑。她的电脑是台式机，总是轰轰的声音，我叫她的电脑是拖拉机。我们一起住了半年，彼此毕业开始正式工作，但低廉的工资让我们依旧只能还住在这个楼里，因为这里是还算黄金位置的大望路地铁边上唯一的年代久远的筒子楼，价格比同小区的高层住宅要便宜很多。\n \n她换到另外一个20平米的大房间里与人合租，我继续一人租这个小房间租了半年。这期间，我很好的大学朋友去了法国读MBA，临走和我坐在小房子里聊天，房子太小了，我有点不好意思让她来。我妈来北京看奥运会也住在这里，因为太热没有空调，我妈睡在地上，我睡在床上。我妈后来说，看见我住的这么小，心里很难受。这期间我还买了我第一个笔记本，是一个二手的笔记本，1200元钱，我就是用这个笔记本开始了我的写作道路，一直到三年后这个本光荣就义。\n  \n再过半年，隔壁的姑娘要换房子，我就去了隔壁20平米的大房间，每月1000元，水电网另计，怎么也要每月1200吧。网络我从一楼的一户人家牵线上来，上来后还分给三家用，因此每月大约20块钱就够了。到现在每次交很贵的网费的时候总是想起这事儿，还总想总隔壁分一根线，可惜没人跟我分了。要说这个房子大而光明，还挺不错的，但唯一的问题是厕所。因为四家合用，隔壁是三个男生，一对夫妻，门口那家是八个洗脚妹，人多到厕所巨堵，到后期天天屎飘在马桶里，你还不得不继续上。更惨烈的是有时候你在旁边洗澡，旁边就是飘着屎的马桶。找人来通了很多很多次，但终究不知道为什么还总是要赌。门口的打工妹用洗衣机总是把水流到楼道里，楼上和楼下的邻居就会来破口大骂，好几次打110报警，每次我都要连哄带骗的安慰邻居，再收拾楼道，因为洗脚妹们开着洗衣机就不知道去哪儿玩儿去了。在这个房子里，我开始每天1500字写博客，雷打不动的坚持，就是从这个房子里开始的。开始用电饭锅给自己做饭，买了一个二手洗衣机200块钱，都是从这里开始。\n  \n房子到期后，我决定离开这个屎太多的地方。于是在网上找到了蒲黄榆的一个房子。这是个大约有20年历史的老房子，是个高层，还是个银行的宿舍，因此邻居都是老人家，且鲜有租客。房子陈旧，但能看得出当年是新房的时候，房东还是花了大力气装成当年最时髦的样子，家具虽然过时，都都是上好的实打实的实木家具。在这里，我结交了非常好的朋友。他们知道我是个半夜写作白天上班的人，因此主动承担起三年倒垃圾打扫卫生的工作，从来不用我动手，也不用操心。可能我做饭比较烂，他们每次都做好饭给我送来吃，从来不让我进厨房，还说我进厨房一次她们要收拾半宿还是她们来吧。起初我在这里租住最小的房间6平米，600元，小小的热热的但很温馨，我就在这个小房间里写了我人生的第一本书《从北京到台湾这么近那么远》。那时候记者来我家采访，三个人根本站不进来，只能我和记者坐在床上，摄影大哥站门口，还一位站走廊里。现在每次看到这本书，都忍不住想到那些小日子，只有梦想，能让人克服一切的困难，让每天的日子，都闪闪发亮！\n  \n后来我转到隔壁20平米1000元每月的房间里，到我走的时候房间的价格差不多1500一个月，因为是房东直租，依旧算是很便宜的价格吧。只是唯一重大的问题是，这个房间楼下正对着一个神经衰弱还有心脏病的老太太，只要我在楼上小心翼翼的走一步路，老太太都会认为是之天大的声音而找上门来，甚至为此心脏病发急救过。后来，房东把新买的地毯都放在了我的房间里，老太太依然能听到声音，甚至半夜一点把110叫来投诉我扰民，可是110来了看见什么都没有随便打发下就走了。无力承受老太太的生命，只能三五天去一趟老太太家，提着瓜果梨桃去看看她是在家好好的还是又去医院了。在这个房间里， 我写了我人生第三本书，并跳了槽，度过了两年半的时光，我毕业后重要的人生转变，职场转变，以及迅速的成熟长大都是在这里，包括遇到至好的朋友，以及终于过上了安全而安稳的生活。\n后来，因为租房的价格愈发昂贵，我买了房子；再后来我结婚，又换到了婚房里。尽管自己的房子干净又整洁，安静又安稳，但我总记得那些租房子的时光，那些铭刻在我青春里的每一天，那些心惊胆战又脏乱差的日子，或者被110训话或被邻居投诉的日子，像利剑挂在我心上，又像星星，回想起来会给自己点赞。直到现在，我依旧喜欢帮朋友找房子，喜欢没事儿看租房网站，总会回想起以前自己租房的日子，以及那些在下班后黑灯瞎火与中介去黑漆漆的小区看房子的场景。\n我一直相信，有一天每一个人都会有自己的房子，自己的家庭。这大千世界的一隅，总有一天会有一盏等着我们回家的灯。而年轻的时候所有的颠沛流离都将成为日后心中的慰藉，是青春的圣火，是跃动的生命。它们闪着光，透着亮，提醒着我们曾经那么年轻，曾经那么敢闯，曾经天不怕地不怕，曾经什么都可以接受和忍耐。\n所有的年轻，有一天都会长大与成熟，当回忆往事的时候，望着远方，怦然一笑，就是对青春时光里所有的所有，最好的诠释与珍藏。\n","tags":["鸡汤"]},{"title":"镜像瘦身利器，Docker的多阶段构建","url":"/2024/09/27/23/","content":"为什么要使用多阶段构建？传统的Docker Build 镜像构建过程中，所有的构建步骤都在同一个镜像层中完成，其中包含了大量的构建工具和中间文件。\n比如使用go mod download、go build后，你的镜像可以轻轻松松的超过1G。在持续部署过程中，分发的成本和时间非常高，有的时候修复一个问题，早1秒部署都可以减少很大的损失，更别说日常了，1G的镜像分发费用也不是一笔小费用。\n什么是多阶段构建？多阶段构建是指在一个Dockerfile中使用多个FROM指令来创建不同的构建阶段。每个阶段都可以有自己的基础镜像，并且每个阶段可以相互独立地运行命令。\n通过这种方式，我们可以将构建过程分为几个部分，每个部分专注于完成特定的任务，如编译代码、复制依赖文件等。\n\n\n类似于上图的逻辑，瘦身的体验相当不错，从1.3G直接缩到28M，效果非常显著。\n在上图中，做一个简单的解释\n\n第一个阶段从基础 Go 镜像中拉取，并执行编译和下载等操作，这些资源在构建完成后实际上并不需要。我们将该阶段命名为 builder。\n\n第二个阶段引用第一阶段的 builder，从中复制构建物。在第二阶段中，只有基础的 Linux 环境和一个 Go 的二进制运行包。\n\n\n使用两个阶段就可以轻松将Go的构建物实现解耦和精简，非常的方便。\n怎么使用多阶段构建？下面是一个Go的dockerfile，供参考\n其实和我们正常的构建操作是差不多的，编写完成后正常执行docker build . 即可\n# 使用已有的 Golang 镜像作为基础镜像FROM golang:1.23.0 AS builder# 设定代理ENV GOPROXY=https://goproxy.cn# 设置工作目录WORKDIR /app# 复制 go.mod 和 go.sum 文件COPY go.mod go.sum ./# 下载依赖RUN go mod download# 复制整个项目COPY . .# 编译 Go 应用RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o main ./src/main/app.go# 使用较小的基础镜像FROM alpine:latest# 安装 ca-certificates 和 tzdataRUN apk --no-cache add ca-certificates tzdata# 设置时区为上海RUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; echo &quot;Asia/Shanghai&quot; &gt; /etc/timezone# 设置工作目录WORKDIR /root/# 从 builder 镜像中复制编译后的二进制文件COPY --from=builder /app/main .# 暴露应用运行的端口EXPOSE 8080# 运行应用CMD [&quot;./main&quot;]\n\n总结无论是可以直接编译机器码的Go、还是已经推出GraalVM的Java，还有其他可以编译机器码的语言都可以使用这种多阶段构建方式进行瘦身。\n其中好处有不仅可以提升部署的速度，更能减少分发的时间和带宽成本。\n","tags":["Docker","容器"]}]